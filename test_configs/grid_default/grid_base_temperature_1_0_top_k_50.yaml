config_name: grid_base_temperature_1_0_top_k_50
model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
is_encoder_decoder: false
task_type: text_generation
inference_type: pure_generative
max_input_tokens: 64
max_output_tokens: 32
min_output_tokens: 0
num_input_prompts: 1
save_outputs: false
decode_token_to_text: false
prompt_source: null
gpu_list:
- 0
num_processes: 1
batching_options:
  batch_size: 1
  strategy: static
  max_tokens_per_batch: null
  dynamic_batching: false
sharding_config:
  strategy: none
  num_shards: 1
latency_simulation:
  enabled: false
  mode: poisson
  target_qps: 1.0
  seed: null
decoder_config:
  temperature: 1.0
  do_sample: false
  top_p: 1.0
  top_k: 50
  min_p: 0.0
  repetition_penalty: 1.0
  no_repeat_ngram_size: 0
  preset: deterministic
quantization_config:
  quantization: false
  load_in_4bit: false
  load_in_8bit: false
  bnb_4bit_compute_dtype: float16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: false
schedule_config:
  enabled: false
  interval: null
  at: null
  days: null
  total_duration: 24h
fp_precision: float16
backend: pytorch
cycle_id: null
num_cycles: 1
query_rate: 1.0
random_seed: null
extra_metadata: {}
