# Test tensor parallel sharding (splits layers across GPUs)
config_name: sharding_tensor_parallel
model_name: meta-llama/Llama-2-7b-hf  # Use a larger model for sharding

sharding:
  strategy: tensor_parallel
  num_shards: 2  # Must not exceed available GPUs

gpus: [0, 1]  # GPUs to use
max_new_tokens: 64
batch_size: 2
