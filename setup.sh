#!/bin/bash
# =============================================================================
# LLenergyMeasure - One-Click Setup
# =============================================================================
#
# Usage:
#   ./setup.sh              # Build PyTorch backend (default)
#   ./setup.sh --backend vllm      # Build vLLM backend instead
#   ./setup.sh --backend tensorrt  # Build TensorRT backend instead
#   ./setup.sh --all               # Build all backends
#   ./setup.sh --local             # Local install (poetry) instead of Docker
#
# After setup:
#   ./lem experiment configs/examples/pytorch_example.yaml -n 10
#   ./lem campaign configs/examples/campaign_example.yaml --dry-run
#
# =============================================================================

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Parse arguments
BACKEND="pytorch"
BUILD_ALL=false
LOCAL_MODE=false
SKIP_BUILD=false

while [[ $# -gt 0 ]]; do
    case $1 in
        --backend)
            BACKEND="$2"
            shift 2
            ;;
        --all)
            BUILD_ALL=true
            shift
            ;;
        --local)
            LOCAL_MODE=true
            shift
            ;;
        --skip-build)
            SKIP_BUILD=true
            shift
            ;;
        -h|--help)
            echo "Usage: ./setup.sh [OPTIONS]"
            echo ""
            echo "Options:"
            echo "  --backend NAME   Backend to build: pytorch (default), vllm, tensorrt"
            echo "  --all            Build all backends"
            echo "  --local          Use local poetry install instead of Docker"
            echo "  --skip-build     Skip Docker image build (use existing images)"
            echo "  -h, --help       Show this help"
            exit 0
            ;;
        *)
            echo -e "${RED}Unknown option: $1${NC}"
            exit 1
            ;;
    esac
done

echo -e "${CYAN}"
echo "╔═══════════════════════════════════════════════════════════════════╗"
echo "║           LLenergyMeasure - Quick Setup                        ║"
echo "╚═══════════════════════════════════════════════════════════════════╝"
echo -e "${NC}"

# =============================================================================
# Step 1: Generate .env file
# =============================================================================
echo -e "${CYAN}→ Step 1: Configuring environment...${NC}"

if [ -f .env ]; then
    echo -e "  ${YELLOW}⚠${NC} .env already exists"
    read -p "  Overwrite? [y/N] " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        echo "  Keeping existing .env"
    else
        PUID=$(id -u)
        PGID=$(id -g)
        cat > .env << EOF
# Generated by setup.sh on $(date)
# Regenerate with: ./setup.sh

# Docker user mapping (your host user)
PUID=$PUID
PGID=$PGID

# HuggingFace token (add yours for gated models like meta-llama)
# Get token at: https://huggingface.co/settings/tokens
HF_TOKEN=
EOF
        echo -e "  ${GREEN}✓${NC} Created .env (PUID=$PUID, PGID=$PGID)"
    fi
else
    PUID=$(id -u)
    PGID=$(id -g)
    cat > .env << EOF
# Generated by setup.sh on $(date)
# Regenerate with: ./setup.sh

# Docker user mapping (your host user)
PUID=$PUID
PGID=$PGID

# HuggingFace token (add yours for gated models like meta-llama)
# Get token at: https://huggingface.co/settings/tokens
HF_TOKEN=
EOF
    echo -e "  ${GREEN}✓${NC} Created .env (PUID=$PUID, PGID=$PGID)"
fi

# =============================================================================
# Step 2: Create required directories
# =============================================================================
echo -e "${CYAN}→ Step 2: Creating directories...${NC}"
mkdir -p results configs
echo -e "  ${GREEN}✓${NC} Created results/ and configs/"

# =============================================================================
# Step 3: Build or install
# =============================================================================
if [ "$LOCAL_MODE" = true ]; then
    echo -e "${CYAN}→ Step 3: Installing locally with Poetry...${NC}"

    if ! command -v poetry &> /dev/null; then
        echo -e "  ${RED}✗${NC} Poetry not found. Install from: https://python-poetry.org/"
        exit 1
    fi

    poetry install --with dev
    poetry run pre-commit install || true
    echo -e "  ${GREEN}✓${NC} Installed with Poetry"

    # Create local lem symlink
    if [ ! -f lem ]; then
        cat > lem << 'WRAPPER'
#!/bin/bash
# lem wrapper - local mode
exec poetry run lem "$@"
WRAPPER
        chmod +x lem
    fi
    echo -e "  ${GREEN}✓${NC} Created ./lem wrapper"

else
    # Docker mode
    if [ "$SKIP_BUILD" = true ]; then
        echo -e "${CYAN}→ Step 3: Skipping Docker build (--skip-build)${NC}"
    else
        echo -e "${CYAN}→ Step 3: Building Docker image(s)...${NC}"

        if ! command -v docker &> /dev/null; then
            echo -e "  ${RED}✗${NC} Docker not found. Install from: https://docs.docker.com/get-docker/"
            exit 1
        fi

        # Build base image first
        echo -e "  Building base image..."
        docker compose build base

        if [ "$BUILD_ALL" = true ]; then
            echo -e "  Building all backends (this may take 20-30 minutes)..."
            docker compose build pytorch vllm tensorrt
            echo -e "  ${GREEN}✓${NC} Built all backends"
        else
            echo -e "  Building $BACKEND backend..."
            docker compose build "$BACKEND"
            echo -e "  ${GREEN}✓${NC} Built $BACKEND backend"
        fi
    fi

    # Create lem wrapper if it doesn't exist
    if [ ! -f lem ]; then
        cat > lem << 'WRAPPER'
#!/bin/bash
# =============================================================================
# lem - Universal CLI wrapper for LLenergyMeasure
# =============================================================================
# Automatically detects environment and runs the right command:
#   - Inside Docker container → exec lem directly
#   - Local poetry install → exec poetry run lem
#   - Otherwise → docker compose run --rm <backend> lem ...
#
# Backend detection:
#   1. LEM_BACKEND env var
#   2. Parse config YAML for backend field
#   3. Default: pytorch
# =============================================================================

set -e

# Detect if we're inside a container
if [ -f /.dockerenv ] || grep -q docker /proc/1/cgroup 2>/dev/null; then
    exec lem "$@"
fi

# Detect if local install exists
if command -v lem &> /dev/null; then
    exec lem "$@"
fi

# Docker mode: detect backend from config or env
BACKEND="${LEM_BACKEND:-pytorch}"

# Try to detect backend from config file argument
for arg in "$@"; do
    if [[ "$arg" == *.yaml ]] || [[ "$arg" == *.yml ]]; then
        if [ -f "$arg" ]; then
            # Extract backend from YAML (simple grep, no yq dependency)
            detected=$(grep -E "^backend:" "$arg" 2>/dev/null | head -1 | awk '{print $2}' | tr -d '"' | tr -d "'")
            if [ -n "$detected" ]; then
                BACKEND="$detected"
                break
            fi
        fi
    fi
done

# Check if the backend image exists
IMAGE="llenergymeasure:$BACKEND"
if ! docker image inspect "$IMAGE" &> /dev/null; then
    echo "Error: Docker image '$IMAGE' not found."
    echo ""
    echo "Build it with:"
    echo "  ./setup.sh --backend $BACKEND"
    echo ""
    echo "Or build all backends:"
    echo "  ./setup.sh --all"
    exit 1
fi

# Run in Docker
exec docker compose run --rm "$BACKEND" lem "$@"
WRAPPER
        chmod +x lem
        echo -e "  ${GREEN}✓${NC} Created ./lem wrapper"
    fi
fi

# =============================================================================
# Step 4: Verify setup
# =============================================================================
echo -e "${CYAN}→ Step 4: Verifying setup...${NC}"

if [ "$LOCAL_MODE" = true ]; then
    if ./lem --version &> /dev/null; then
        echo -e "  ${GREEN}✓${NC} Installation verified"
    else
        echo -e "  ${YELLOW}⚠${NC} Could not verify installation"
    fi
else
    # Skip GPU verification if no NVIDIA driver
    if command -v nvidia-smi &> /dev/null; then
        echo -e "  Checking GPU access..."
        if docker compose run --rm "$BACKEND" python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')" 2>/dev/null; then
            echo -e "  ${GREEN}✓${NC} GPU access verified"
        else
            echo -e "  ${YELLOW}⚠${NC} GPU verification failed (may still work)"
        fi
    else
        echo -e "  ${YELLOW}⚠${NC} nvidia-smi not found, skipping GPU check"
    fi
fi

# =============================================================================
# Step 5: Optional symlink to ~/.local/bin
# =============================================================================
echo ""
read -p "Install 'lem' to ~/.local/bin for system-wide access? [y/N] " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    mkdir -p ~/.local/bin
    ln -sf "$(pwd)/lem" ~/.local/bin/lem
    echo -e "${GREEN}✓${NC} Symlinked to ~/.local/bin/lem"

    # Check if ~/.local/bin is in PATH
    if [[ ":$PATH:" != *":$HOME/.local/bin:"* ]]; then
        echo -e "${YELLOW}Note:${NC} Add ~/.local/bin to your PATH:"
        echo "  echo 'export PATH=\"\$HOME/.local/bin:\$PATH\"' >> ~/.bashrc"
    fi
fi

# =============================================================================
# Done!
# =============================================================================
echo ""
echo -e "${GREEN}╔═══════════════════════════════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║                    Setup Complete!                                 ║${NC}"
echo -e "${GREEN}╚═══════════════════════════════════════════════════════════════════╝${NC}"
echo ""
echo -e "Run your first experiment:"
echo -e "  ${CYAN}./lem experiment configs/examples/pytorch_example.yaml -n 10${NC}"
echo ""
echo -e "Or try a campaign:"
echo -e "  ${CYAN}./lem campaign configs/examples/campaign_example.yaml --dry-run${NC}"
echo ""
if [ -z "${HF_TOKEN:-}" ]; then
    echo -e "${YELLOW}Tip:${NC} Add your HuggingFace token to .env for gated models (meta-llama, mistral)"
fi
