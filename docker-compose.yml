# Docker Compose with multiple backend support
#
# Quick Start:
#   pip install -e .              # Install llenergymeasure
#   lem doctor                    # Check environment
#   docker compose build pytorch  # Build backend image
#
# Manual Usage:
#   Build base first:  docker compose build base
#   PyTorch (default): docker compose run --rm pytorch lem --help
#   vLLM:              docker compose run --rm vllm lem --help
#   TensorRT:          docker compose run --rm tensorrt lem --help
#
# Examples:
#   # PyTorch backend (default)
#   docker compose run --rm pytorch lem experiment configs/examples/pytorch_example.yaml
#
#   # vLLM backend
#   docker compose run --rm vllm lem experiment configs/examples/vllm_example.yaml
#
#   # TensorRT backend
#   docker compose run --rm tensorrt lem experiment configs/examples/tensorrt_example.yaml
#
#   # Development shells
#   docker compose --profile dev run --rm pytorch-dev /bin/bash
#   docker compose --profile dev run --rm vllm-dev /bin/bash
#   docker compose --profile dev run --rm tensorrt-dev /bin/bash

# Shared configuration anchor
x-common: &common
  # PUID/PGID pattern for host permission mapping (like LinuxServer.io)
  # Usage: PUID=$(id -u) PGID=$(id -g) docker compose run ...
  # Container starts as root, entrypoint drops to PUID/PGID user

  # GPU access with privileged mode for NVML energy metrics
  privileged: true
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu, utility]

  # Environment variables
  environment:
    # PUID/PGID for host permission mapping (REQUIRED - set in .env)
    # Auto-generated by 'lem doctor' or 'lem campaign', or set manually
    - PUID=${PUID:?Run 'lem doctor' to auto-generate .env, or set PUID manually}
    - PGID=${PGID:?Run 'lem doctor' to auto-generate .env, or set PGID manually}
    # Standard config
    - HF_TOKEN
    - CUDA_VISIBLE_DEVICES
    - HF_HOME=/app/.cache/huggingface
    - PIP_NO_CACHE_DIR=1
    - CODECARBON_LOG_LEVEL=warning
    - NVIDIA_DISABLE_REQUIRE=true
    # Container sees /app paths regardless of host mount source
    - LLM_ENERGY_RESULTS_DIR=/app/results
    - LLM_ENERGY_CONFIGS_DIR=/app/configs
    - LLM_ENERGY_STATE_DIR=/app/.state

  working_dir: /app

  healthcheck:
    test: ["CMD", "python", "-c", "import torch; assert torch.cuda.is_available()"]
    interval: 30s
    timeout: 10s
    retries: 3

services:
  # ==========================================================================
  # Base image - shared foundation (build this first)
  # ==========================================================================
  base:
    build:
      context: .
      dockerfile: docker/Dockerfile.base
    image: llenergymeasure-base:latest

  # ==========================================================================
  # PyTorch Backend - HuggingFace Transformers + Accelerate
  # ==========================================================================
  pytorch:
    <<: *common
    volumes:
      # Bind mounts: user-accessible data
      - ${LLM_ENERGY_CONFIGS_DIR:-./configs}:/app/configs
      - ${LLM_ENERGY_RESULTS_DIR:-./results}:/app/results
      - ./scripts:/app/scripts:ro
      # Named volumes: container-managed (no permission issues)
      - hf-cache:/app/.cache/huggingface
      - experiment-state:/app/.state
    build:
      context: .
      dockerfile: docker/Dockerfile.pytorch
      target: runtime
      args:
        BASE_IMAGE: llenergymeasure-base:latest
    image: llenergymeasure:pytorch
    depends_on:
      base:
        condition: service_completed_successfully

  pytorch-dev:
    <<: *common
    volumes:
      - .:/app
      - hf-cache:/app/.cache/huggingface
      - experiment-state:/app/.state
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.pytorch
      target: dev
      args:
        BASE_IMAGE: llenergymeasure-base:latest
    image: llenergymeasure:pytorch-dev
    depends_on:
      base:
        condition: service_completed_successfully
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # vLLM Backend - High-performance inference with PagedAttention etc
  # ==========================================================================
  vllm:
    <<: *common
    # vLLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      # Bind mounts: user-accessible data
      - ${LLM_ENERGY_CONFIGS_DIR:-./configs}:/app/configs
      - ${LLM_ENERGY_RESULTS_DIR:-./results}:/app/results
      - ./scripts:/app/scripts:ro
      # Named volumes: container-managed (no permission issues)
      - hf-cache:/app/.cache/huggingface
      - experiment-state:/app/.state
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm
      target: runtime
      args:
        BASE_IMAGE: llenergymeasure-base:latest
    image: llenergymeasure:vllm
    depends_on:
      base:
        condition: service_completed_successfully

  vllm-dev:
    <<: *common
    # vLLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - .:/app
      - hf-cache:/app/.cache/huggingface
      - experiment-state:/app/.state
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm
      target: dev
      args:
        BASE_IMAGE: llenergymeasure-base:latest
    image: llenergymeasure:vllm-dev
    depends_on:
      base:
        condition: service_completed_successfully
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # TensorRT-LLM Backend - High-performance compiled inference
  # ==========================================================================
  tensorrt:
    <<: *common
    # TensorRT-LLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      # Bind mounts: user-accessible data
      - ${LLM_ENERGY_CONFIGS_DIR:-./configs}:/app/configs
      - ${LLM_ENERGY_RESULTS_DIR:-./results}:/app/results
      - ./scripts:/app/scripts:ro
      # Named volumes: container-managed (no permission issues)
      - hf-cache:/app/.cache/huggingface
      - trt-engine-cache:/app/.cache/tensorrt-engines
      - experiment-state:/app/.state
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorrt
      target: runtime
      args:
        BASE_IMAGE: llenergymeasure-base:latest
    image: llenergymeasure:tensorrt
    depends_on:
      base:
        condition: service_completed_successfully

  tensorrt-dev:
    <<: *common
    # TensorRT-LLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - .:/app
      - hf-cache:/app/.cache/huggingface
      - trt-engine-cache:/app/.cache/tensorrt-engines
      - experiment-state:/app/.state
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorrt
      target: dev
      args:
        BASE_IMAGE: llenergymeasure-base:latest
    image: llenergymeasure:tensorrt-dev
    depends_on:
      base:
        condition: service_completed_successfully
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # Legacy aliases (backwards compatibility)
  # ==========================================================================
  llenergymeasure-app:
    extends:
      service: pytorch

  llenergymeasure-dev:
    extends:
      service: pytorch-dev
    profiles: ["dev"]

# =============================================================================
# Named Volumes (Docker-managed, persist across container restarts)
# =============================================================================
# These volumes store large caches and internal state.
# They persist unless you explicitly run: docker compose down -v
#
# To clear specific volumes:
#   docker volume rm llenergymeasure_hf-cache
#   docker volume rm llenergymeasure_experiment-state
#
volumes:
  # HuggingFace model cache (10-100+ GB)
  hf-cache:
    name: lem-hf-cache

  # TensorRT compiled engine cache (1-10+ GB)
  trt-engine-cache:
    name: lem-trt-engine-cache

  # Experiment state and progress tracking
  experiment-state:
    name: lem-experiment-state
