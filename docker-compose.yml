services:
  llm-energy-measure:
    build:
      context: .
      dockerfile: Dockerfile
    image: llm-energy-measure:latest

    # GPU access
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Volumes
    volumes:
      # Experiment configs (read-only)
      - ./configs:/app/configs:ro
      # Results output
      - ./results:/app/results
      # Optional: mount HF cache from host (requires correct permissions)
      # - ${HF_HOME:-~/.cache/huggingface}:/tmp/hf_cache

    # Environment variables
    environment:
      - HF_TOKEN
      - CUDA_VISIBLE_DEVICES
      - HF_HOME=/tmp/hf_cache
      - TRANSFORMERS_CACHE=/tmp/hf_cache
      - CODECARBON_LOG_LEVEL=warning

    # Working directory
    working_dir: /app

    # Default: show help
    command: ["--help"]
