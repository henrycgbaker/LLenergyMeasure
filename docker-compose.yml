# Docker Compose with multiple backend support
#
# Usage:
#   Build base first:  docker compose build base
#   PyTorch (default): docker compose run --rm pytorch llm-energy-measure --help
#   vLLM:              docker compose run --rm vllm llm-energy-measure --help
#   TensorRT:          docker compose run --rm tensorrt llm-energy-measure --help
#   Development:       docker compose --profile dev run --rm pytorch-dev /bin/bash
#
# Examples:
#   # PyTorch backend (default)
#   docker compose run --rm pytorch llm-energy-measure experiment configs/example_pytorch.yaml
#
#   # vLLM backend
#   docker compose run --rm vllm llm-energy-measure experiment configs/example_vllm.yaml
#
#   # TensorRT backend
#   docker compose run --rm tensorrt llm-energy-measure experiment configs/example_tensorrt.yaml
#   # Development with PyTorch
#   docker compose --profile dev run --rm pytorch-dev /bin/bash
#
#   # Development with vLLM
#   docker compose --profile dev run --rm vllm-dev /bin/bash
#
#   # Development with TensorRT
#   docker compose --profile dev run --rm tensorrt-dev /bin/bash

# Shared configuration anchor
x-common: &common
  # PUID/PGID pattern for host permission mapping (like LinuxServer.io)
  # Usage: PUID=$(id -u) PGID=$(id -g) docker compose run ...
  # Container starts as root, entrypoint drops to PUID/PGID user

  # GPU access with privileged mode for NVML energy metrics
  privileged: true
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu, utility]

  # Environment variables
  environment:
    # PUID/PGID for host permission mapping (entrypoint.sh handles these)
    # Pass these when running: PUID=$(id -u) PGID=$(id -g) docker compose run ...
    - PUID
    - PGID
    # Standard config
    - HF_TOKEN
    - CUDA_VISIBLE_DEVICES
    - HF_HOME=/app/.cache/huggingface
    - PIP_NO_CACHE_DIR=1
    - CODECARBON_LOG_LEVEL=warning
    - NVIDIA_DISABLE_REQUIRE=true
    # Container sees /app paths regardless of host mount source
    - LLM_ENERGY_RESULTS_DIR=/app/results
    - LLM_ENERGY_CONFIGS_DIR=/app/configs
    - LLM_ENERGY_STATE_DIR=/app/.state

  working_dir: /app

  healthcheck:
    test: ["CMD", "python", "-c", "import torch; assert torch.cuda.is_available()"]
    interval: 30s
    timeout: 10s
    retries: 3

services:
  # ==========================================================================
  # Base image - shared foundation (build this first)
  # ==========================================================================
  base:
    build:
      context: .
      dockerfile: docker/Dockerfile.base
    image: llm-energy-measure-base:latest

  # ==========================================================================
  # PyTorch Backend - HuggingFace Transformers + Accelerate
  # ==========================================================================
  pytorch:
    <<: *common
    volumes:
      - ${LLM_ENERGY_CONFIGS_DIR:-./configs}:/app/configs
      - ./scripts:/app/scripts:ro
      - ${LLM_ENERGY_RESULTS_DIR:-./results}:/app/results
      # Mount HF cache for gated models - avoids re-auth in child processes
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
    build:
      context: .
      dockerfile: docker/Dockerfile.pytorch
      target: runtime
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:pytorch
    depends_on:
      - base

  pytorch-dev:
    <<: *common
    volumes:
      - .:/app
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.pytorch
      target: dev
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:pytorch-dev
    depends_on:
      - base
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # vLLM Backend - High-performance inference with PagedAttention etc
  # ==========================================================================
  vllm:
    <<: *common
    # vLLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - ${LLM_ENERGY_CONFIGS_DIR:-./configs}:/app/configs
      - ./scripts:/app/scripts:ro
      - ${LLM_ENERGY_RESULTS_DIR:-./results}:/app/results
      # Mount HF cache for gated models - avoids re-auth in child processes
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm
      target: runtime
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:vllm
    depends_on:
      - base

  vllm-dev:
    <<: *common
    # vLLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - .:/app
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm
      target: dev
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:vllm-dev
    depends_on:
      - base
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # TensorRT-LLM Backend - High-performance compiled inference
  # ==========================================================================
  tensorrt:
    <<: *common
    # TensorRT-LLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - ${LLM_ENERGY_CONFIGS_DIR:-./configs}:/app/configs
      - ./scripts:/app/scripts:ro
      - ${LLM_ENERGY_RESULTS_DIR:-./results}:/app/results
      # Mount HF cache for gated models - avoids re-auth in child processes
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
      # Engine cache for compiled TensorRT engines
      - ${TRT_ENGINE_CACHE:-~/.cache/tensorrt-engines}:/app/.cache/tensorrt-engines
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorrt
      target: runtime
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:tensorrt
    depends_on:
      - base

  tensorrt-dev:
    <<: *common
    # TensorRT-LLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - .:/app
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
      # Engine cache for compiled TensorRT engines
      - ${TRT_ENGINE_CACHE:-~/.cache/tensorrt-engines}:/app/.cache/tensorrt-engines
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorrt
      target: dev
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:tensorrt-dev
    depends_on:
      - base
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # Legacy aliases (backwards compatibility)
  # ==========================================================================
  llm-energy-measure-app:
    extends:
      service: pytorch

  llm-energy-measure-dev:
    extends:
      service: pytorch-dev
    profiles: ["dev"]
