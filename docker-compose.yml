# Docker Compose with multiple backend support
#
# Usage:
#   Build base first:  docker compose build base
#   PyTorch (default): docker compose run --rm pytorch llm-energy-measure --help
#   vLLM:              docker compose run --rm vllm llm-energy-measure --help
#   TensorRT:          docker compose run --rm tensorrt llm-energy-measure --help
#   Development:       docker compose --profile dev run --rm pytorch-dev /bin/bash
#
# Examples:
#   # PyTorch backend (default)
#   docker compose run --rm pytorch llm-energy-measure experiment config.yaml -d alpaca -n 100
#
#   # vLLM backend
#   docker compose run --rm vllm llm-energy-measure experiment config.yaml -d alpaca -n 100 --backend vllm
#
#   # TensorRT backend
#   docker compose run --rm tensorrt llm-energy-measure experiment config.yaml -d alpaca -n 100 --backend tensorrt
#
#   # Development with PyTorch
#   docker compose --profile dev run --rm pytorch-dev /bin/bash
#
#   # Development with vLLM
#   docker compose --profile dev run --rm vllm-dev /bin/bash
#
#   # Development with TensorRT
#   docker compose --profile dev run --rm tensorrt-dev /bin/bash

# Shared configuration anchor
x-common: &common
  # Run as host user: DOCKER_UID=$(id -u) DOCKER_GID=$(id -g) docker compose ...
  # Defaults to root (0:0) for backwards compatibility
  user: "${DOCKER_UID:-0}:${DOCKER_GID:-0}"

  # GPU access with privileged mode for NVML energy metrics
  privileged: true
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu, utility]

  # Environment variables
  environment:
    - HF_TOKEN
    - CUDA_VISIBLE_DEVICES
    - HF_HOME=/app/.cache/huggingface
    - PIP_NO_CACHE_DIR=1
    - CODECARBON_LOG_LEVEL=warning
    - NVIDIA_DISABLE_REQUIRE=true

  working_dir: /app

  healthcheck:
    test: ["CMD", "python", "-c", "import torch; assert torch.cuda.is_available()"]
    interval: 30s
    timeout: 10s
    retries: 3

services:
  # ==========================================================================
  # Base image - shared foundation (build this first)
  # ==========================================================================
  base:
    build:
      context: .
      dockerfile: docker/Dockerfile.base
    image: llm-energy-measure-base:latest

  # ==========================================================================
  # PyTorch Backend - HuggingFace Transformers + Accelerate
  # ==========================================================================
  pytorch:
    <<: *common
    volumes:
      - ./configs:/app/configs:ro
      - ./scripts:/app/scripts:ro
      - ./results:/app/results
    build:
      context: .
      dockerfile: docker/Dockerfile.pytorch
      target: runtime
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:pytorch
    depends_on:
      - base

  pytorch-dev:
    <<: *common
    volumes:
      - .:/app
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.pytorch
      target: dev
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:pytorch-dev
    depends_on:
      - base
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # vLLM Backend - High-performance inference with PagedAttention
  # ==========================================================================
  vllm:
    <<: *common
    # vLLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - ./configs:/app/configs:ro
      - ./scripts:/app/scripts:ro
      - ./results:/app/results
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm
      target: runtime
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:vllm
    depends_on:
      - base

  vllm-dev:
    <<: *common
    # vLLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - .:/app
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.vllm
      target: dev
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:vllm-dev
    depends_on:
      - base
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # TensorRT-LLM Backend - High-performance compiled inference
  # ==========================================================================
  tensorrt:
    <<: *common
    # TensorRT-LLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - ./configs:/app/configs:ro
      - ./scripts:/app/scripts:ro
      - ./results:/app/results
      # Engine cache for compiled TensorRT engines
      - ${TRT_ENGINE_CACHE:-~/.cache/tensorrt-engines}:/app/.cache/tensorrt-engines
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorrt
      target: runtime
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:tensorrt
    depends_on:
      - base

  tensorrt-dev:
    <<: *common
    # TensorRT-LLM requires shared memory for multiprocessing
    ipc: host
    volumes:
      - .:/app
      - ${HF_HOME:-~/.cache/huggingface}:/app/.cache/huggingface
      # Engine cache for compiled TensorRT engines
      - ${TRT_ENGINE_CACHE:-~/.cache/tensorrt-engines}:/app/.cache/tensorrt-engines
    profiles: ["dev"]
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorrt
      target: dev
      args:
        BASE_IMAGE: llm-energy-measure-base:latest
    image: llm-energy-measure:tensorrt-dev
    depends_on:
      - base
    entrypoint: ["/app/scripts/dev-entrypoint.sh"]
    command: []

  # ==========================================================================
  # Legacy aliases (backwards compatibility)
  # ==========================================================================
  llm-energy-measure-app:
    extends:
      service: pytorch

  llm-energy-measure-dev:
    extends:
      service: pytorch-dev
    profiles: ["dev"]
