"""Sweep grid expansion and cycle ordering for study configurations."""

from __future__ import annotations

import hashlib
import itertools
import json
import logging
import random
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING, Any

import yaml
from pydantic import ValidationError

from llenergymeasure.config.models import ExperimentConfig
from llenergymeasure.exceptions import ConfigError

if TYPE_CHECKING:
    from llenergymeasure.config.models import StudyConfig

if sys.version_info >= (3, 11):
    from enum import StrEnum
else:
    from enum import Enum

    class StrEnum(str, Enum):
        """Backport of StrEnum for Python < 3.11."""


logger = logging.getLogger(__name__)

# Keys that belong to the study YAML structure, not to individual experiments.
# These are stripped from base: files and excluded from the fixed dict.
_STUDY_ONLY_KEYS = frozenset({"sweep", "experiments", "execution", "base", "name", "version"})


class CycleOrder(StrEnum):
    SEQUENTIAL = "sequential"
    INTERLEAVED = "interleaved"
    SHUFFLED = "shuffled"


@dataclass
class SkippedConfig:
    """An ExperimentConfig that failed Pydantic validation during grid expansion."""

    raw_config: dict[str, Any]
    reason: str
    errors: list[dict[str, Any]] = field(default_factory=list)

    @property
    def short_label(self) -> str:
        """Short label for display: 'backend, precision'."""
        backend = self.raw_config.get("backend", "unknown")
        precision = self.raw_config.get("precision", "?")
        return f"{backend}, {precision}"

    def to_dict(self) -> dict[str, Any]:
        """Serialise for StudyConfig.skipped_configs."""
        return {
            "raw_config": self.raw_config,
            "reason": self.reason,
            "short_label": self.short_label,
            "errors": self.errors,
        }


# =============================================================================
# Public API
# =============================================================================


def expand_grid(
    raw_study: dict[str, Any],
    study_yaml_path: Path | None = None,
) -> tuple[list[ExperimentConfig], list[SkippedConfig]]:
    """Expand sweep dimensions into a flat list of ExperimentConfig.

    Resolution order:
    1. Load base: file (optional DRY inheritance)
    2. Build fixed dict from non-sweep/non-experiments/non-execution/non-base/non-name keys
    3. Expand sweep: block into raw config dicts
    4. Append explicit experiments: list entries
    5. Pydantic-validate each raw dict, collecting valid + skipped

    Returns (valid_experiments, skipped_configs).
    Raises ConfigError if all configs are invalid or no experiments produced.
    """
    # Step 1: Load base: inheritance
    base_dict = _load_base(raw_study.get("base"), study_yaml_path)

    # Step 2: Fixed dict — experiment-level fields shared across all grid points
    fixed = _extract_fixed(raw_study)
    merged_fixed = {**base_dict, **fixed}  # inline fields override base

    # Step 3: Expand sweep: block into raw config dicts
    sweep = raw_study.get("sweep", {})
    sweep_raw_configs = _expand_sweep(sweep, merged_fixed)

    # Step 4: Append explicit experiments: list entries
    explicit_entries = raw_study.get("experiments", [])
    explicit_raw_configs = [{**merged_fixed, **exp} for exp in explicit_entries]

    all_raw_configs = sweep_raw_configs + explicit_raw_configs

    # Guard: no experiments produced at all
    if not all_raw_configs:
        raise ConfigError(
            "Study produced no experiments. "
            "Add a 'sweep:' block, 'experiments:' list, or inline 'model:' field."
        )

    # Step 5: Pydantic-validate each raw dict
    valid: list[ExperimentConfig] = []
    skipped: list[SkippedConfig] = []

    for raw_config in all_raw_configs:
        try:
            valid.append(ExperimentConfig(**raw_config))
        except (ValidationError, TypeError) as exc:
            reason = str(exc)
            errors: list[dict[str, Any]] = []
            if isinstance(exc, ValidationError):
                errors = [dict(e) for e in exc.errors()]
            skipped.append(SkippedConfig(raw_config=raw_config, reason=reason, errors=errors))

    total = len(valid) + len(skipped)

    # Guard: all configs invalid
    if len(valid) == 0:
        first_reasons = "; ".join(s.reason[:120] for s in skipped[:5])
        raise ConfigError(
            f"nothing to run — all {total} generated config(s) are invalid. "
            f"First failures: {first_reasons}"
        )

    # Warning: >50% skip rate
    if total > 0 and len(skipped) / total > 0.5:
        logger.warning(
            "Most of your sweep is invalid (%d/%d configs skipped). "
            "Check your config combinations.",
            len(skipped),
            total,
        )
        for s in skipped:
            logger.warning("  Skipped (%s): %s", s.short_label, s.reason[:200])

    return valid, skipped


def compute_study_design_hash(experiments: list[ExperimentConfig]) -> str:
    """SHA-256[:16] of the resolved experiment list (execution block excluded).

    Deterministic: uses json.dumps with sort_keys=True. Identical experiment lists
    produce the same hash across calls and interpreter restarts.
    """
    canonical = json.dumps([exp.model_dump() for exp in experiments], sort_keys=True)
    return hashlib.sha256(canonical.encode()).hexdigest()[:16]


def apply_cycles(
    experiments: list[ExperimentConfig],
    n_cycles: int,
    cycle_order: CycleOrder,
    study_design_hash: str,
    shuffle_seed: int | None = None,
) -> list[ExperimentConfig]:
    """Return the ordered execution sequence for n_cycles repetitions.

    sequential:   [A, A, A, B, B, B]  — all cycles of each experiment together
    interleaved:  [A, B, A, B, A, B]  — one cycle of each experiment, repeated
    shuffled:     random per-cycle order, seeded from study_design_hash by default
    """
    if cycle_order == CycleOrder.SEQUENTIAL:
        return [exp for exp in experiments for _ in range(n_cycles)]

    if cycle_order == CycleOrder.INTERLEAVED:
        return experiments * n_cycles

    # shuffled
    seed = shuffle_seed if shuffle_seed is not None else int(study_design_hash, 16) & 0xFFFFFFFF
    rng = random.Random(seed)
    result: list[ExperimentConfig] = []
    for _ in range(n_cycles):
        cycle = list(experiments)
        rng.shuffle(cycle)
        result.extend(cycle)
    return result


def format_preflight_summary(
    study_config: StudyConfig,
    skipped: list[SkippedConfig] | None = None,
) -> str:
    """Return pre-flight display string for terminal output.

    Format (CONTEXT.md locked):
        Study [abc123de]: 4 configs x 3 cycles = 12 runs
        Order: interleaved
        Skipping 2/6: (per-skip log line with reason)
          - pytorch, fp32: [Pydantic message]
        WARNING: 67% of sweep configs are invalid — check your sweep dimensions.

    Args:
        study_config: Resolved StudyConfig (after load_study_config).
        skipped: Optional list of SkippedConfig from expand_grid.
            If None, derives skip info from study_config.skipped_configs.

    Returns:
        Multi-line string for terminal display.
    """
    n_cycles = study_config.execution.n_cycles
    n_runs = len(study_config.experiments)
    # n_configs is the unique config count (before cycle multiplication)
    n_configs = n_runs // n_cycles if n_cycles > 0 else n_runs
    hash_display = study_config.study_design_hash or "unknown"

    lines = [
        f"Study [{hash_display}]: {n_configs} configs x {n_cycles} cycles = {n_runs} runs",
        f"Order: {study_config.execution.cycle_order}",
    ]

    # Handle skipped configs display
    skipped_dicts = (
        study_config.skipped_configs if skipped is None else [s.to_dict() for s in skipped]
    )
    if skipped_dicts:
        total_generated = n_configs + len(skipped_dicts)
        lines.append(f"Skipping {len(skipped_dicts)}/{total_generated}:")
        for s in skipped_dicts:
            label = s.get("short_label", "unknown")
            reason = s.get("reason", "unknown error")
            lines.append(f"  - {label}: {reason}")

        skip_rate = len(skipped_dicts) / total_generated if total_generated > 0 else 0
        if skip_rate > 0.5:
            lines.append(
                f"  WARNING: {skip_rate:.0%} of sweep configs are invalid "
                "-- check your sweep dimensions."
            )

    return "\n".join(lines)


# =============================================================================
# Private helpers
# =============================================================================


def _extract_fixed(raw_study: dict[str, Any]) -> dict[str, Any]:
    """Return experiment-level fields from raw_study (all keys except study-only ones)."""
    return {k: v for k, v in raw_study.items() if k not in _STUDY_ONLY_KEYS}


def _load_base(base_path_str: str | None, study_yaml_path: Path | None) -> dict[str, Any]:
    """Load a base experiment config file, stripping study-only keys.

    Path is resolved relative to the study YAML file's directory.
    Hard error (ConfigError) if the file does not exist.
    """
    if base_path_str is None:
        return {}

    base_path = Path(base_path_str)
    if not base_path.is_absolute() and study_yaml_path is not None:
        base_path = study_yaml_path.parent / base_path

    if not base_path.exists():
        raise ConfigError(
            f"base: file not found: {base_path}. "
            "Path is resolved relative to the study YAML file's directory."
        )

    with base_path.open() as fh:
        raw = yaml.safe_load(fh) or {}

    # Strip study-only keys — base: accepts experiment config files only
    return {k: v for k, v in raw.items() if k not in _STUDY_ONLY_KEYS}


def _expand_sweep(sweep: dict[str, Any], fixed: dict[str, Any]) -> list[dict[str, Any]]:
    """Expand a sweep: block into a flat list of raw experiment config dicts.

    Dotted keys (e.g. pytorch.batch_size) are backend-scoped dimensions.
    Non-dotted keys are universal dimensions applied to all backends.

    If the sweep is empty and fixed has a 'model' key, return [fixed] (single experiment).
    If the sweep is empty and no model, return [] (no experiments).
    """
    if not sweep:
        if fixed.get("model"):
            return [dict(fixed)]
        return []

    # Separate universal dims from backend-scoped dims
    universal_dims: dict[str, list[Any]] = {}
    scoped_dims: dict[str, dict[str, list[Any]]] = {}  # {backend: {param: [values]}}

    for key, values in sweep.items():
        if not isinstance(values, list):
            values = [values]
        if "." in key:
            backend_name, param = key.split(".", 1)
            scoped_dims.setdefault(backend_name, {})[param] = values
        else:
            universal_dims[key] = values

    # Determine which backends to iterate over
    fixed_backend = fixed.get("backend", "pytorch")
    if isinstance(fixed_backend, list):
        backends = list(fixed_backend)
    elif scoped_dims:
        backends = list(scoped_dims.keys())
    else:
        backends = [fixed_backend]

    results: list[dict[str, Any]] = []

    for backend in backends:
        # Combine universal dims with this backend's scoped dims
        backend_scoped = scoped_dims.get(backend, {})
        all_dim_keys = list(universal_dims.keys()) + list(backend_scoped.keys())
        all_dim_values = list(universal_dims.values()) + list(backend_scoped.values())

        if not all_dim_keys:
            # No dimensions for this backend — produce one config
            config_dict: dict[str, Any] = dict(fixed)
            # Remove list-valued backend field
            config_dict["backend"] = backend
            results.append(config_dict)
            continue

        for combo in itertools.product(*all_dim_values):
            config_dict = dict(fixed)
            # Override list-valued backend with the specific backend string
            config_dict["backend"] = backend

            for dim_key, value in zip(all_dim_keys, combo, strict=False):
                if dim_key in backend_scoped:
                    # Backend-scoped parameter: goes into config_dict[backend][param]
                    config_dict.setdefault(backend, {})[dim_key] = value
                else:
                    # Universal parameter: goes at top level
                    config_dict[dim_key] = value

            results.append(config_dict)

    return results
