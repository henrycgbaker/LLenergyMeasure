config_name: "pytorch-streaming-test"
model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
backend: pytorch
num_input_prompts: 10
max_output_tokens: 64
streaming: true
streaming_warmup_requests: 2
