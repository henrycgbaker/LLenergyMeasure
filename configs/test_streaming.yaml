# Test config for streaming latency measurement
config_name: test-pytorch-streaming
model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
backend: pytorch

# Enable streaming for TTFT/ITL measurement
streaming: true
streaming_warmup_requests: 2

# Small sample for quick test
num_input_prompts: 15
max_output_tokens: 32
max_input_tokens: 128

# Deterministic for reproducibility
fp_precision: float16
decoder:
  preset: deterministic

# Single GPU
gpus: [1]  # Use GPU 1 which has most free memory
