config_name: vllm_base_streaming_False
model_name: Qwen/Qwen2.5-0.5B
backend: vllm
gpus:
- 0
max_input_tokens: 64
max_output_tokens: 32
num_input_prompts: 5
fp_precision: float16
decoder:
  preset: deterministic
dataset:
  name: ai_energy_score
  sample_size: 5
vllm:
  max_num_seqs: 64
  gpu_memory_utilization: 0.7
  max_model_len: 512
streaming: false
