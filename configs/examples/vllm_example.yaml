schema_version: '3.0.0'
config_name: vllm-comprehensive

model_name: Qwen/Qwen2.5-0.5B
backend: vllm
gpus: [0, 1, 2, 3]

max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 100

fp_precision: float16
random_seed: 42

dataset:
  name: ai_energy_score
  sample_size: 100

decoder:
  preset: deterministic

# vLLM-specific optimizations
vllm:
  max_num_seqs: 256
  max_model_len: 1024
  gpu_memory_utilization: 0.9
  tensor_parallel_size: 2
  enable_prefix_caching: true
  enable_chunked_prefill: true
  enforce_eager: true  # Required for tensor parallel
  kv_cache_dtype: auto
  block_size: 16

# Measurement settings
warmup:
  enabled: true
  convergence_detection: true
  min_prompts: 5
  cv_threshold: 0.05

baseline:
  enabled: true
  cache_ttl_sec: 3600

timeseries:
  enabled: true
  sample_interval_ms: 200

# See docs/backends.md for vLLM tuning guide
