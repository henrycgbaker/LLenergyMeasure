# =============================================================================
# FULL vLLM Backend Configuration
# =============================================================================
# Complete reference for all available parameters when using backend: vllm
# Copy this file and modify for your experiments.
#
# REQUIREMENTS:
#   - NVIDIA GPU (any modern GPU with sufficient VRAM)
#   - Linux only (vLLM does not support Windows/macOS)
#   - Install: pip install lem[vllm]
#   - WARNING: Do not install alongside [tensorrt] - dependency conflicts
#
# ARCHITECTURE NOTE:
#   This config follows "backend-native" architecture where:
#   - Tier 1 (top-level): Universal params with identical semantics across backends
#   - Tier 2 (vllm section): vLLM-specific params using native vLLM naming
# =============================================================================

# =============================================================================
# TIER 1: UNIVERSAL CONFIGURATION
# =============================================================================
# These parameters have identical semantics across all backends.

# -----------------------------------------------------------------------------
# REQUIRED: Experiment Identity
# -----------------------------------------------------------------------------
config_name: vllm-full-example
model_name: Qwen/Qwen2.5-0.5B  # Non-gated. For Llama: meta-llama/Llama-2-7b-hf (requires HF_TOKEN)

# Optional: LoRA adapter (HuggingFace Hub ID or local path)
# adapter: tloen/alpaca-lora-7b  # Alpaca instruction-tuned adapter for Llama-2-7b

# -----------------------------------------------------------------------------
# DATASET CONFIGURATION
# -----------------------------------------------------------------------------
# Built-in aliases: ai_energy_score, alpaca, sharegpt, gsm8k, mmlu
# Or use any HuggingFace dataset path (e.g., tatsu-lab/alpaca)
dataset:
  name: ai_energy_score
  sample_size: 100
  split: train
  # column: text  # Auto-detected if not set

# Alternative: Advanced prompt configuration (mutually exclusive with dataset)
# prompts:
#   type: huggingface
#   dataset: tatsu-lab/alpaca
#   split: train
#   subset: null
#   column: instruction
#   sample_size: 100
#   shuffle: false
#   seed: 42

# -----------------------------------------------------------------------------
# TOKEN LIMITS
# -----------------------------------------------------------------------------
max_input_tokens: 512
max_output_tokens: 128
min_output_tokens: 0

# -----------------------------------------------------------------------------
# GPU CONFIGURATION
# -----------------------------------------------------------------------------
gpus: [0, 1]

# -----------------------------------------------------------------------------
# PRECISION
# -----------------------------------------------------------------------------
fp_precision: float16  # float32 | float16 | bfloat16

# -----------------------------------------------------------------------------
# BACKEND
# -----------------------------------------------------------------------------
backend: vllm

# -----------------------------------------------------------------------------
# UNIVERSAL DECODER SETTINGS
# -----------------------------------------------------------------------------
# These sampling params have identical semantics across all backends.
# Backend-specific decoder extensions (min_p, best_of, logprobs) are
# in the vllm section below.
#
# top_k (Universal):
#   All backends support top_k with the same semantics. The "disabled"
#   convention differs: PyTorch/TensorRT use 0, vLLM uses -1.
#   Set top_k=0 to disable; the vLLM backend converts this to -1 internally.
decoder:
  preset: standard  # deterministic | standard | creative | factual | null

  # Individual params (only meaningful when preset: null, or to override preset)
  temperature: 0.7
  do_sample: true
  top_k: 40         # Top-k sampling (0 = disabled) - UNIVERSAL across all backends
  top_p: 0.9
  repetition_penalty: 1.1

# -----------------------------------------------------------------------------
# TRAFFIC SIMULATION
# -----------------------------------------------------------------------------
traffic_simulation:
  enabled: true
  mode: poisson
  target_qps: 10.0
  seed: 42

# -----------------------------------------------------------------------------
# STREAMING
# -----------------------------------------------------------------------------
streaming: true
streaming_warmup_requests: 10

# -----------------------------------------------------------------------------
# SCHEDULE
# -----------------------------------------------------------------------------
schedule:
  enabled: false
  interval: null
  at: null
  days: null
  total_duration: 24h

# -----------------------------------------------------------------------------
# I/O CONFIGURATION
# -----------------------------------------------------------------------------
io:
  results_dir: null

# -----------------------------------------------------------------------------
# EXPERIMENT TRACKING
# -----------------------------------------------------------------------------
num_cycles: 3
query_rate: 10.0
random_seed: 42
save_outputs: true
decode_token_to_text: true

# -----------------------------------------------------------------------------
# EXTRA METADATA
# -----------------------------------------------------------------------------
extra_metadata: {}

# =============================================================================
# TIER 2: VLLM-SPECIFIC CONFIGURATION
# =============================================================================
# All vLLM-specific parameters using native vLLM naming conventions.
# These map directly to vLLM LLM() constructor and SamplingParams.
#
# Key differences from PyTorch:
# - Continuous batching: vLLM manages batching internally via max_num_seqs
# - Parallelism: Use tensor_parallel_size/pipeline_parallel_size directly
# - No BitsAndBytes: Use vLLM-native quantization (awq, gptq, fp8, etc.)

vllm:
  # ===========================================================================
  # Memory & Concurrency
  # ===========================================================================
  # Native LLM() constructor parameters
  max_num_seqs: 128  # Maximum concurrent sequences per iteration
  max_num_batched_tokens: 8192  # Maximum tokens per iteration (null=auto)
  gpu_memory_utilization: 0.9  # Fraction of GPU memory for KV cache
  swap_space: 8.0  # CPU swap space per GPU in GiB
  cpu_offload_gb: 0.0  # CPU memory for weight offloading in GiB

  # ===========================================================================
  # KV Cache Configuration
  # ===========================================================================
  enable_prefix_caching: true  # Automatic prefix caching (30-50% throughput gain)
  enable_chunked_prefill: true  # Chunk large prefills with decode requests
  kv_cache_dtype: auto  # auto | float16 | bfloat16 | fp8
  block_size: 16  # KV cache block size: 8 | 16 | 32

  # ===========================================================================
  # Context & Sequence Length
  # ===========================================================================
  max_model_len: 2048  # Maximum context length (null = model's native max)
  max_seq_len_to_capture: null  # Maximum length for CUDA graph capture

  # ===========================================================================
  # Execution Mode
  # ===========================================================================
  enforce_eager: false  # Disable CUDA graphs (for debugging)

  # ===========================================================================
  # Parallelism
  # ===========================================================================
  # Native vLLM parameters - use these instead of shared parallelism config
  tensor_parallel_size: 2  # Split layers horizontally across GPUs
  pipeline_parallel_size: 1  # Pipeline stages (for very large models)
  distributed_backend: ray  # mp (multiprocessing) | ray (Ray cluster)
  disable_custom_all_reduce: false

  # ===========================================================================
  # Attention Configuration
  # ===========================================================================
  attention: null
  # attention:
  #   backend: auto  # auto | FLASH_ATTN | FLASHINFER | TORCH_SDPA
  #   flash_version: null  # 2 | 3 (3 for H100/Hopper GPUs)
  #   disable_sliding_window: false

  # ===========================================================================
  # Speculative Decoding
  # ===========================================================================
  speculative: null
  # speculative:
  #   model: null  # Draft model name/path
  #   num_tokens: 5  # Tokens to speculate per step (1-10)
  #   method: ngram  # ngram | eagle | eagle3 | medusa | mlp | lookahead
  #   prompt_lookup_min: 1  # Min n-gram window for ngram method
  #   prompt_lookup_max: null  # Max n-gram window
  #   draft_tp_size: 1  # Tensor parallel size for draft model

  # ===========================================================================
  # LoRA Adapter Configuration
  # ===========================================================================
  lora:
    enabled: false
    max_loras: 1  # Maximum concurrent LoRA adapters
    max_rank: 16  # Maximum LoRA rank supported
    extra_vocab_size: 256

  # ===========================================================================
  # Quantization (vLLM-native methods)
  # ===========================================================================
  # Note: vLLM does NOT use BitsAndBytes. Use these native methods instead.
  quantization: null  # awq | gptq | fp8 | marlin | bitsandbytes | squeezellm
  load_format: auto  # auto | pt | safetensors | gguf

  # ===========================================================================
  # Decoder Extensions (vLLM-specific)
  # ===========================================================================
  # Native SamplingParams extensions
  # Note: top_k is now in the universal decoder section above.
  #   vLLM uses -1 for disabled (we convert from decoder.top_k=0 internally)
  min_p: 0.05  # Min probability sampling
  best_of: 2  # Generate N sequences, return best (requires swap_space)
  logprobs: 5  # Return top-k log probabilities per token (1-20)
  logit_bias: null  # Per-token logit adjustments {token_id: bias}

  # ===========================================================================
  # Escape Hatch
  # ===========================================================================
  extra: {}  # Additional kwargs passed directly to vLLM LLM()
