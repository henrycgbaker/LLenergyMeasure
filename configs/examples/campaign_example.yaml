schema_version: '3.0.0'
campaign_name: backend-comparison

# Model to benchmark across backends
model: Qwen/Qwen2.5-0.5B

# Dataset for all experiments
dataset: ai_energy_score
num_samples: 100

# Grid expansion - generates configs for each combination
# Note: TensorRT requires Ampere+ GPU (A100, RTX 30xx/40xx, etc.)
grid:
  backends: [pytorch, vllm, tensorrt]
  axes:
    batch_size: [1, 4]
    fp_precision: [float16]

# Execution settings
execution:
  cycles: 3  # Repeat for statistical robustness
  structure: interleaved  # Fair comparison ordering
  warmup_prompts: 5
  warmup_timeout_seconds: 30
  config_gap_seconds: 60  # Thermal recovery between configs
  cycle_gap_seconds: 300  # Full thermal reset between cycles

# Cold start benchmarking (optional)
cold_start:
  force_cold_start: false  # Set true for cold-start benchmarks

# Daemon mode (optional, for scheduled benchmarks)
# daemon:
#   enabled: true
#   at: "02:00"  # Run at 2 AM
#   interval: "24h"

# Results grouping for summary output (display-only, doesn't affect stored results)
# At campaign completion, groups results by specified fields and computes
# bootstrap confidence intervals per group for comparison.
# Individual experiment results are stored unchanged - grouping is post-hoc analysis.
group_by:
  - backend  # Compare pytorch vs vllm vs tensorrt

# Other useful groupings:
# group_by: [backend, batch_size]  # Cross-comparison matrix
# group_by: [fp_precision]         # Precision impact analysis
# group_by: [model]                # Model comparison (if grid.models set)

# CLI override: lem campaign campaign.yaml --group-by backend,batch_size

# See docs/cli.md for campaign configuration reference
