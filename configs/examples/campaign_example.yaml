schema_version: '3.0.0'
campaign_name: backend-comparison

# Model to benchmark across backends
model: Qwen/Qwen2.5-0.5B

# Dataset for all experiments
dataset: ai_energy_score
num_samples: 100

# Grid expansion - generates configs for each combination
grid:
  backends: [pytorch, vllm]
  axes:
    batch_size: [1, 4, 8]
    fp_precision: [float16, bfloat16]

# Execution settings
execution:
  cycles: 3  # Repeat for statistical robustness
  structure: interleaved  # Fair comparison ordering
  warmup_prompts: 5
  warmup_timeout_seconds: 30
  config_gap_seconds: 60  # Thermal recovery between configs
  cycle_gap_seconds: 300  # Full thermal reset between cycles

# Cold start benchmarking (optional)
cold_start:
  force_cold_start: false  # Set true for cold-start benchmarks

# Daemon mode (optional, for scheduled benchmarks)
# daemon:
#   enabled: true
#   at: "02:00"  # Run at 2 AM
#   interval: "24h"

# Results grouping (for summary output)
# Use with: lem campaign campaign.yaml --group-by backend,batch_size

# See docs/cli.md for campaign configuration reference
