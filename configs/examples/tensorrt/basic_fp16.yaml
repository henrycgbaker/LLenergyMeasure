# TensorRT-LLM basic FP16 configuration
#
# This is a minimal TensorRT config that builds an engine from a HuggingFace
# checkpoint with FP16 precision. The engine will be cached for future runs.
#
# Usage:
#   llm-energy-measure experiment configs/examples/tensorrt/basic_fp16.yaml -n 10

config_name: tensorrt_basic_fp16
model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
backend: tensorrt

max_input_tokens: 256
max_output_tokens: 64
num_input_prompts: 10
gpus: [0]

fp_precision: float16

decoder:
  preset: deterministic

# TensorRT-specific configuration
tensorrt:
  # Build configuration
  max_batch_size: 8
  builder_opt_level: 3

  # Quantization (none = FP16)
  quantization:
    method: none

  # Runtime configuration
  kv_cache_type: paged
  enable_chunked_context: true

# I/O paths (optional - defaults to ./results or LLM_ENERGY_RESULTS_DIR env var)
# io:
#   results_dir: /path/to/results
