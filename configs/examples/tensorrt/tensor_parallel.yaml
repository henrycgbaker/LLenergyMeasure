# TensorRT-LLM tensor parallel configuration for multi-GPU inference
#
# Splits model layers horizontally across multiple GPUs for larger models
# or improved throughput. TP size must be power of 2.
#
# Usage:
#   llm-energy-measure experiment configs/examples/tensorrt/tensor_parallel.yaml -n 50

config_name: tensorrt_tensor_parallel
model_name: meta-llama/Llama-2-70b-hf
backend: tensorrt

max_input_tokens: 2048
max_output_tokens: 256
num_input_prompts: 50
gpus: [0, 1, 2, 3]

fp_precision: float16

# Sharding configuration (used by TensorRT for TP)
sharding:
  strategy: tensor_parallel
  num_shards: 4

decoder:
  preset: deterministic

# TensorRT-specific configuration
tensorrt:
  # Build configuration
  max_batch_size: 16
  builder_opt_level: 3

  # Tensor parallelism (if not using sharding config)
  # tp_size: 4  # Alternative to sharding.num_shards

  # FP8 for optimal performance on Hopper
  quantization:
    method: fp8

  # Runtime configuration
  kv_cache_type: paged
  enable_chunked_context: true
  gpu_memory_utilization: 0.9
