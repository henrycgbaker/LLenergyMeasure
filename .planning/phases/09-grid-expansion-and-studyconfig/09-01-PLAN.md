---
phase: 09-grid-expansion-and-studyconfig
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/study/__init__.py
  - src/llenergymeasure/study/grid.py
  - tests/unit/test_study_grid.py
autonomous: true
requirements: [CFG-11, CFG-13, CFG-14, CFG-15, CFG-16]

must_haves:
  truths:
    - "ExecutionConfig validates n_cycles >= 1, cycle_order literal, gap fields, shuffle_seed"
    - "StudyConfig holds list[ExperimentConfig] + ExecutionConfig + study_design_hash + skipped_configs"
    - "expand_grid() resolves sweep: block into Cartesian product of ExperimentConfig objects"
    - "Dotted notation pytorch.batch_size routes to backend section dict, not top-level"
    - "Three modes work: grid sweep only, explicit experiments: list only, combined sweep + explicit"
    - "apply_cycles() produces correct ordering for sequential, interleaved, and shuffled"
    - "compute_study_design_hash() returns stable 16-char hex, excludes execution block"
    - "Invalid ExperimentConfig combinations are collected as SkippedConfig, not raised"
    - "base: file loaded relative to study YAML path, study-only keys stripped"
    - "All configs invalid raises ConfigError; >50% skip rate triggers warning"
  artifacts:
    - path: "src/llenergymeasure/config/models.py"
      provides: "ExecutionConfig model, expanded StudyConfig model"
      contains: "class ExecutionConfig"
    - path: "src/llenergymeasure/study/__init__.py"
      provides: "study package init"
    - path: "src/llenergymeasure/study/grid.py"
      provides: "expand_grid, apply_cycles, compute_study_design_hash, SkippedConfig, CycleOrder"
      exports: ["expand_grid", "apply_cycles", "compute_study_design_hash", "SkippedConfig", "CycleOrder"]
    - path: "tests/unit/test_study_grid.py"
      provides: "Unit tests for grid expansion, cycle ordering, hash, invalid handling, base: resolution"
  key_links:
    - from: "src/llenergymeasure/study/grid.py"
      to: "src/llenergymeasure/config/models.py"
      via: "imports ExperimentConfig for Pydantic validation of expanded dicts"
      pattern: "from llenergymeasure\\.config\\.models import ExperimentConfig"
    - from: "src/llenergymeasure/study/grid.py"
      to: "src/llenergymeasure/exceptions.py"
      via: "raises ConfigError on base: file not found, all-invalid"
      pattern: "from llenergymeasure\\.exceptions import ConfigError"
---

<objective>
Implement the Pydantic config models (ExecutionConfig, expanded StudyConfig) and the pure grid expansion module (study/grid.py) with full TDD coverage.

Purpose: This is the data-transformation foundation of Phase 9 — YAML sweep dictionaries become flat lists of validated ExperimentConfig objects. All downstream phases (manifest, subprocess runner, CLI wiring) depend on these models and the expand_grid() function.

Output: ExecutionConfig and StudyConfig models in config/models.py; study/ package with grid.py containing expand_grid(), apply_cycles(), compute_study_design_hash(); comprehensive unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/09-grid-expansion-and-studyconfig/09-CONTEXT.md
@.planning/phases/09-grid-expansion-and-studyconfig/09-RESEARCH.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/config/models.py (existing M1 stub to replace):
```python
class StudyConfig(BaseModel):
    """M1 stub — Phase 9 replaces this with full implementation."""
    model_config = {"extra": "forbid"}
    experiments: list[ExperimentConfig] = Field(..., min_length=1)
    name: str | None = Field(default=None)
```

From src/llenergymeasure/config/models.py (ExperimentConfig — key fields for grid expansion):
```python
class ExperimentConfig(BaseModel):
    model_config = {"extra": "forbid"}
    model: str = Field(..., min_length=1)
    backend: Literal["pytorch", "vllm", "tensorrt"] = Field(default="pytorch")
    n: int = Field(default=100, ge=1)
    dataset: str | SyntheticDatasetConfig = Field(default="aienergyscore")
    precision: Literal["fp32", "fp16", "bf16"] = Field(default="bf16")
    # Backend sections (None = use backend's own defaults)
    pytorch: "PyTorchConfig | None" = Field(default=None)
    vllm: "VLLMConfig | None" = Field(default=None)
    tensorrt: "TensorRTConfig | None" = Field(default=None)
    # ... cross-validator: validate_backend_section_match
```

From src/llenergymeasure/domain/experiment.py (hash pattern to replicate):
```python
def compute_measurement_config_hash(config: ExperimentConfig) -> str:
    canonical = json.dumps(config.model_dump(), sort_keys=True)
    return hashlib.sha256(canonical.encode()).hexdigest()[:16]
```

From src/llenergymeasure/exceptions.py:
```python
class ConfigError(LLEMError): ...
```

From src/llenergymeasure/config/loader.py (existing helpers — DO NOT reuse _unflatten for sweep):
```python
def _load_file(path: Path | str) -> dict[str, Any]: ...
def deep_merge(base: dict[str, Any], overlay: dict[str, Any]) -> dict[str, Any]: ...
def _unflatten(flat: dict[str, Any]) -> dict[str, Any]: ...  # CLI overrides only, NOT sweep
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ExecutionConfig + expand StudyConfig models, create study/ package with grid.py, and write failing tests</name>
  <files>
    src/llenergymeasure/config/models.py
    src/llenergymeasure/study/__init__.py
    src/llenergymeasure/study/grid.py
    tests/unit/test_study_grid.py
  </files>
  <action>
**Step 1: Add ExecutionConfig and expand StudyConfig in config/models.py**

Replace the M1 `StudyConfig` stub (at the bottom of the file, after `_rebuild_experiment_config()`) with:

1. `ExecutionConfig(BaseModel)`:
   - `model_config = {"extra": "forbid"}`
   - `n_cycles: int = Field(default=1, ge=1)` — Pydantic default=1 (CLI sets effective default=3 in Phase 12)
   - `cycle_order: Literal["sequential", "interleaved", "shuffled"] = Field(default="sequential")` — sequential is the conservative Pydantic default; CLI Phase 12 can default to interleaved
   - `config_gap_seconds: float | None = Field(default=None, ge=0.0)` — None = use user config machine default
   - `cycle_gap_seconds: float | None = Field(default=None, ge=0.0)` — None = use user config machine default
   - `shuffle_seed: int | None = Field(default=None)` — None = derived from study_design_hash

2. Expand `StudyConfig(BaseModel)`:
   - Keep: `experiments: list[ExperimentConfig] = Field(..., min_length=1)`, `name: str | None`
   - Add: `execution: ExecutionConfig = Field(default_factory=ExecutionConfig)`
   - Add: `study_design_hash: str | None = Field(default=None)` — set by loader after expansion
   - Add: `skipped_configs: list[dict] = Field(default_factory=list)` — invalid combos for post-hoc review
   - Keep: `model_config = {"extra": "forbid"}`

**Step 2: Create study/ package**

Create `src/llenergymeasure/study/__init__.py` with minimal exports:
```python
"""Study module — sweep expansion, cycle ordering, manifest, runner."""
```

Create `src/llenergymeasure/study/grid.py` with stubs (function signatures + docstrings + `raise NotImplementedError`):

```python
"""Sweep grid expansion and cycle ordering for study configurations."""

from __future__ import annotations

import hashlib
import itertools
import json
import random
from dataclasses import dataclass, field
from enum import StrEnum
from pathlib import Path
from typing import Any

import yaml
from pydantic import ValidationError

from llenergymeasure.config.models import ExperimentConfig
from llenergymeasure.exceptions import ConfigError


class CycleOrder(StrEnum):
    SEQUENTIAL = "sequential"
    INTERLEAVED = "interleaved"
    SHUFFLED = "shuffled"


@dataclass
class SkippedConfig:
    """An ExperimentConfig that failed Pydantic validation during grid expansion."""
    raw_config: dict[str, Any]
    reason: str
    errors: list[dict[str, Any]] = field(default_factory=list)

    @property
    def short_label(self) -> str:
        """Short label for display: 'backend, precision, key=value'."""
        backend = self.raw_config.get("backend", "unknown")
        precision = self.raw_config.get("precision", "?")
        return f"{backend}, {precision}"

    def to_dict(self) -> dict[str, Any]:
        """Serialise for StudyConfig.skipped_configs."""
        return {
            "raw_config": self.raw_config,
            "reason": self.reason,
            "short_label": self.short_label,
            "errors": self.errors,
        }


def expand_grid(
    raw_study: dict[str, Any],
    study_yaml_path: Path | None = None,
) -> tuple[list[ExperimentConfig], list[SkippedConfig]]:
    """Expand sweep dimensions into a flat list of ExperimentConfig."""
    raise NotImplementedError


def compute_study_design_hash(experiments: list[ExperimentConfig]) -> str:
    """SHA-256[:16] of the resolved experiment list (execution block excluded)."""
    raise NotImplementedError


def apply_cycles(
    experiments: list[ExperimentConfig],
    n_cycles: int,
    cycle_order: CycleOrder,
    study_design_hash: str,
    shuffle_seed: int | None = None,
) -> list[ExperimentConfig]:
    """Return the ordered execution sequence for n_cycles repetitions."""
    raise NotImplementedError
```

**Step 3: Write comprehensive failing tests in tests/unit/test_study_grid.py**

Write tests covering ALL the following behaviours (RED phase — all must fail initially):

1. **ExecutionConfig model tests:**
   - Default values (n_cycles=1, cycle_order="sequential", gaps=None, shuffle_seed=None)
   - Validation: n_cycles=0 raises ValidationError
   - extra="forbid": unknown fields raise ValidationError

2. **StudyConfig model tests:**
   - Accepts experiments + execution + study_design_hash + skipped_configs
   - min_length=1 on experiments: empty list raises ValidationError

3. **expand_grid() tests — grid sweep mode (CFG-13, CFG-14):**
   - Universal sweep: `{"model": "gpt2", "backend": "pytorch", "sweep": {"precision": ["fp16", "bf16"], "n": [50, 100]}}` → 4 ExperimentConfig objects
   - Backend-scoped sweep: `{"model": "gpt2", "sweep": {"pytorch.batch_size": [1, 8]}}` → 2 configs with `pytorch.batch_size` routed to the `pytorch` section dict (not top-level)
   - Multi-backend scoped: `{"model": "gpt2", "backend": ["pytorch", "vllm"], "sweep": {"precision": ["fp16", "bf16"], "pytorch.batch_size": [1, 8], "vllm.max_num_seqs": [64, 256]}}` → pytorch gets 2x2=4, vllm gets 2x2=4, total 8 (independent grids per backend)

4. **expand_grid() tests — explicit experiments mode (CFG-14):**
   - `{"experiments": [{"model": "gpt2", "backend": "pytorch"}, {"model": "gpt2", "backend": "vllm"}]}` → 2 configs

5. **expand_grid() tests — combined mode (CFG-14):**
   - Sweep + explicit → combined list (sweep configs first, then explicit appended)

6. **expand_grid() tests — base: resolution:**
   - `base:` loads experiment YAML relative to study_yaml_path
   - `base:` strips study-only keys (sweep, experiments, execution, base, name)
   - Missing base file raises ConfigError

7. **expand_grid() tests — invalid combination handling:**
   - Invalid configs collected as SkippedConfig, valid ones returned
   - All invalid → raises ConfigError ("nothing to run")
   - SkippedConfig.short_label returns "backend, precision"
   - SkippedConfig.to_dict() serialises correctly

8. **compute_study_design_hash() tests (CFG-16):**
   - Returns 16-char hex string
   - Same experiments → same hash (deterministic)
   - Different experiments → different hash
   - Stable across calls (json.dumps sort_keys=True prevents ordering issues)

9. **apply_cycles() tests (CFG-15, implied by CFG-14):**
   - sequential with 3 cycles and [A, B] → [A, A, A, B, B, B]
   - interleaved with 3 cycles and [A, B] → [A, B, A, B, A, B]
   - shuffled with seed → deterministic reproducible order
   - shuffled with same study_design_hash → same order every time
   - n_cycles=1 → original list unchanged

Use `tmp_path` pytest fixture for base: file tests. Create temporary YAML files as needed.

For ExperimentConfig creation in tests, use the minimum required: `ExperimentConfig(model="gpt2")` (all other fields have defaults).

**Important:** Do NOT use `_unflatten()` from loader.py for sweep key parsing. The sweep grammar handles dotted keys as backend-scoped dimensions (different semantics from CLI override unflattening). Create a separate `_parse_sweep_dims()` or handle inline in `_expand_sweep()`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_grid.py -x --tb=short 2>&1 | tail -20</automated>
  </verify>
  <done>
    All tests exist and FAIL (RED phase). ExecutionConfig and StudyConfig models pass their own validation tests. study/ package exists with grid.py stubs. Test file covers all 9 test groups listed above.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement expand_grid(), compute_study_design_hash(), apply_cycles() — make all tests pass</name>
  <files>
    src/llenergymeasure/study/grid.py
  </files>
  <action>
**GREEN phase — implement the stubs to make all tests pass.**

**1. Implement `expand_grid()`:**

Resolution order:
1. Load `base:` file (optional DRY inheritance) via `_load_base()`
2. Build "fixed" dict from non-sweep, non-experiments, non-execution, non-base, non-name keys
3. Expand `sweep:` block into raw config dicts via `_expand_sweep()`
4. Append explicit `experiments:` list entries (each merged with fixed dict)
5. Pydantic-validate each raw dict → collect valid ExperimentConfig + SkippedConfig

Key implementation details:
- `_extract_fixed(raw_study)`: returns all top-level keys EXCEPT `sweep`, `experiments`, `execution`, `base`, `name`, `version`. These are the experiment-level fields shared across all grid points.
- Fixed dict merging: `{**base_dict, **fixed}` — inline fields override base.
- For explicit experiments: `{**fixed, **exp}` — each experiment entry overrides fixed.

**2. Implement `_expand_sweep()`:**

```python
def _expand_sweep(sweep: dict[str, Any], fixed: dict[str, Any]) -> list[dict[str, Any]]:
```

Parse dotted keys: split on first `.` to get `(backend, param)`. Non-dotted keys are universal dimensions.

Determine backends:
- If scoped keys exist → backends from scope prefixes
- Else if `fixed["backend"]` is a list → backends from that list
- Else → `[fixed.get("backend", "pytorch")]`

For each backend, build its grid:
- Dimensions = universal dims + that backend's scoped dims
- `itertools.product(*values)` → raw config dicts
- Backend-scoped params go into `config_dict.setdefault(backend_name, {})[param] = value`
- Universal params go at top level: `config_dict[key] = value`
- Set `config_dict["backend"] = backend`
- Do NOT include other backends' sections in this config dict

If sweep is empty and fixed has a `model` key, return `[fixed]` (single experiment from inline fields).
If sweep is empty and no model, return `[]`.

**3. Implement `_load_base()`:**

```python
def _load_base(base_path_str: str | None, study_yaml_path: Path | None) -> dict[str, Any]:
```

- If base_path_str is None, return `{}`
- Resolve path relative to study_yaml_path.parent (if not absolute)
- Hard error (ConfigError) if file doesn't exist
- `yaml.safe_load()` the file
- Strip study-only keys: `{"sweep", "experiments", "execution", "base", "name"}`
- Return remaining dict

**4. Implement `compute_study_design_hash()`:**

Identical pattern to `compute_measurement_config_hash()` in domain/experiment.py:
```python
canonical = json.dumps([exp.model_dump() for exp in experiments], sort_keys=True)
return hashlib.sha256(canonical.encode()).hexdigest()[:16]
```

**5. Implement `apply_cycles()`:**

```python
if cycle_order == CycleOrder.SEQUENTIAL:
    return [exp for exp in experiments for _ in range(n_cycles)]
if cycle_order == CycleOrder.INTERLEAVED:
    return experiments * n_cycles
# shuffled
seed = shuffle_seed if shuffle_seed is not None else int(study_design_hash, 16) & 0xFFFFFFFF
rng = random.Random(seed)
result = []
for _ in range(n_cycles):
    cycle = list(experiments)
    rng.shuffle(cycle)
    result.extend(cycle)
return result
```

**6. Implement `_extract_fixed()`:**

```python
_STUDY_ONLY_KEYS = {"sweep", "experiments", "execution", "base", "name", "version"}

def _extract_fixed(raw_study: dict[str, Any]) -> dict[str, Any]:
    return {k: v for k, v in raw_study.items() if k not in _STUDY_ONLY_KEYS}
```

**7. Handle the "all invalid" guard in expand_grid():**

After collecting valid and skipped lists:
- If `len(valid) + len(skipped) == 0`: raise `ConfigError("Study produced no experiments...")`
- If `len(valid) == 0`: raise `ConfigError` with count + first 5 skip reasons
- If `len(skipped) / total > 0.5`: log a warning (but don't error — just note it; the warning text will be in the skipped list for the pre-flight display)

Return `(valid, skipped)`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_grid.py -x -v --tb=short 2>&1 | tail -40</automated>
  </verify>
  <done>
    All tests in test_study_grid.py pass (GREEN phase). expand_grid() correctly handles grid sweep, explicit experiments, combined mode, base: resolution, and invalid combination collection. compute_study_design_hash() returns stable 16-char hex. apply_cycles() produces correct ordering for all three modes. Existing tests still pass (no regressions).
  </done>
</task>

</tasks>

<verification>
```bash
# All Phase 9 Plan 01 tests pass
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool
python -m pytest tests/unit/test_study_grid.py -v --tb=short

# No regressions in existing tests
python -m pytest tests/unit/ -x --tb=short

# Type check
python -m mypy src/llenergymeasure/study/grid.py src/llenergymeasure/config/models.py --ignore-missing-imports

# Import check — study package importable
python -c "from llenergymeasure.study.grid import expand_grid, apply_cycles, compute_study_design_hash, CycleOrder, SkippedConfig; print('OK')"

# Model check — ExecutionConfig and StudyConfig importable
python -c "from llenergymeasure.config.models import ExecutionConfig, StudyConfig; print('OK')"
```
</verification>

<success_criteria>
- ExecutionConfig and StudyConfig models are fully defined with all fields from CFG-11, CFG-15
- study/ package exists with grid.py exporting expand_grid, apply_cycles, compute_study_design_hash, CycleOrder, SkippedConfig
- Grid sweep produces correct Cartesian product (CFG-13, CFG-14 Mode A)
- Backend-scoped dotted keys (pytorch.batch_size) route to backend section dict (CFG-13)
- Explicit experiments mode works (CFG-14 Mode B)
- Combined mode works (CFG-14 Mode C)
- base: file inheritance works with path resolution relative to study YAML
- Invalid combinations collected as SkippedConfig, all-invalid raises ConfigError
- study_design_hash is 16-char hex, excludes execution block, deterministic (CFG-16)
- Cycle ordering correct for sequential, interleaved, shuffled (CFG-15)
- All unit tests pass; no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/09-grid-expansion-and-studyconfig/09-01-SUMMARY.md`
</output>
