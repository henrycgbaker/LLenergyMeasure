---
phase: 07-cli
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/cli/_display.py
  - src/llenergymeasure/cli/_vram.py
  - tests/unit/test_cli_display.py
autonomous: true
requirements: [CLI-08, CLI-09, CLI-14]

must_haves:
  truths:
    - "Result summary prints grouped sections (Energy, Performance, Timing) to stdout"
    - "FLOPs displayed with method and confidence annotation from process_results[0].compute_metrics (falls back to bare scalar if process_results is empty)"
    - "All numeric values formatted to 3 significant figures"
    - "Pydantic ValidationError formatted with friendly header and did-you-mean suggestions"
    - "VRAM estimation returns weights, KV cache, overhead, and total in GB"
    - "Duration formatted as human-readable (Xs, Xm Xs, Xh Xm)"
  artifacts:
    - path: "src/llenergymeasure/cli/_display.py"
      provides: "Plain-text formatting utilities for CLI output"
      min_lines: 80
    - path: "src/llenergymeasure/cli/_vram.py"
      provides: "VRAM estimation for --dry-run"
      min_lines: 30
    - path: "tests/unit/test_cli_display.py"
      provides: "Unit tests for display utilities and VRAM estimator"
      min_lines: 40
  key_links:
    - from: "src/llenergymeasure/cli/_display.py"
      to: "ExperimentResult"
      via: "print_result_summary(result) reads result fields"
      pattern: "def print_result_summary"
    - from: "src/llenergymeasure/cli/_display.py"
      to: "pydantic.ValidationError"
      via: "format_validation_error(e) wraps Pydantic errors"
      pattern: "def format_validation_error"
    - from: "src/llenergymeasure/cli/_vram.py"
      to: "ExperimentConfig"
      via: "estimate_vram(config) uses model/precision"
      pattern: "def estimate_vram"
---

<objective>
Build shared plain-text display utilities and VRAM estimator that both `llem run` and `llem config` commands will consume.

Purpose: Foundation modules for CLI output — result summaries, error formatting, duration/number formatting, and VRAM estimation for `--dry-run`. These must exist before the command modules can be written.

Output: `cli/_display.py` (~100 LOC), `cli/_vram.py` (~50 LOC), unit tests
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cli/07-CONTEXT.md
@.planning/phases/07-cli/07-RESEARCH.md

<interfaces>
<!-- Key types the executor needs. Extracted from codebase. -->

From src/llenergymeasure/domain/experiment.py:
```python
class ExperimentResult(BaseModel):
    schema_version: str
    experiment_id: str
    measurement_config_hash: str
    backend: str
    total_tokens: int
    total_energy_j: float
    total_inference_time_sec: float
    avg_tokens_per_second: float
    avg_energy_per_token_j: float
    total_flops: float
    baseline_power_w: float | None
    energy_adjusted_j: float | None
    measurement_warnings: list[str]
    warmup_excluded_samples: int | None
    start_time: datetime
    end_time: datetime
    timeseries: str | None
    latency_stats: LatencyStatistics | None

    @property
    def duration_sec(self) -> float: ...
```

From src/llenergymeasure/domain/metrics.py (relevant subset):
```python
class ComputeMetrics(BaseModel):
    flops_total: float
    flops_per_token: float
    flops_per_second: float
    flops_method: str = Field("unknown", description="Method used (calflops, architecture, parameter)")
    flops_confidence: str = Field("unknown", description="Confidence level (high, medium, low)")
```

Note: ExperimentResult.total_flops is a top-level scalar. flops_method and flops_confidence
are on ComputeMetrics, accessible via result.process_results[0].compute_metrics when
process_results is non-empty.

From src/llenergymeasure/config/models.py:
```python
class ExperimentConfig(BaseModel):
    model: str
    backend: Literal["pytorch", "vllm", "tensorrt"]
    n: int = 100
    dataset: str | SyntheticDatasetConfig
    precision: Literal["fp32", "fp16", "bf16"]
    max_input_tokens: int = 512
    max_output_tokens: int = 128
    output_dir: str | None = None
```

From src/llenergymeasure/config/ssot.py:
```python
PRECISION_SUPPORT: dict[str, list[str]]  # backend -> supported precisions
DECODING_SUPPORT: dict[str, list[str]]   # backend -> supported strategies
```

From src/llenergymeasure/exceptions.py:
```python
class LLEMError(Exception): ...
class ConfigError(LLEMError): ...
class BackendError(LLEMError): ...
class PreFlightError(LLEMError): ...
class ExperimentError(LLEMError): ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create _display.py — plain-text formatting utilities</name>
  <files>src/llenergymeasure/cli/_display.py</files>
  <action>
Create `src/llenergymeasure/cli/_display.py` with the following functions:

1. **`_sig3(value: float) -> str`**: Format float to 3 significant figures. Handle zero, very large, and very small numbers. Use `math.log10` and `math.floor` for digit calculation. Examples: `312.4 -> "312"`, `3.12 -> "3.12"`, `0.00312 -> "0.00312"`, `847.0 -> "847"`.

2. **`_format_duration(seconds: float) -> str`**: Human-readable duration. `<60s -> "4.2s"`, `60-3600 -> "4m 32s"`, `>3600 -> "1h 05m"`.

3. **`print_result_summary(result: ExperimentResult) -> None`**: Print grouped result summary to stdout (not stderr — this is the scientific record). Sections:
   - **Energy**: total_energy_j, baseline_power_w (if not None), energy_adjusted_j (if not None)
   - **Performance**: avg_tokens_per_second, total_flops with method/confidence annotation (if > 0), latency_stats TTFT/ITL (if latency_stats is not None). For FLOPs method/confidence: extract from `result.process_results[0].compute_metrics.flops_method` and `.flops_confidence` if `result.process_results` is non-empty; otherwise display total_flops as a bare scalar. Display format: `FLOPs  1.23e+12 (calflops, high)` — method and confidence in parentheses after the value. This satisfies CONTEXT.md locked decision: "Include FLOPs estimate (with method/confidence) in the summary."
   - **Timing**: duration_sec (formatted), warmup_excluded_samples (if not None)
   - **Warnings**: each measurement_warnings entry printed inline at end
   All numeric values via `_sig3()`. Use `print()` calls, not Rich.
   First line: `f"Result: {result.experiment_id}"` then blank line then sections.
   Per CONTEXT.md: strictly raw metrics only, no derived ratios.

4. **`print_dry_run(config: ExperimentConfig, vram: dict[str, float] | None, gpu_vram_gb: float | None, verbose: bool = False) -> None`**: Dry-run output. Show "Config (resolved)" section with model, backend, precision, batch_size (from pytorch section if present), dataset, n, output_dir. If `verbose`, append source annotations (e.g., "bf16 (default)") — compare field values against ExperimentConfig defaults to determine which are non-default. Then "VRAM estimate" section: if vram is None, show "(unavailable)"; otherwise show weights_gb, kv_cache_gb, overhead_gb, total_gb. If gpu_vram_gb available, show "/ X GB available" and "OK" or "WARNING: may not fit". Final line: "Config valid. Run without --dry-run to start."

5. **`format_error(error: LLEMError, verbose: bool = False) -> str`**: Format LLEMError for stderr output. If verbose, include full traceback via `traceback.format_exc()`. Otherwise, just the error message. Prefix with error class name (e.g., "ConfigError: ...").

6. **`format_validation_error(e: ValidationError) -> str`**: Wrap Pydantic ValidationError with friendly header. Iterate `e.errors()`. Header: `f"Config validation failed ({len(errors)} error{'s' if n>1 else ''}):"`. For each error, show `loc -> msg`. For errors where `type == 'literal_error'`, attempt did-you-mean via `difflib.get_close_matches()` against valid options from `config/ssot.py` (PRECISION_SUPPORT values flattened, backend names). Import difflib (stdlib). Do NOT catch or wrap ValidationError — just format it.

7. **`print_experiment_header(config: ExperimentConfig) -> None`**: Print one-line experiment header showing model + backend + any non-default parameters (precision if not bf16, n if not 100, etc.). Print to stderr (progress-area). Example: `"Experiment: gpt2 | pytorch | bf16 | n=50"`.

All output to stdout unless noted (progress/header to stderr). No Rich imports anywhere. No emoji.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.cli._display import _sig3, _format_duration, print_result_summary, format_validation_error; print(_sig3(312.4)); print(_format_duration(272))"</automated>
  </verify>
  <done>_display.py exists with all 7 functions, imports cleanly, _sig3 and _format_duration produce correct output</done>
</task>

<task type="auto">
  <name>Task 2: Create _vram.py + unit tests for display and VRAM</name>
  <files>src/llenergymeasure/cli/_vram.py, tests/unit/test_cli_display.py</files>
  <action>
**_vram.py:**

Create `src/llenergymeasure/cli/_vram.py` with:

1. **`DTYPE_BYTES`** constant: `{"fp32": 4, "fp16": 2, "bf16": 2, "int8": 1, "int4": 0.5}`

2. **`estimate_vram(config: ExperimentConfig) -> dict[str, float] | None`**: Estimate VRAM usage for a given experiment config. Steps:
   - Try to get model parameter count from HF Hub metadata: `huggingface_hub.HfApi().model_info(config.model)`. Look for `.safetensors` attribute with `total` field (total param count). Wrap in `try/except Exception` with 5-second timeout — return `None` on any failure (network error, model not found, etc.). Per Research doc: "non-blocking on network failure".
   - Calculate `weights_gb = (param_count * DTYPE_BYTES.get(config.precision, 2)) / 1e9`
   - Estimate KV cache: Try to get `n_layers`, `n_heads`, `head_dim` from HF Hub config. If available: `kv_bytes = 2 * n_layers * 1 * config.max_input_tokens * n_heads * head_dim * DTYPE_BYTES.get(config.precision, 2)`. `kv_gb = kv_bytes / 1e9`. If arch details unavailable, `kv_gb = 0.0`.
   - `overhead_gb = weights_gb * 0.15` (empirical 15% activation/framework overhead)
   - Return `{"weights_gb": weights_gb, "kv_cache_gb": kv_gb, "overhead_gb": overhead_gb, "total_gb": weights_gb + kv_gb + overhead_gb}`
   - If param_count cannot be determined at all, return `None`.

3. **`get_gpu_vram_gb() -> float | None`**: Return total VRAM of first GPU in GB, or None if unavailable. Use pynvml: `nvmlInit()`, `nvmlDeviceGetHandleByIndex(0)`, `nvmlDeviceGetMemoryInfo(h).total / 1e9`. Wrap in `try/except Exception` — return None on failure.

**Unit tests (tests/unit/test_cli_display.py):**

Test the pure functions (no GPU/network needed):
- `test_sig3_integer`: `_sig3(312.4) == "312"`
- `test_sig3_decimal`: `_sig3(3.12) == "3.12"`
- `test_sig3_small`: `_sig3(0.00312) == "0.00312"`
- `test_sig3_zero`: `_sig3(0) == "0"`
- `test_format_duration_seconds`: `_format_duration(4.2) == "4.2s"`
- `test_format_duration_minutes`: `_format_duration(272) == "4m 32s"`
- `test_format_duration_hours`: `_format_duration(3900) == "1h 05m"`
- `test_vram_dtype_bytes`: verify DTYPE_BYTES has expected entries
- `test_format_validation_error`: create a mock ValidationError (use `ExperimentConfig(model="x", backend="pytorh")` to trigger a real one), verify output contains "Config validation failed" header
- `test_format_error_concise`: verify format_error strips traceback when verbose=False
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_cli_display.py -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>_vram.py imports cleanly, all unit tests in test_cli_display.py pass</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.cli._display import print_result_summary, format_validation_error, _sig3"` succeeds
2. `python -c "from llenergymeasure.cli._vram import estimate_vram, get_gpu_vram_gb"` succeeds
3. `python -m pytest tests/unit/test_cli_display.py -x -v` — all tests pass
4. No `import rich` in any new file: `grep -r "import rich" src/llenergymeasure/cli/_display.py src/llenergymeasure/cli/_vram.py` returns nothing
</verification>

<success_criteria>
- `_display.py` contains all 7 functions with correct output routing (stdout for results, stderr for headers)
- `_vram.py` returns a dict with keys weights_gb, kv_cache_gb, overhead_gb, total_gb or None
- All unit tests pass without GPU or network access
- No Rich dependency anywhere in new code
</success_criteria>

<output>
After completion, create `.planning/phases/07-cli/07-01-SUMMARY.md`
</output>
