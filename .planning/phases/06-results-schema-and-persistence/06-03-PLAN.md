---
phase: 06-results-schema-and-persistence
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - src/llenergymeasure/results/aggregation.py
  - src/llenergymeasure/results/exporters.py
  - tests/unit/test_aggregation_v2.py
autonomous: true
requirements: [RES-12, RES-20, RES-21]

must_haves:
  truths:
    - "aggregate_results() returns ExperimentResult (v2.0 schema) from list of RawProcessResult"
    - "Late aggregation concatenates per-process latencies before computing statistics (no average-of-averages)"
    - "validate_process_completeness() performs 4 checks: count, index contiguity, no duplicates, marker files"
    - "aggregation.py uses stdlib logging (not loguru)"
    - "Campaign-level aggregation functions removed (dead code for v2.0)"
    - "export_aggregated_to_csv() uses ExperimentResult v2.0 field names"
    - "exporters.py uses stdlib logging (not loguru)"
  artifacts:
    - path: "src/llenergymeasure/results/aggregation.py"
      provides: "v2.0 aggregation: aggregate_results() -> ExperimentResult with v2.0 fields"
      contains: "def aggregate_results"
    - path: "src/llenergymeasure/results/exporters.py"
      provides: "CSV exporter using v2.0 ExperimentResult"
      contains: "def export_aggregated_to_csv"
    - path: "tests/unit/test_aggregation_v2.py"
      provides: "Unit tests for v2.0 aggregation"
  key_links:
    - from: "src/llenergymeasure/results/aggregation.py"
      to: "src/llenergymeasure/domain/experiment.py"
      via: "imports ExperimentResult, RawProcessResult, AggregationMetadata"
      pattern: "from llenergymeasure.domain.experiment import"
    - from: "src/llenergymeasure/results/exporters.py"
      to: "src/llenergymeasure/domain/experiment.py"
      via: "imports ExperimentResult (via AggregatedResult alias)"
      pattern: "from llenergymeasure.domain.experiment import"
---

<objective>
Update results/aggregation.py to produce v2.0 ExperimentResult — strip v1.x dead code (campaign functions), replace loguru with stdlib logging, and ensure late aggregation correctly populates all v2.0 fields. Update results/exporters.py to use v2.0 field names and stdlib logging. Write unit tests for aggregation.

Purpose: Aggregation is the internal mechanism that combines per-process raw results (multi-GPU) into the single ExperimentResult. For M1 single-GPU, aggregation is trivial (one process), but the machinery must correctly produce v2.0 ExperimentResult. The CSV exporter must also reference v2.0 fields.
Output: Updated aggregation.py, updated exporters.py, unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-results-schema-and-persistence/06-CONTEXT.md
@.planning/phases/06-results-schema-and-persistence/06-RESEARCH.md
@.planning/phases/06-results-schema-and-persistence/06-01-SUMMARY.md
@src/llenergymeasure/results/aggregation.py
@src/llenergymeasure/results/exporters.py

<interfaces>
<!-- Key types from Plan 01 output (ExperimentResult v2.0 schema) -->

From src/llenergymeasure/domain/experiment.py (rewritten in Plan 01):
```python
class ExperimentResult(BaseModel):
    schema_version: str = Field(default="2.0")
    experiment_id: str = Field(...)
    measurement_config_hash: str = Field(...)
    backend: str = Field(default="pytorch")
    measurement_methodology: Literal["total", "steady_state", "windowed"] = Field(...)
    steady_state_window: tuple[float, float] | None = Field(default=None)
    total_tokens: int = Field(...)
    total_energy_j: float = Field(...)
    total_inference_time_sec: float = Field(...)
    avg_tokens_per_second: float = Field(...)
    avg_energy_per_token_j: float = Field(...)
    total_flops: float = Field(...)
    baseline_power_w: float | None = Field(default=None)
    energy_adjusted_j: float | None = Field(default=None)
    energy_per_device_j: list[float] | None = Field(default=None)
    energy_breakdown: EnergyBreakdown | None = Field(default=None)
    multi_gpu: MultiGPUMetrics | None = Field(default=None)
    environment_snapshot: EnvironmentSnapshot | None = Field(default=None)
    measurement_warnings: list[str] = Field(default_factory=list)
    warmup_excluded_samples: int | None = Field(default=None)
    reproducibility_notes: str = Field(default="...")
    timeseries: str | None = Field(default=None)
    start_time: datetime = Field(...)
    end_time: datetime = Field(...)
    effective_config: dict[str, Any] = Field(default_factory=dict)
    process_results: list[RawProcessResult] = Field(default_factory=list)
    aggregation: AggregationMetadata | None = Field(default=None)
    thermal_throttle: ThermalThrottleInfo | None = Field(default=None)
    warmup_result: WarmupResult | None = Field(default=None)
    latency_stats: LatencyStatistics | None = Field(default=None)
    extended_metrics: ExtendedEfficiencyMetrics | None = Field(default=None)
    model_config = {"frozen": True}

class RawProcessResult(BaseModel):
    # ... per-process raw data (unchanged from current) ...
    process_index: int = Field(...)
    gpu_id: int = Field(...)
    timestamps: Timestamps = Field(...)
    inference_metrics: InferenceMetrics = Field(...)
    energy_metrics: EnergyMetrics = Field(...)
    compute_metrics: ComputeMetrics = Field(...)
    per_request_latencies_ms: list[float] = Field(default_factory=list)
    gpu_utilisation_samples: list[float] = Field(default_factory=list)
    # ... etc ...

class AggregationMetadata(BaseModel):
    method: str = Field(default="sum_energy_avg_throughput")
    num_processes: int = Field(...)
    temporal_overlap_verified: bool = Field(default=False)
    gpu_attribution_verified: bool = Field(default=False)
    warnings: list[str] = Field(default_factory=list)
```

From src/llenergymeasure/results/aggregation.py (current, needs updating):
```python
# Key functions to preserve:
def validate_process_completeness(experiment_id, raw_results, expected_processes, results_dir) -> CompletenessReport: ...
def aggregate_results(raw_results, experiment_id, ...) -> AggregatedResult: ...
# Internal helpers: _aggregate_inference_metrics(), _aggregate_energy_metrics(), etc.

# Functions to REMOVE (v1.x dead code):
# aggregate_campaign_results() — campaign-level, not in v2.0
# aggregate_campaign_with_grouping() — campaign-level, not in v2.0
# calculate_efficiency_metrics() — not in v2.0 path
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update aggregation.py and exporters.py for v2.0</name>
  <files>
    src/llenergymeasure/results/aggregation.py
    src/llenergymeasure/results/exporters.py
  </files>
  <action>
  **Update `src/llenergymeasure/results/aggregation.py`:**

  This is an UPDATE, not a full rewrite. Preserve the core aggregation logic (sum energy, average throughput, concatenate latencies), but align to v2.0 models.

  **1. Replace loguru with stdlib logging:**
  Change `from loguru import logger` to `import logging; logger = logging.getLogger(__name__)`.
  Update all `logger.debug(...)`, `logger.warning(...)`, `logger.info(...)` calls — loguru and stdlib logging have the same API for these methods, so no functional changes needed beyond the import.

  **2. Update `aggregate_results()` return type and construction:**

  The function currently returns `AggregatedResult` (alias for ExperimentResult). It must construct the result using v2.0 field names:

  - `measurement_config_hash`: pass through from the first RawProcessResult's effective_config (or accept as parameter). For M1 single-GPU, the hash is computed by the caller (PyTorchBackend._build_result) and passed in. Add `measurement_config_hash: str` parameter to `aggregate_results()`.
  - `measurement_methodology`: pass through as parameter. Add `measurement_methodology: str = "total"` parameter.
  - `schema_version`: use `"2.0"` literal (not `SCHEMA_VERSION` constant)
  - Remove construction of v1.x-only fields: `cli_overrides`, `config_warnings`, `parameter_provenance`, `preset_chain`, `energy_tracking_failed`, `environment` (old EnvironmentMetadata)
  - Add new fields to the constructor: `measurement_config_hash`, `measurement_methodology`, `steady_state_window=None`, `baseline_power_w=None`, `energy_adjusted_j=None`, `energy_per_device_j=None`, `multi_gpu=None`, `warmup_excluded_samples=None`, `measurement_warnings=[]`, `environment_snapshot=None`, `timeseries=None`
  - For single-GPU (num_processes=1): aggregation is trivial — copy metrics from the single process result

  Update the function signature to accept these new parameters:
  ```python
  def aggregate_results(
      raw_results: list[RawProcessResult],
      experiment_id: str,
      measurement_config_hash: str,
      measurement_methodology: str = "total",
      steady_state_window: tuple[float, float] | None = None,
      baseline_power_w: float | None = None,
      energy_adjusted_j: float | None = None,
      energy_per_device_j: list[float] | None = None,
      energy_breakdown: EnergyBreakdown | None = None,
      multi_gpu: MultiGPUMetrics | None = None,
      environment_snapshot: EnvironmentSnapshot | None = None,
      measurement_warnings: list[str] | None = None,
      warmup_excluded_samples: int | None = None,
      warmup_result: WarmupResult | None = None,
      thermal_throttle: ThermalThrottleInfo | None = None,
      timeseries: str | None = None,
      effective_config: dict[str, Any] | None = None,
  ) -> ExperimentResult:
  ```

  **3. Remove dead code:**
  Delete the following functions that are v1.x campaign-level code not used in v2.0:
  - `aggregate_campaign_results()` (if present)
  - `aggregate_campaign_with_grouping()` (if present)
  - `calculate_efficiency_metrics()` (if present)

  Read the file carefully before deleting — only remove functions confirmed dead. Preserve:
  - `validate_process_completeness()` (needed for RES-12)
  - `CompletenessReport` class
  - `aggregate_results()` and its internal helpers
  - `_aggregate_inference_metrics()`, `_aggregate_energy_metrics()`, `_aggregate_compute_metrics()`
  - `_aggregate_extended_metrics_from_results()` (late aggregation — key for RES-20)

  **4. Update imports:**
  - Change `from llenergymeasure.domain.experiment import AggregatedResult` to `from llenergymeasure.domain.experiment import ExperimentResult` (use v2.0 name directly)
  - Keep the `AggregationMetadata` and `RawProcessResult` imports
  - Add imports for new types: `EnergyBreakdown`, `MultiGPUMetrics`, `EnvironmentSnapshot`, `WarmupResult`, `ThermalThrottleInfo`

  **Update `src/llenergymeasure/results/exporters.py`:**

  **1. Replace loguru with stdlib logging:**
  Change `from loguru import logger` to `import logging; logger = logging.getLogger(__name__)`.

  **2. Update import:**
  Change `from llenergymeasure.domain.experiment import AggregatedResult, RawProcessResult` to `from llenergymeasure.domain.experiment import ExperimentResult, RawProcessResult`.
  Update the `export_aggregated_to_csv()` function signature to accept `list[ExperimentResult]` instead of `list[AggregatedResult]`. Since `AggregatedResult = ExperimentResult`, this is type-equivalent, but using the v2.0 name makes the code clearer.

  **3. No functional changes to the flatten/export logic** — the `flatten_model()` function uses `model_dump()` which will automatically pick up v2.0 fields. Column names in CSV will change (new fields added, removed fields absent), which is correct.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.results.aggregation import aggregate_results, validate_process_completeness
from llenergymeasure.results.exporters import export_aggregated_to_csv
# Verify imports work (no loguru dependency)
print('Imports OK')

# Verify aggregate_results accepts new params
import inspect
sig = inspect.signature(aggregate_results)
assert 'measurement_config_hash' in sig.parameters, 'Missing measurement_config_hash param'
assert 'measurement_methodology' in sig.parameters, 'Missing measurement_methodology param'
print('Signature OK')
"</automated>
  </verify>
  <done>aggregation.py returns v2.0 ExperimentResult with all new fields. loguru replaced with stdlib logging in both modules. Campaign-level dead code removed. Late aggregation preserved. exporters.py uses v2.0 import names.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for v2.0 aggregation</name>
  <files>
    tests/unit/test_aggregation_v2.py
  </files>
  <action>
  **Create `tests/unit/test_aggregation_v2.py`:**

  Write focused unit tests (no GPU required). These test v2.0 aggregation behaviour, NOT the old test file (which tests v1.x patterns).

  **Helper fixture:** Create a `make_raw_result()` fixture that constructs a valid `RawProcessResult`:
  ```python
  @pytest.fixture
  def make_raw_result():
      def _make(process_index=0, gpu_id=0, **overrides):
          from llenergymeasure.domain.experiment import RawProcessResult, Timestamps
          from llenergymeasure.domain.metrics import InferenceMetrics, EnergyMetrics, ComputeMetrics
          from datetime import datetime

          defaults = dict(
              experiment_id="test-001",
              process_index=process_index,
              gpu_id=gpu_id,
              config_name="test",
              model_name="gpt2",
              timestamps=Timestamps.from_times(
                  datetime(2026, 2, 26, 14, 0), datetime(2026, 2, 26, 14, 0, 10)
              ),
              inference_metrics=InferenceMetrics(
                  total_tokens=500, total_prompts=10,
                  avg_tokens_per_second=50.0, total_inference_time_sec=10.0,
              ),
              energy_metrics=EnergyMetrics(
                  total_energy_joules=25.0, avg_power_watts=2.5,
                  energy_per_token_joules=0.05,
              ),
              compute_metrics=ComputeMetrics(
                  total_flops=5e11, peak_memory_mb=4000.0,
              ),
              per_request_latencies_ms=[100.0, 110.0, 95.0, 105.0, 90.0],
          )
          defaults.update(overrides)
          return RawProcessResult(**defaults)
      return _make
  ```

  Tests:

  1. `test_aggregate_single_process()` — single RawProcessResult produces ExperimentResult with matching metrics
  2. `test_aggregate_returns_experiment_result()` — return type is ExperimentResult (not old AggregatedResult by type name)
  3. `test_aggregate_schema_version()` — aggregated result has `schema_version == "2.0"`
  4. `test_aggregate_measurement_config_hash()` — hash passed through to result
  5. `test_aggregate_measurement_methodology()` — methodology passed through to result
  6. `test_aggregate_energy_sum()` — two processes: energy summed (25 + 30 = 55)
  7. `test_aggregate_tokens_sum()` — two processes: tokens summed
  8. `test_aggregate_late_aggregation_latencies()` — two processes with different latency lists: all latencies concatenated, not averaged
  9. `test_aggregate_process_results_embedded()` — process_results list contains original RawProcessResult objects
  10. `test_aggregate_metadata_num_processes()` — aggregation.num_processes matches input count
  11. `test_validate_process_completeness_complete()` — 2 processes with indices 0,1, both have markers → is_complete=True
  12. `test_validate_process_completeness_missing()` — 1 of 2 missing → is_complete=False
  13. `test_validate_process_completeness_duplicate()` — two process_index=0 → duplicate detected
  14. `test_no_loguru_import()` — grep the aggregation.py source for 'loguru', assert not found
  15. `test_csv_exporter_no_loguru()` — grep exporters.py for 'loguru', assert not found

  For the validate_process_completeness tests that check marker files, create a temp directory with the appropriate file structure.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_aggregation_v2.py -x -v 2>&1 | tail -25</automated>
  </verify>
  <done>15+ unit tests pass confirming: v2.0 ExperimentResult produced by aggregation, energy summed correctly, late aggregation preserves raw latencies, process completeness validation works, loguru removed from both modules.</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.results.aggregation import aggregate_results; import inspect; sig = inspect.signature(aggregate_results); assert 'measurement_config_hash' in sig.parameters"` — v2.0 signature
2. `python -c "import ast; src = open('src/llenergymeasure/results/aggregation.py').read(); tree = ast.parse(src); assert not any(isinstance(n, ast.ImportFrom) and n.module == 'loguru' for n in ast.walk(tree)); print('No loguru')"` — loguru removed from aggregation
3. `python -c "import ast; src = open('src/llenergymeasure/results/exporters.py').read(); tree = ast.parse(src); assert not any(isinstance(n, ast.ImportFrom) and n.module == 'loguru' for n in ast.walk(tree)); print('No loguru')"` — loguru removed from exporters
4. `python -m pytest tests/unit/test_aggregation_v2.py -x -v` — all tests pass
</verification>

<success_criteria>
- aggregate_results() produces ExperimentResult with schema_version="2.0" and all v2.0 fields
- Late aggregation concatenates per-process data (no average-of-averages)
- validate_process_completeness() performs 4 checks (RES-12)
- Campaign-level dead code removed
- loguru replaced with stdlib logging in both aggregation.py and exporters.py
- CSV exporter uses v2.0 ExperimentResult field names
- 15+ unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/06-results-schema-and-persistence/06-03-SUMMARY.md`
</output>
