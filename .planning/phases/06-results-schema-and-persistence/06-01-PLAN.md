---
phase: 06-results-schema-and-persistence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/domain/experiment.py
  - src/llenergymeasure/domain/metrics.py
  - tests/unit/test_experiment_result_v2.py
autonomous: true
requirements: [RES-01, RES-02, RES-03, RES-04, RES-05, RES-06, RES-07, RES-08, RES-09, RES-10, RES-11]

must_haves:
  truths:
    - "ExperimentResult has schema_version='2.0' (not '2.0.0')"
    - "ExperimentResult has measurement_config_hash field (str, computed externally)"
    - "ExperimentResult has measurement_methodology Literal['total', 'steady_state', 'windowed']"
    - "ExperimentResult has steady_state_window: tuple[float, float] | None"
    - "ExperimentResult has baseline_power_w, energy_adjusted_j, energy_per_device_j fields"
    - "ExperimentResult has energy_breakdown: EnergyBreakdown | None"
    - "ExperimentResult has reproducibility_notes with fixed default string"
    - "ExperimentResult has environment_snapshot: EnvironmentSnapshot | None"
    - "ExperimentResult has measurement_warnings: list[str]"
    - "ExperimentResult has warmup_excluded_samples: int | None"
    - "ExperimentResult is frozen (model_config frozen=True)"
    - "AggregatedResult = ExperimentResult alias is preserved"
    - "compute_measurement_config_hash() returns 16-char hex string from ExperimentConfig"
    - "MultiGPUMetrics model exists with num_gpus, energy_per_gpu_j, energy_total_j, energy_per_output_token_j"
  artifacts:
    - path: "src/llenergymeasure/domain/experiment.py"
      provides: "ExperimentResult v2.0 schema with all required fields"
      contains: "measurement_config_hash"
    - path: "src/llenergymeasure/domain/metrics.py"
      provides: "MultiGPUMetrics model"
      contains: "class MultiGPUMetrics"
    - path: "tests/unit/test_experiment_result_v2.py"
      provides: "Unit tests for v2.0 ExperimentResult schema"
  key_links:
    - from: "src/llenergymeasure/domain/experiment.py"
      to: "src/llenergymeasure/domain/metrics.py"
      via: "imports EnergyBreakdown, WarmupResult, ThermalThrottleInfo, MultiGPUMetrics"
      pattern: "from llenergymeasure.domain.metrics import"
    - from: "src/llenergymeasure/domain/experiment.py"
      to: "src/llenergymeasure/domain/environment.py"
      via: "imports EnvironmentSnapshot"
      pattern: "from llenergymeasure.domain.environment import"
    - from: "src/llenergymeasure/__init__.py"
      to: "src/llenergymeasure/domain/experiment.py"
      via: "ExperimentResult re-export"
      pattern: "from llenergymeasure.domain.experiment import ExperimentResult"
---

<objective>
Rewrite ExperimentResult to v2.0 schema with all required fields — measurement_config_hash, measurement_methodology, steady_state_window, energy detail fields, reproducibility_notes, warmup_excluded_samples, and schema_version="2.0". Add MultiGPUMetrics model and the config hash utility function. Write comprehensive unit tests.

Purpose: ExperimentResult is the central data model consumed by persistence (Plan 02) and aggregation (Plan 03). Without the v2.0 schema, no downstream code can produce or consume correct results.
Output: Rewritten ExperimentResult, MultiGPUMetrics model, compute_measurement_config_hash() utility, unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-results-schema-and-persistence/06-CONTEXT.md
@.planning/phases/06-results-schema-and-persistence/06-RESEARCH.md
@.product/designs/result-schema.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/domain/metrics.py (existing, imported by experiment.py):
```python
class EnergyBreakdown(BaseModel):
    raw_energy_j: float = Field(...)
    baseline_power_w: float | None = Field(default=None)
    baseline_duration_sec: float | None = Field(default=None)
    adjusted_energy_j: float | None = Field(default=None)
    adjustment_method: str | None = Field(default=None)
    provenance: str = Field(default="nvml_polling")

class WarmupResult(BaseModel):
    converged: bool = Field(...)
    final_cv: float = Field(...)
    iterations_completed: int = Field(...)
    target_cv: float = Field(...)
    max_prompts: int = Field(...)
    latencies_ms: list[float] = Field(default_factory=list)
    # thermal_floor_wait_s added in Phase 5 Plan 02

class ThermalThrottleInfo(BaseModel):
    # ... existing thermal detection fields ...

class LatencyStatistics(BaseModel):
    # ... TTFT/ITL stats ...

class ExtendedEfficiencyMetrics(BaseModel):
    # ... extended efficiency analysis fields (all optional) ...
```

From src/llenergymeasure/domain/environment.py (existing):
```python
class EnvironmentSnapshot(BaseModel):
    python_version: str = Field(...)
    cuda_version: str | None = Field(default=None)
    cuda_driver_version: str | None = Field(default=None)
    gpu_names: list[str] = Field(default_factory=list)
    gpu_vram_mb: list[int] = Field(default_factory=list)
    pip_freeze: list[str] = Field(default_factory=list)
    tool_version: str = Field(...)
    platform: str = Field(...)
    os_version: str | None = Field(default=None)

class EnvironmentMetadata(BaseModel):
    # older v1.x model, kept for backward compat
```

From src/llenergymeasure/constants.py:
```python
SCHEMA_VERSION = "2.0.0"  # NOTE: Do NOT use this for ExperimentResult.schema_version
                          # The design doc specifies "2.0" not "2.0.0"
```

From src/llenergymeasure/config/models.py:
```python
class ExperimentConfig(BaseModel):
    model_config = {"extra": "forbid"}
    model: str = Field(...)
    backend: Literal["pytorch", "vllm", "tensorrt"] = Field(default="pytorch")
    # ... all config fields ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite ExperimentResult to v2.0, add MultiGPUMetrics, add config hash utility</name>
  <files>
    src/llenergymeasure/domain/experiment.py
    src/llenergymeasure/domain/metrics.py
  </files>
  <action>
  **Add `MultiGPUMetrics` to `src/llenergymeasure/domain/metrics.py`:**

  Add the model after the existing `EnergyBreakdown` class:

  ```python
  class MultiGPUMetrics(BaseModel):
      """Per-device energy breakdown for multi-GPU experiments."""
      num_gpus: int = Field(..., description="Number of GPUs used")
      energy_per_gpu_j: list[float] = Field(..., description="Per-device energy in joules")
      energy_total_j: float = Field(..., description="Sum of energy across all devices")
      energy_per_output_token_j: float = Field(
          ..., description="Primary cross-configuration efficiency metric"
      )
  ```

  **Add `compute_measurement_config_hash()` to `src/llenergymeasure/domain/experiment.py`:**

  Add as a module-level utility function (not a method on ExperimentResult — avoids frozen model issues and circular imports):

  ```python
  import hashlib
  import json

  def compute_measurement_config_hash(config: "ExperimentConfig") -> str:
      """SHA-256[:16] of ExperimentConfig. Layer 3 fields excluded by design.

      Layer 3 fields (datacenter_pue, grid_carbon_intensity) are not in
      ExperimentConfig (they live in user config only), so model_dump()
      naturally excludes them. No special exclusion logic needed.
      """
      canonical = json.dumps(config.model_dump(), sort_keys=True)
      return hashlib.sha256(canonical.encode()).hexdigest()[:16]
  ```

  Use `from __future__ import annotations` and `TYPE_CHECKING` for the `ExperimentConfig` type hint to avoid circular imports.

  **Rewrite `ExperimentResult` in `src/llenergymeasure/domain/experiment.py`:**

  Replace the current ExperimentResult class entirely. The new schema:

  ```python
  class ExperimentResult(BaseModel):
      """Experiment result — the user-visible output of a measurement run.

      Combines raw results from all processes into a single result with proper
      aggregation (sum energy, average throughput). For single-GPU experiments,
      process_results has exactly one item.

      v2.0 schema: all fields ship together (decision #50).
      """

      # Identity
      schema_version: str = Field(default="2.0", description="Result schema version")
      experiment_id: str = Field(..., description="Unique experiment identifier")
      measurement_config_hash: str = Field(
          ..., description="SHA-256[:16] of ExperimentConfig (environment excluded)"
      )

      # Backend
      backend: str = Field(default="pytorch", description="Inference backend used")
      backend_version: str | None = Field(
          default=None, description="Backend version string for reproducibility"
      )

      # Methodology (RES-03, RES-04)
      measurement_methodology: Literal["total", "steady_state", "windowed"] = Field(
          ..., description="What was measured — total run, steady-state window, or explicit window"
      )
      steady_state_window: tuple[float, float] | None = Field(
          default=None,
          description="(start_sec, end_sec) of measurement window relative to experiment start"
      )

      # Core metrics
      total_tokens: int = Field(..., description="Total tokens across all processes")
      total_energy_j: float = Field(..., description="Total energy (sum across processes)")
      total_inference_time_sec: float = Field(..., description="Total inference time")
      avg_tokens_per_second: float = Field(..., description="Average throughput")
      avg_energy_per_token_j: float = Field(..., description="Average energy per token")
      total_flops: float = Field(..., description="Total FLOPs (reference metadata)")

      # Energy detail (RES-06, RES-07)
      baseline_power_w: float | None = Field(
          default=None, description="Idle GPU power (W) measured before experiment"
      )
      energy_adjusted_j: float | None = Field(
          default=None, description="Baseline-subtracted energy attributable to inference"
      )
      energy_per_device_j: list[float] | None = Field(
          default=None, description="Per-GPU energy breakdown (Zeus backend only)"
      )
      energy_breakdown: EnergyBreakdown | None = Field(
          default=None, description="Detailed energy breakdown with baseline adjustment"
      )

      # Multi-GPU (from result-schema.md design)
      multi_gpu: MultiGPUMetrics | None = Field(
          default=None, description="Multi-GPU metrics. None for single-GPU runs."
      )

      # Environment (RES-09)
      environment_snapshot: EnvironmentSnapshot | None = Field(
          default=None, description="Full software+hardware environment snapshot"
      )

      # Quality (RES-08, RES-10, RES-11)
      measurement_warnings: list[str] = Field(
          default_factory=list,
          description="Measurement quality warnings (e.g., short duration, thermal drift)"
      )
      warmup_excluded_samples: int | None = Field(
          default=None,
          description="Number of prompts excluded during warmup. None when methodology=total."
      )
      reproducibility_notes: str = Field(
          default="Energy measured via NVML polling. Accuracy +/-5%. "
                  "Results may vary with thermal state and system load.",
          description="Fixed disclaimer about measurement accuracy"
      )

      # Timeseries sidecar reference
      timeseries: str | None = Field(
          default=None, description="Relative filename of timeseries sidecar (e.g. 'timeseries.parquet')"
      )

      # Timestamps
      start_time: datetime = Field(..., description="Earliest process start time")
      end_time: datetime = Field(..., description="Latest process end time")

      # Effective configuration (for reproducibility)
      effective_config: dict[str, Any] = Field(
          default_factory=dict,
          description="Full resolved config",
      )

      # Per-process breakdown (embedded, not separate files per CONTEXT.md)
      process_results: list[RawProcessResult] = Field(
          default_factory=list, description="Original per-process results"
      )
      aggregation: AggregationMetadata | None = Field(
          default=None, description="Aggregation metadata (populated for multi-GPU)"
      )

      # Carry-forward fields (v1.x compat, used by aggregation/CLI)
      thermal_throttle: ThermalThrottleInfo | None = Field(
          default=None, description="GPU thermal and power throttling information"
      )
      warmup_result: WarmupResult | None = Field(
          default=None, description="Warmup convergence detection result"
      )
      latency_stats: LatencyStatistics | None = Field(
          default=None, description="Computed TTFT/ITL statistics from streaming inference"
      )
      extended_metrics: ExtendedEfficiencyMetrics | None = Field(
          default=None, description="Extended efficiency metrics (when computed)"
      )

      model_config = {"frozen": True}

      @property
      def duration_sec(self) -> float:
          """Total experiment duration."""
          return (self.end_time - self.start_time).total_seconds()

      @property
      def tokens_per_joule(self) -> float:
          """Overall energy efficiency."""
          if self.total_energy_j > 0:
              return self.total_tokens / self.total_energy_j
          return 0.0
  ```

  **Key changes from the current ExperimentResult:**
  1. `schema_version` default changed from `SCHEMA_VERSION` ("2.0.0") to `"2.0"` literal (design doc specifies "2.0"). Do NOT import `SCHEMA_VERSION` constant.
  2. Added: `measurement_config_hash`, `measurement_methodology`, `steady_state_window`, `baseline_power_w`, `energy_adjusted_j`, `energy_per_device_j`, `multi_gpu`, `warmup_excluded_samples`, `reproducibility_notes`, `warmup_result`
  3. Renamed: `timeseries_path` to `timeseries` (CONTEXT.md locked decision)
  4. `aggregation` changed from required field to `AggregationMetadata | None` (optional — not needed for single-GPU)
  5. `extended_metrics` changed from `Field(default_factory=ExtendedEfficiencyMetrics)` to `ExtendedEfficiencyMetrics | None = None` (only populated when computed)
  6. Removed v1.x-only fields: `cli_overrides`, `config_warnings`, `parameter_provenance`, `preset_chain`, `energy_tracking_failed`, `environment` (replaced by `environment_snapshot`)
  7. `measurement_warnings` kept (was added in Phase 5 Plan 02, but must ensure it's in v2.0 schema)

  **Preserve these unchanged:**
  - `Timestamps` class
  - `RawProcessResult` class (internal, not user-visible)
  - `AggregationMetadata` class
  - `StudyResult` class
  - `AggregatedResult = ExperimentResult` alias at bottom

  **Update imports at top of file:**
  - Add `import hashlib`, `import json` for the hash function
  - Add `from typing import Literal` to existing typing imports
  - Import `MultiGPUMetrics` from `domain.metrics`
  - Keep `from __future__ import annotations` to defer type evaluation
  - Use `TYPE_CHECKING` guard for `ExperimentConfig` import (needed by hash function)
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.domain.experiment import ExperimentResult, compute_measurement_config_hash, AggregatedResult
from llenergymeasure.domain.metrics import MultiGPUMetrics
from datetime import datetime

# Verify schema_version default
import inspect
sig = inspect.signature(ExperimentResult)
# Check key fields exist
fields = ExperimentResult.model_fields
assert 'measurement_config_hash' in fields, 'Missing measurement_config_hash'
assert 'measurement_methodology' in fields, 'Missing measurement_methodology'
assert 'steady_state_window' in fields, 'Missing steady_state_window'
assert 'baseline_power_w' in fields, 'Missing baseline_power_w'
assert 'energy_adjusted_j' in fields, 'Missing energy_adjusted_j'
assert 'energy_per_device_j' in fields, 'Missing energy_per_device_j'
assert 'multi_gpu' in fields, 'Missing multi_gpu'
assert 'reproducibility_notes' in fields, 'Missing reproducibility_notes'
assert 'warmup_excluded_samples' in fields, 'Missing warmup_excluded_samples'
assert 'timeseries' in fields, 'Missing timeseries'
assert 'schema_version' in fields, 'Missing schema_version'

# Verify alias preserved
assert AggregatedResult is ExperimentResult, 'AggregatedResult alias broken'

# Verify MultiGPUMetrics
m = MultiGPUMetrics(num_gpus=2, energy_per_gpu_j=[10.0, 12.0], energy_total_j=22.0, energy_per_output_token_j=0.001)
assert m.num_gpus == 2

# Verify hash function
from llenergymeasure.config.models import ExperimentConfig
cfg = ExperimentConfig(model='gpt2')
h = compute_measurement_config_hash(cfg)
assert len(h) == 16 and all(c in '0123456789abcdef' for c in h), f'Invalid hash: {h}'
print('All schema checks passed')
"</automated>
  </verify>
  <done>ExperimentResult has all v2.0 fields (RES-01 through RES-11). schema_version defaults to "2.0". measurement_config_hash, measurement_methodology, steady_state_window, energy detail fields, reproducibility_notes, warmup_excluded_samples all present. MultiGPUMetrics model exists. compute_measurement_config_hash() returns 16-char hex string. AggregatedResult alias preserved.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for ExperimentResult v2.0 schema</name>
  <files>
    tests/unit/test_experiment_result_v2.py
  </files>
  <action>
  **Create `tests/unit/test_experiment_result_v2.py`:**

  Write comprehensive unit tests (no GPU required):

  1. `test_schema_version_default_is_2_0()` — construct ExperimentResult with required fields, assert `schema_version == "2.0"` (not "2.0.0")
  2. `test_measurement_config_hash_field()` — construct with `measurement_config_hash="abcdef0123456789"`, verify it round-trips
  3. `test_measurement_methodology_total()` — `measurement_methodology="total"` validates
  4. `test_measurement_methodology_steady_state()` — `measurement_methodology="steady_state"` validates
  5. `test_measurement_methodology_invalid()` — `measurement_methodology="invalid"` raises ValidationError
  6. `test_steady_state_window_tuple()` — `steady_state_window=(12.3, 67.8)` validates and round-trips
  7. `test_steady_state_window_none()` — `steady_state_window=None` validates (methodology=total)
  8. `test_baseline_power_w_optional()` — field defaults to None, can be set to float
  9. `test_energy_adjusted_j_optional()` — field defaults to None, can be set to float
  10. `test_energy_per_device_j_optional()` — field defaults to None, can be set to list[float]
  11. `test_energy_breakdown_optional()` — can embed EnergyBreakdown or leave as None
  12. `test_multi_gpu_optional()` — can embed MultiGPUMetrics or leave as None
  13. `test_reproducibility_notes_default()` — verify default string contains "NVML" and "accuracy"
  14. `test_measurement_warnings_default_empty()` — defaults to empty list
  15. `test_warmup_excluded_samples_optional()` — defaults to None, can be set to int
  16. `test_timeseries_field()` — `timeseries="timeseries.parquet"` validates; `timeseries=None` validates
  17. `test_environment_snapshot_optional()` — can embed EnvironmentSnapshot or leave as None
  18. `test_frozen_model()` — assignment after construction raises ValidationError (frozen=True)
  19. `test_aggregated_result_alias()` — `AggregatedResult is ExperimentResult`
  20. `test_duration_sec_property()` — start + end times produce correct duration
  21. `test_tokens_per_joule_property()` — correct efficiency calculation
  22. `test_compute_measurement_config_hash()` — known ExperimentConfig produces 16-char hex
  23. `test_config_hash_deterministic()` — same config always produces same hash
  24. `test_config_hash_different_configs()` — different configs produce different hashes
  25. `test_multi_gpu_metrics_model()` — MultiGPUMetrics validates with all required fields
  26. `test_json_round_trip()` — `ExperimentResult.model_validate_json(result.model_dump_json())` round-trips all fields including datetime, tuple, and nested models

  Create a shared fixture `make_experiment_result(**overrides)` that constructs a valid ExperimentResult with minimal required fields:
  ```python
  @pytest.fixture
  def make_result():
      def _make(**kwargs):
          defaults = dict(
              experiment_id="test-001",
              measurement_config_hash="abcdef0123456789",
              measurement_methodology="total",
              total_tokens=1000,
              total_energy_j=50.0,
              total_inference_time_sec=10.0,
              avg_tokens_per_second=100.0,
              avg_energy_per_token_j=0.05,
              total_flops=1e12,
              start_time=datetime(2026, 2, 26, 14, 0, 0),
              end_time=datetime(2026, 2, 26, 14, 0, 10),
          )
          defaults.update(kwargs)
          return ExperimentResult(**defaults)
      return _make
  ```

  Import from the correct v2.0 modules: `from llenergymeasure.domain.experiment import ExperimentResult, AggregatedResult, compute_measurement_config_hash`, `from llenergymeasure.domain.metrics import EnergyBreakdown, MultiGPUMetrics`, `from llenergymeasure.config.models import ExperimentConfig`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_experiment_result_v2.py -x -v 2>&1 | tail -30</automated>
  </verify>
  <done>26+ unit tests pass confirming: schema_version="2.0", all v2.0 fields present and validate correctly, JSON round-trip works, config hash is deterministic and 16-char hex, MultiGPUMetrics validates, frozen model enforced, AggregatedResult alias works.</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.domain.experiment import ExperimentResult; assert ExperimentResult.model_fields['schema_version'].default == '2.0'"` — RES-05
2. `python -c "from llenergymeasure.domain.experiment import ExperimentResult; assert 'measurement_config_hash' in ExperimentResult.model_fields"` — RES-02
3. `python -c "from llenergymeasure.domain.experiment import compute_measurement_config_hash; from llenergymeasure.config.models import ExperimentConfig; h = compute_measurement_config_hash(ExperimentConfig(model='gpt2')); assert len(h) == 16"` — RES-02 hash
4. `python -c "from llenergymeasure.domain.experiment import AggregatedResult, ExperimentResult; assert AggregatedResult is ExperimentResult"` — v1.x compat
5. `python -c "from llenergymeasure.domain.metrics import MultiGPUMetrics"` — MultiGPUMetrics exists
6. `python -m pytest tests/unit/test_experiment_result_v2.py -x -v` — all tests pass
7. `python -c "from llenergymeasure import ExperimentResult; print('public API OK')"` — __init__.py still works
</verification>

<success_criteria>
- ExperimentResult.schema_version defaults to "2.0"
- All 11 RES requirements (RES-01 through RES-11) have corresponding fields
- compute_measurement_config_hash() returns 16-char hex deterministically
- MultiGPUMetrics model validates
- JSON round-trip preserves all fields (including datetime and tuple)
- AggregatedResult alias preserved
- 26+ unit tests pass
- Public API import (`from llenergymeasure import ExperimentResult`) still works
</success_criteria>

<output>
After completion, create `.planning/phases/06-results-schema-and-persistence/06-01-SUMMARY.md`
</output>
