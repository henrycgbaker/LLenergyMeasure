---
phase: 02-campaign-orchestrator
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/campaign_config.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "CampaignConfig supports two-level grid definition (shared + backend-specific params)"
    - "CampaignConfig supports explicit experiment list alongside grid"
    - "CampaignConfig has force_cold_start option (default false)"
    - "CampaignConfig has daemon schedule with interval and total_duration"
    - "CampaignConfig has health_check configuration"
    - "CampaignConfig has IO path configuration for campaign directories"
    - "python-on-whales is declared as a dependency"
  artifacts:
    - path: "src/llenergymeasure/config/campaign_config.py"
      provides: "Extended CampaignConfig with grid, manifest, cold start, health check, daemon, IO models"
      contains: "class CampaignGridConfig"
    - path: "pyproject.toml"
      provides: "python-on-whales dependency declaration"
      contains: "python-on-whales"
  key_links:
    - from: "src/llenergymeasure/config/campaign_config.py"
      to: "CampaignGridConfig"
      via: "grid field on CampaignConfig"
      pattern: "grid.*CampaignGridConfig"
---

<objective>
Extend CampaignConfig Pydantic models with all Phase 2 configuration fields and add python-on-whales dependency.

Purpose: All other Phase 2 plans depend on these config models. This is the data layer foundation — grid definition, manifest schema, cold start, health checks, daemon scheduling, and IO paths.
Output: Extended campaign_config.py with ~8 new Pydantic models, python-on-whales in pyproject.toml.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-CONTEXT.md
@.planning/phases/02-campaign-orchestrator/02-RESEARCH.md
@src/llenergymeasure/config/campaign_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend CampaignConfig with Phase 2 models</name>
  <files>src/llenergymeasure/config/campaign_config.py</files>
  <action>
Extend the existing campaign_config.py with these new Pydantic models. Keep all existing models (CampaignExecutionConfig, CampaignScheduleConfig, CampaignConfig) intact — this is additive.

**New models to add:**

1. `CampaignGridConfig(BaseModel)` — Two-level grid definition:
   - `backends: list[str]` — Backends to sweep (e.g., ["pytorch", "vllm"]). Validated against known backends.
   - `models: list[str] | None = None` — Model names to sweep
   - `shared: dict[str, list[Any]] = {}` — Shared grid axes (e.g., `{"fp_precision": ["float16", "bfloat16"]}`)
   - `backend_overrides: dict[str, dict[str, Any]] = {}` — Per-backend param overrides keyed by backend name (e.g., `{"pytorch": {"batch_size": [1, 4, 8]}, "vllm": {"max_num_seqs": [1, 4, 8]}}`)
   - Field validator: backends must be in {"pytorch", "vllm", "tensorrt"}

2. `CampaignExplicitExperiment(BaseModel)` — Single explicit experiment entry:
   - `config: str` — Path to experiment config YAML
   - `overrides: dict[str, Any] = {}` — Optional inline overrides applied on top of config

3. `CampaignHealthCheckConfig(BaseModel)` — Container health monitoring:
   - `enabled: bool = True`
   - `interval_experiments: int = Field(default=0, ge=0)` — Check every N experiments (0 = disabled, check per-cycle only)
   - `gpu_memory_threshold_pct: float = Field(default=90.0, ge=50.0, le=100.0)` — GPU memory % threshold for unhealthy
   - `restart_on_failure: bool = True`
   - `max_restarts: int = Field(default=3, ge=0)` — Max container restarts per campaign

4. `CampaignColdStartConfig(BaseModel)` — Cold start benchmarking:
   - `force_cold_start: bool = False` — Unload model between experiments
   - `restart_container: bool = False` — Full container restart for guaranteed clean state (slower but complete)

5. `CampaignDaemonConfig(BaseModel)` — Extends existing CampaignScheduleConfig:
   - `enabled: bool = False`
   - `at: str | None = None` — Start time (HH:MM). Reuse the existing time validator from CampaignScheduleConfig.
   - `interval: str | None = None` — Between cycles (e.g., "6h", "30m"). Field validator parses to seconds.
   - `total_duration: str | None = None` — Max campaign duration (e.g., "48h"). Field validator parses to seconds.
   - `quiet: bool = True` — Suppress interactive output in daemon mode
   - Helper methods: `interval_seconds` property, `total_duration_seconds` property
   - Field validators for interval/total_duration: accept "Nh" or "Nm" format, reject invalid

6. `CampaignIOConfig(BaseModel)` — IO path configuration:
   - `results_dir: str = "results"` — Results output directory
   - `configs_dir: str = "configs"` — Config files directory
   - `state_dir: str = ".state"` — State/manifest directory
   - `manifest_filename: str = "campaign_manifest.json"`
   - Property: `manifest_path` returns `Path(state_dir) / manifest_filename`

**Extend existing CampaignConfig** with new fields:
- `grid: CampaignGridConfig | None = None` — Grid definition (mutually supportive with `configs`)
- `experiments: list[CampaignExplicitExperiment] = []` — Explicit experiment list
- `health_check: CampaignHealthCheckConfig = Field(default_factory=CampaignHealthCheckConfig)`
- `cold_start: CampaignColdStartConfig = Field(default_factory=CampaignColdStartConfig)`
- `daemon: CampaignDaemonConfig | None = None` — Daemon/scheduling (replaces `schedule` field over time, keep `schedule` for backwards compat)
- `io: CampaignIOConfig = Field(default_factory=CampaignIOConfig)`

**Model validator on CampaignConfig:**
- At least one of `configs` (list), `grid`, or `experiments` must be provided. Make `configs` field default to `[]` (not required) since grid can now generate configs.
- Update `validate_configs_exist` to only run when `configs` is non-empty.

**Important details:**
- Keep `from __future__ import annotations` at top (existing)
- Follow existing field documentation patterns (description= on each Field)
- Add all new models to `__all__`
- Don't remove or modify any existing model behaviour — purely additive
  </action>
  <verify>
    python -c "
from llenergymeasure.config.campaign_config import (
    CampaignConfig, CampaignGridConfig, CampaignHealthCheckConfig,
    CampaignColdStartConfig, CampaignDaemonConfig, CampaignIOConfig,
    CampaignExplicitExperiment
)
# Test grid config
grid = CampaignGridConfig(backends=['pytorch', 'vllm'])
assert grid.backends == ['pytorch', 'vllm']
# Test health check defaults
hc = CampaignHealthCheckConfig()
assert hc.enabled is True
assert hc.gpu_memory_threshold_pct == 90.0
# Test cold start defaults
cs = CampaignColdStartConfig()
assert cs.force_cold_start is False
# Test IO defaults
io = CampaignIOConfig()
assert io.results_dir == 'results'
# Test CampaignConfig with grid (no configs needed)
c = CampaignConfig(campaign_name='test', grid=CampaignGridConfig(backends=['pytorch']))
assert c.grid is not None
# Test backwards compat: existing configs-only mode still works
import tempfile, yaml
from pathlib import Path
td = tempfile.mkdtemp()
cfg = Path(td) / 'test.yaml'
cfg.write_text(yaml.dump({'model': {'name': 'gpt2'}, 'backend': 'pytorch'}))
c2 = CampaignConfig(campaign_name='test2', configs=[str(cfg)])
assert len(c2.configs) == 1
print('All config model checks passed')
"
  </verify>
  <done>All Phase 2 Pydantic config models created with correct defaults, validation, and backwards compatibility. CampaignConfig accepts grid, explicit experiments, or configs list.</done>
</task>

<task type="auto">
  <name>Task 2: Add python-on-whales dependency</name>
  <files>pyproject.toml</files>
  <action>
Add python-on-whales as an optional dependency in pyproject.toml under a new `campaign` extras group.

In the `[project.optional-dependencies]` section, add:
```
campaign = ["python-on-whales>=0.70"]
```

Also add it to the `dev` extras group so it's available during development.

Do NOT add it as a core dependency — it's only needed when orchestrating Docker containers from the host. Users running inside containers don't need it.
  </action>
  <verify>
    cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && pip install -e ".[campaign]" 2>&1 | tail -5 && python -c "from python_on_whales import DockerClient; print('python-on-whales importable')"
  </verify>
  <done>python-on-whales>=0.70 is available as optional `campaign` dependency and installs cleanly.</done>
</task>

</tasks>

<verification>
- `python -c "from llenergymeasure.config.campaign_config import CampaignConfig, CampaignGridConfig"` succeeds
- All existing tests pass: `pytest tests/unit/config/ -x -q`
- python-on-whales importable after `pip install -e ".[campaign]"`
</verification>

<success_criteria>
- CampaignConfig extended with grid, health_check, cold_start, daemon, io, notification, experiments fields
- 7+ new Pydantic models with validation
- Backwards compatible: existing campaign YAML files still parse
- python-on-whales declared and installable
</success_criteria>

<output>
After completion, create `.planning/phases/02-campaign-orchestrator/02-01-SUMMARY.md`
</output>
