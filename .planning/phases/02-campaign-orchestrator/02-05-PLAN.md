---
phase: 02-campaign-orchestrator
plan: 05
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/llenergymeasure/orchestration/grid.py
autonomous: true

must_haves:
  truths:
    - "Grid expander generates cartesian product from campaign grid config"
    - "Grid expansion applies backend-specific overrides per backend"
    - "Each generated config is validated via ExperimentConfig Pydantic instantiation (dry-run)"
    - "Invalid configs are filtered with reasons, not silently dropped"
    - "Grid validation uses SSOT introspection for backend-param compatibility"
    - "Summary report shows valid/filtered/warned counts per backend"
  artifacts:
    - path: "src/llenergymeasure/orchestration/grid.py"
      provides: "expand_campaign_grid() and validate_campaign_grid() functions"
      exports: ["expand_campaign_grid", "validate_campaign_grid", "GridExpansionResult"]
      min_lines: 100
  key_links:
    - from: "src/llenergymeasure/orchestration/grid.py"
      to: "src/llenergymeasure/config/models.py"
      via: "ExperimentConfig instantiation for validation"
      pattern: "ExperimentConfig"
    - from: "src/llenergymeasure/orchestration/grid.py"
      to: "src/llenergymeasure/config/introspection.py"
      via: "SSOT backend capability checking"
      pattern: "get_backend_params|get_backend_capabilities"
    - from: "src/llenergymeasure/orchestration/grid.py"
      to: "src/llenergymeasure/config/campaign_config.py"
      via: "CampaignGridConfig input"
      pattern: "CampaignGridConfig"
---

<objective>
Implement backend-aware grid generation and validation for campaign configs.

Purpose: CAMP-02 requirement — users define a two-level grid (shared + backend-specific params) and the system generates valid experiment configs via cartesian product, filtering invalid backend x param combinations automatically. Uses Pydantic-first dry-run validation as decided in CONTEXT.md.
Output: New orchestration/grid.py with grid expansion and validation functions.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-CONTEXT.md
@.planning/phases/02-campaign-orchestrator/02-RESEARCH.md
@.planning/phases/02-campaign-orchestrator/02-01-SUMMARY.md
@src/llenergymeasure/config/campaign_config.py
@src/llenergymeasure/config/models.py
@src/llenergymeasure/config/introspection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create grid expansion module</name>
  <files>src/llenergymeasure/orchestration/grid.py</files>
  <action>
Create `src/llenergymeasure/orchestration/grid.py` with campaign grid expansion and validation.

**GridValidationIssue(BaseModel):**
- `config_desc: str` — Human-readable description of the config (e.g., "pytorch/float16/batch_size=4")
- `reason: str` — Why it was filtered
- `severity: Literal["error", "warning"]` — Error = invalid (filtered), Warning = may fail at runtime (kept)

**GridExpansionResult(BaseModel):**
- `valid_configs: list[dict[str, Any]]` — Config dicts that passed validation
- `filtered_configs: list[GridValidationIssue]` — Configs that failed Pydantic validation
- `warnings: list[GridValidationIssue]` — Configs with hardware/env warnings (still included in valid)
- `total_generated: int` — Total configs before filtering
- Property: `summary` — Returns human-readable summary string: "Generated N experiments (M filtered as invalid: reasons...). W have hardware warnings."

**expand_campaign_grid function:**
```python
def expand_campaign_grid(
    grid: CampaignGridConfig,
    base_config: dict[str, Any] | None = None,
) -> list[dict[str, Any]]:
    """Expand campaign grid into individual experiment config dicts.

    Generates cartesian product of:
    - backends x models x shared params x backend-specific params

    Args:
        grid: Campaign grid configuration.
        base_config: Optional base config dict to merge into each variation.

    Returns:
        List of config dicts (one per experiment).
    """
```

Implementation:
1. Start with `base_config or {}`
2. Build axes for cartesian product:
   - `backends` axis from `grid.backends`
   - `models` axis from `grid.models` (if provided, else [None])
   - For each key in `grid.shared`, create an axis
3. Generate cartesian product using `itertools.product`
4. For each combination:
   - Set `backend` field
   - Set `model.name` field (if model is not None)
   - Set shared params (handle nested keys like `decoder.preset` — split on "." and set nested dict)
   - Apply backend-specific overrides from `grid.backend_overrides[backend]` if present
   - If backend_overrides contain lists, expand those too (nested cartesian product per backend)
5. Return list of config dicts

**validate_campaign_grid function:**
```python
def validate_campaign_grid(
    config_dicts: list[dict[str, Any]],
) -> GridExpansionResult:
    """Validate expanded grid configs via Pydantic dry-run instantiation.

    Two-tier validation:
    1. Pydantic: Attempt ExperimentConfig(**config_dict) — catches invalid combos
    2. SSOT: Check backend capabilities and skip conditions — adds warnings

    Args:
        config_dicts: List of experiment config dicts from expand_campaign_grid.

    Returns:
        GridExpansionResult with valid configs, filtered configs, and warnings.
    """
```

Implementation:
1. For each config_dict in config_dicts:
   a. Try `ExperimentConfig(**config_dict)` — if ValidationError, add to filtered_configs with reason
   b. If valid, check SSOT warnings:
      - Import `get_backend_capabilities, get_param_skip_conditions` from introspection
      - Check if any params have hardware skip conditions (e.g., needs Ampere GPU)
      - If warnings found, add to warnings list but KEEP config in valid_configs
2. Build and return GridExpansionResult

**Helper function for nested dict setting:**
```python
def _set_nested(d: dict, key_path: str, value: Any) -> None:
    """Set a value in a nested dict using dot notation (e.g., 'decoder.preset')."""
```

**Important details:**
- Import ExperimentConfig from `config.models` (not loader)
- Import introspection functions for Tier 2 checks
- Handle the case where ExperimentConfig needs certain required fields — if base_config doesn't provide them, the Pydantic validation will catch it (that's correct behaviour)
- Use `from __future__ import annotations`
- Use loguru for debug logging
- Add all public functions and models to `__all__`
  </action>
  <verify>
    python -c "
from llenergymeasure.orchestration.grid import (
    expand_campaign_grid, validate_campaign_grid, GridExpansionResult
)
from llenergymeasure.config.campaign_config import CampaignGridConfig

# Test basic grid expansion
grid = CampaignGridConfig(
    backends=['pytorch'],
    shared={'fp_precision': ['float16', 'bfloat16']},
)
base = {'model': {'name': 'gpt2'}, 'max_output_tokens': 32}
configs = expand_campaign_grid(grid, base_config=base)
assert len(configs) == 2  # 1 backend x 2 precisions
assert all(c['backend'] == 'pytorch' for c in configs)
print(f'Basic expansion: {len(configs)} configs')

# Test multi-backend expansion
grid2 = CampaignGridConfig(
    backends=['pytorch', 'vllm'],
    shared={'fp_precision': ['float16']},
    backend_overrides={
        'pytorch': {'batch_size': [1, 4]},
        'vllm': {'max_num_seqs': [1, 4]},
    }
)
configs2 = expand_campaign_grid(grid2, base_config=base)
# pytorch: 1 precision x 2 batch_sizes = 2
# vllm: 1 precision x 2 max_num_seqs = 2
assert len(configs2) == 4
print(f'Multi-backend expansion: {len(configs2)} configs')

# Test validation (GridExpansionResult)
result = validate_campaign_grid(configs)
assert isinstance(result, GridExpansionResult)
assert result.total_generated == 2
print(f'Validation: {len(result.valid_configs)} valid, {len(result.filtered_configs)} filtered')

print('All grid tests passed')
"
  </verify>
  <done>Grid expansion generates correct cartesian products with backend-specific overrides. Pydantic-first validation filters invalid combos with reasons. SSOT warnings for hardware-dependent configs.</done>
</task>

</tasks>

<verification>
- `python -c "from llenergymeasure.orchestration.grid import expand_campaign_grid"` succeeds
- Grid produces correct cartesian product count
- Invalid configs filtered with human-readable reasons
- `ruff check src/llenergymeasure/orchestration/grid.py` passes
</verification>

<success_criteria>
- Cartesian product of backends x models x shared params x backend-specific params
- Backend-specific overrides applied correctly per backend
- Pydantic dry-run validation filters invalid combos
- SSOT introspection adds hardware warnings (kept in valid, flagged)
- Summary report shows valid/filtered/warned counts
</success_criteria>

<output>
After completion, create `.planning/phases/02-campaign-orchestrator/02-05-SUMMARY.md`
</output>
