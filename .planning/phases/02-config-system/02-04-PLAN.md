---
phase: 02-config-system
plan: 04
type: execute
wave: 2
depends_on: [02-01]
files_modified:
  - src/llenergymeasure/config/introspection.py
  - src/llenergymeasure/config/__init__.py
autonomous: true
requirements: [CFG-02, CFG-03]

must_haves:
  truths:
    - "config/introspection.py returns the full ExperimentConfig JSON schema"
    - "Per-field metadata includes backend_support: list[str] indicating which backends expose each param"
    - "get_backend_params() works against the new v2.0 PyTorchConfig, VLLMConfig, TensorRTConfig"
    - "get_shared_params() reflects v2.0 field renames: precision (not fp_precision), n (not num_input_prompts)"
    - "config/__init__.py exports the clean public surface for the config subsystem"
  artifacts:
    - path: "src/llenergymeasure/config/introspection.py"
      provides: "SSOT introspection with backend_support metadata and get_experiment_config_schema()"
      exports: ["get_backend_params", "get_shared_params", "get_experiment_config_schema"]
    - path: "src/llenergymeasure/config/__init__.py"
      provides: "Config subsystem public API"
      exports: ["ExperimentConfig", "load_experiment_config", "load_user_config"]
  key_links:
    - from: "src/llenergymeasure/config/introspection.py"
      to: "src/llenergymeasure/config/models.py"
      via: "ExperimentConfig.model_json_schema()"
      pattern: "model_json_schema"
    - from: "src/llenergymeasure/config/introspection.py"
      to: "src/llenergymeasure/config/backend_configs.py"
      via: "get_params_from_model(PyTorchConfig, ...)"
      pattern: "get_params_from_model"
---

<objective>
Update introspection.py for v2.0 field renames and add backend_support metadata + JSON schema export. Update config/__init__.py to expose the clean v2.0 public surface.

Purpose: Introspection is the SSOT for parameter metadata consumed by the test suite, future CLI, and doc generator. Must reflect v2.0 field names and backend capability correctly.
Output: introspection.py with updated field paths + backend_support, config/__init__.py with clean exports.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/02-config-system/02-CONTEXT.md
@.planning/phases/02-config-system/02-01-SUMMARY.md

<interfaces>
<!-- From Plan 01 output — v2.0 field names -->

ExperimentConfig (v2.0) shared fields:
  model, backend, precision, n, random_seed,
  max_input_tokens, max_output_tokens,
  decoder (DecoderConfig), warmup (WarmupConfig), baseline (BaselineConfig),
  pytorch, vllm, tensorrt (backend sections),
  lora, passthrough_kwargs, output_dir

DecoderConfig fields: temperature, do_sample, top_k, top_p, repetition_penalty, preset

PyTorchConfig (v2.0): batch_size, attn_implementation, torch_compile, load_in_4bit, load_in_8bit, num_processes
VLLMConfig (v2.0): max_num_seqs, tensor_parallel_size, gpu_memory_utilization, enable_prefix_caching, quantization
TensorRTConfig (v2.0): max_batch_size, tp_size, quantization, engine_path

<!-- Decisions from CONTEXT.md -->
Scope split: Phase 2 = update for field renames + add backend_support.
Phase 4.1 = full param completeness audit.
Architecture unchanged: introspection reflects backend configs as standalone classes.
Per-field metadata: ADD backend_support: list[str] alongside existing fields.
Consumers: test suite (zero-maintenance test generation), doc generator.
Doc generation deferred: Phase 8 only.
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update introspection.py for v2.0 fields and add backend_support</name>
  <files>src/llenergymeasure/config/introspection.py</files>
  <action>
Update introspection.py to reflect v2.0 field renames and add `backend_support` metadata. The core introspection infrastructure (_extract_param_metadata, get_params_from_model) is solid — keep it. Update the higher-level functions.

**Changes required:**

1. **get_shared_params() — update field names:**
   - `fp_precision` → `precision` (now a top-level ExperimentConfig field, not a shared subfield path)
   - `streaming` → REMOVED (not in v2.0 ExperimentConfig)
   - `max_input_tokens`, `max_output_tokens` stay — update defaults/constraints if needed
   - Add `n` (was `num_input_prompts`): `int`, default=100, ge=1
   - Keep `decoder.*` sub-fields from `DecoderConfig` model

   Updated `get_shared_params()`:
   ```python
   def get_shared_params() -> dict[str, dict[str, Any]]:
       """Get shared/universal parameters from ExperimentConfig.

       Returns params that are universal across all backends:
       - Top-level: precision, n, max_input_tokens, max_output_tokens, random_seed
       - Decoder: temperature, do_sample, top_p, top_k, repetition_penalty, preset
       """
       from llenergymeasure.config.models import DecoderConfig

       shared: dict[str, dict[str, Any]] = {}

       # Decoder params (introspected from model)
       decoder_params = get_params_from_model(DecoderConfig, prefix="decoder")
       shared.update(decoder_params)

       # Top-level universal params — defined manually for explicit backend_support
       shared["precision"] = {
           "path": "precision",
           "name": "precision",
           "type_str": "literal",
           "default": "bf16",
           "description": "Floating point precision",
           "options": ["fp32", "fp16", "bf16"],
           "test_values": ["fp32", "fp16", "bf16"],
           "constraints": {},
           "optional": False,
           "backend_support": ["pytorch", "vllm", "tensorrt"],
       }
       shared["n"] = {
           "path": "n",
           "name": "n",
           "type_str": "int",
           "default": 100,
           "description": "Number of prompts from dataset",
           "options": None,
           "test_values": [10, 100, 500],
           "constraints": {"ge": 1},
           "optional": False,
           "backend_support": ["pytorch", "vllm", "tensorrt"],
       }
       shared["max_input_tokens"] = {
           "path": "max_input_tokens",
           "name": "max_input_tokens",
           "type_str": "int",
           "default": 512,
           "description": "Maximum input token length",
           "options": None,
           "test_values": [64, 128, 256],
           "constraints": {"ge": 1},
           "optional": False,
           "backend_support": ["pytorch", "vllm", "tensorrt"],
       }
       shared["max_output_tokens"] = {
           "path": "max_output_tokens",
           "name": "max_output_tokens",
           "type_str": "int",
           "default": 128,
           "description": "Maximum output token length",
           "options": None,
           "test_values": [32, 128, 256],
           "constraints": {"ge": 1},
           "optional": False,
           "backend_support": ["pytorch", "vllm", "tensorrt"],
       }
       return shared
   ```

2. **Add backend_support to backend params:**
   Update `get_backend_params()` to add `backend_support` to each returned param:
   ```python
   def get_backend_params(backend: str) -> dict[str, dict[str, Any]]:
       ...
       params = get_params_from_model(model_class, prefix=backend)
       # Add backend_support to every param
       for param in params.values():
           param["backend_support"] = [backend]
       # Apply custom test value overrides
       ...
       return params
   ```

3. **Add get_experiment_config_schema():**
   ```python
   def get_experiment_config_schema() -> dict[str, Any]:
       """Return the full ExperimentConfig JSON schema (Pydantic v2 schema).

       Returns:
           JSON-serialisable dict with the complete schema including all
           properties, types, constraints, and nested model schemas.
           Uses Pydantic's built-in model_json_schema() — always in sync
           with the actual model definition.
       """
       from llenergymeasure.config.models import ExperimentConfig
       return ExperimentConfig.model_json_schema()
   ```

4. **Remove stale functions** that reference v1.x-only concepts:
   - `get_campaign_params()` — CampaignConfig is gone (M2 scope)
   - `get_campaign_grid_params()` — same
   - `get_campaign_health_check_params()` — same
   These import from `llenergymeasure.config.campaign_config` which no longer exists.

5. **Keep as-is:**
   - `_extract_param_metadata()` — solid implementation, keep
   - `get_params_from_model()` — solid, keep
   - `_get_custom_test_values()` — update paths if needed (remove vllm.max_num_batched_tokens if that field no longer exists in v2.0 VLLMConfig)
   - `get_mutual_exclusions()` — update for v2.0 fields
   - `get_backend_specific_params()` — update to reflect v2.0 minimal backend config fields
   - `get_backend_capabilities()` — keep, will auto-adapt as backend configs are updated
   - `get_capability_matrix_markdown()` — keep
   - `get_param_skip_conditions()` — update for v2.0 fields (remove references to fields that no longer exist)
   - `get_validation_rules()` — update for v2.0 cross-validators
   - `list_all_param_paths()`, `get_param_test_values()`, `get_param_options()` — keep as-is
   - `get_streaming_constraints()`, `get_streaming_incompatible_tests()` — streaming is Phase 5, remove these or keep as stubs returning empty

6. **Update _get_custom_test_values()** — remove paths for fields that no longer exist in v2.0 VLLMConfig:
   ```python
   def _get_custom_test_values() -> dict[str, list[Any]]:
       return {
           # vLLM: max_model_len no longer in v2.0 minimal VLLMConfig — remove
           # TensorRT: max_input_len no longer in v2.0 minimal TensorRTConfig — remove
       }
       # Return empty dict — no special overrides needed for v2.0 minimal configs
       return {}
   ```

7. **Update get_all_params()** to use updated functions:
   ```python
   def get_all_params() -> dict[str, dict[str, dict[str, Any]]]:
       return {
           "shared": get_shared_params(),
           "pytorch": get_backend_params("pytorch"),
           "vllm": get_backend_params("vllm"),
           "tensorrt": get_backend_params("tensorrt"),
       }
   ```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.config.introspection import (
    get_shared_params, get_backend_params, get_experiment_config_schema, get_all_params
)

# Shared params use v2.0 names
shared = get_shared_params()
assert 'precision' in shared, f'precision not in shared: {list(shared.keys())}'
assert 'n' in shared, f'n not in shared: {list(shared.keys())}'
assert 'fp_precision' not in shared, 'fp_precision should not be in v2.0 shared params'
assert 'num_input_prompts' not in shared, 'num_input_prompts should not be in v2.0 shared params'

# backend_support field present
prec = shared['precision']
assert 'backend_support' in prec
assert 'pytorch' in prec['backend_support']

# Backend params work
pytorch_params = get_backend_params('pytorch')
assert len(pytorch_params) > 0
for param in pytorch_params.values():
    assert 'backend_support' in param
    assert param['backend_support'] == ['pytorch']

# JSON schema returns dict
schema = get_experiment_config_schema()
assert isinstance(schema, dict)
assert 'properties' in schema
assert 'model' in schema['properties']
assert 'precision' in schema['properties']

print('Introspection checks passed')
"</automated>
  </verify>
  <done>get_shared_params() returns 'precision' and 'n' (not 'fp_precision' or 'num_input_prompts'); all params have 'backend_support' field; get_experiment_config_schema() returns a dict with 'properties' containing 'model' and 'precision'</done>
</task>

<task type="auto">
  <name>Task 2: Update config/__init__.py to export v2.0 public surface</name>
  <files>src/llenergymeasure/config/__init__.py</files>
  <action>
Rewrite config/__init__.py to expose a clean v2.0 public surface. The config subsystem's public API for use by Phase 3 (library API) and Phase 7 (CLI).

```python
"""Configuration subsystem for llenergymeasure.

Public API:
- ExperimentConfig: Main experiment configuration model
- load_experiment_config: Load from YAML/JSON with CLI override support
- load_user_config: Load user preferences from XDG config dir
- get_user_config_path: Return the XDG user config path
"""

from llenergymeasure.config.models import (
    ExperimentConfig,
    DecoderConfig,
    WarmupConfig,
    BaselineConfig,
    SyntheticDatasetConfig,
    LoRAConfig,
)
from llenergymeasure.config.loader import (
    load_experiment_config,
    deep_merge,
)
from llenergymeasure.config.user_config import (
    UserConfig,
    load_user_config,
    get_user_config_path,
)

__all__ = [
    # Core config model
    "ExperimentConfig",
    "DecoderConfig",
    "WarmupConfig",
    "BaselineConfig",
    "SyntheticDatasetConfig",
    "LoRAConfig",
    # Loading
    "load_experiment_config",
    "deep_merge",
    # User config
    "UserConfig",
    "load_user_config",
    "get_user_config_path",
]
```

NOTE: Do NOT export backend configs (PyTorchConfig, etc.) from this __init__.py — they are accessed as sub-fields of ExperimentConfig, not directly imported by users. The library API (Phase 3) will decide what goes into the top-level `llenergymeasure.__init__.py`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.config import (
    ExperimentConfig, DecoderConfig, WarmupConfig, BaselineConfig,
    load_experiment_config, load_user_config, get_user_config_path,
    UserConfig, __all__
)
assert 'ExperimentConfig' in __all__
assert 'load_experiment_config' in __all__
assert 'load_user_config' in __all__
assert 'get_user_config_path' in __all__

# Smoke test
cfg = ExperimentConfig(model='gpt2')
assert cfg.model == 'gpt2'
print('config/__init__.py checks passed')
"</automated>
  </verify>
  <done>from llenergymeasure.config import ExperimentConfig, load_experiment_config, load_user_config all succeed; __all__ lists all public names; ExperimentConfig(model="gpt2") works</done>
</task>

</tasks>

<verification>
```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool
python -c "
# Full integration check across all 4 plans
from llenergymeasure.config import ExperimentConfig, load_experiment_config, load_user_config
from llenergymeasure.config.introspection import get_experiment_config_schema, get_shared_params, get_backend_params

# Schema has correct field names
schema = get_experiment_config_schema()
props = schema.get('properties', {})
assert 'model' in props
assert 'precision' in props
assert 'n' in props
assert 'fp_precision' not in props
assert 'model_name' not in props
assert 'num_input_prompts' not in props

# Params have backend_support
shared = get_shared_params()
for name, meta in shared.items():
    assert 'backend_support' in meta, f'{name} missing backend_support'

pytorch = get_backend_params('pytorch')
for name, meta in pytorch.items():
    assert 'backend_support' in meta, f'{name} missing backend_support'
    assert 'pytorch' in meta['backend_support']

print('Phase 2 integration checks passed')
"
```
</verification>

<success_criteria>
- `get_shared_params()` returns `precision` and `n` (not `fp_precision`/`num_input_prompts`)
- All param metadata dicts include `backend_support: list[str]`
- `get_experiment_config_schema()` returns a JSON-serialisable dict with `properties.model` and `properties.precision`
- `from llenergymeasure.config import ExperimentConfig, load_experiment_config` succeeds
- No import errors from removed v1.x functions (campaign_config, etc.)
</success_criteria>

<output>
After completion, create `.planning/phases/02-config-system/02-04-SUMMARY.md`
</output>
