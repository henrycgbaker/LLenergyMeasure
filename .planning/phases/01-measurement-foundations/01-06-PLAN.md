---
phase: 01-measurement-foundations
plan: 06
type: execute
wave: 4
depends_on: ["01-05"]
files_modified:
  - tests/unit/test_core_power_thermal.py
  - tests/unit/test_core_baseline.py
  - tests/unit/test_core_warmup.py
  - tests/unit/test_core_environment.py
  - tests/unit/test_domain_schema_v3.py
  - tests/unit/test_results_exporters_v3.py
  - tests/unit/test_results_timeseries.py
autonomous: false

must_haves:
  truths:
    - "Unit tests cover all new Phase 1 modules (power_thermal, baseline, warmup, environment, timeseries)"
    - "Schema v3 domain model tests verify backwards compatibility with v2 results"
    - "CSV export tests verify new grouped-prefix columns"
    - "All tests pass without GPU (mocked NVML)"
    - "UAT: fresh clone, install, validate tool runs (checkpoint)"
  artifacts:
    - path: "tests/unit/test_core_power_thermal.py"
      provides: "PowerThermalSampler tests with mocked NVML"
    - path: "tests/unit/test_core_baseline.py"
      provides: "Baseline measurement tests (caching, adjustment, energy breakdown)"
    - path: "tests/unit/test_core_warmup.py"
      provides: "Warmup convergence tests (convergence, non-convergence, disabled, fixed mode)"
    - path: "tests/unit/test_core_environment.py"
      provides: "Environment metadata collection tests"
    - path: "tests/unit/test_domain_schema_v3.py"
      provides: "Schema v3 model tests (new fields, backwards compat)"
    - path: "tests/unit/test_results_exporters_v3.py"
      provides: "Extended CSV export tests"
    - path: "tests/unit/test_results_timeseries.py"
      provides: "Time-series export/load tests"
  key_links:
    - from: "tests/unit/"
      to: "src/llenergymeasure/"
      via: "import and test all new Phase 1 modules"
      pattern: "from llenergymeasure"
---

<objective>
Write comprehensive unit tests for all Phase 1 modules and conduct UAT round 1 (fresh clone validation).

Purpose: MEAS-09 (UAT) plus test coverage for all new code. Tests run without GPU using mocked NVML.
Output: 7 new test files covering all Phase 1 functionality. UAT checkpoint validates end-to-end installation.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-CONTEXT.md

Key source files and test patterns:
@tests/conftest.py
@tests/unit/test_config_models.py  (testing pattern reference)
@tests/unit/test_core_inference.py  (testing pattern reference)

Prior plan summaries:
@.planning/phases/01-measurement-foundations/01-01-SUMMARY.md
@.planning/phases/01-measurement-foundations/01-02-SUMMARY.md
@.planning/phases/01-measurement-foundations/01-03-SUMMARY.md
@.planning/phases/01-measurement-foundations/01-04-SUMMARY.md
@.planning/phases/01-measurement-foundations/01-05-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Unit tests for Phase 1 modules</name>
  <files>
    tests/unit/test_core_power_thermal.py
    tests/unit/test_core_baseline.py
    tests/unit/test_core_warmup.py
    tests/unit/test_core_environment.py
    tests/unit/test_domain_schema_v3.py
    tests/unit/test_results_exporters_v3.py
    tests/unit/test_results_timeseries.py
  </files>
  <action>
    Create 7 test files following existing test patterns (see test_config_models.py, test_core_inference.py for style). All tests must run WITHOUT GPU — mock pynvml where needed.

    **1. `test_domain_schema_v3.py`** — Schema v3 model tests:
    - `TestEnergyBreakdown`: Test construction, None defaults, with baseline values
    - `TestThermalThrottleInfo`: Test defaults (all False), with throttle detected
    - `TestWarmupResult`: Test converged/not-converged states
    - `TestEnvironmentMetadata`: Test construction, `summary_line` property output format
    - `TestBackwardsCompatibility`: Construct RawProcessResult WITHOUT new fields (v2 style), verify it works. Construct WITH new fields (v3 style), verify it works. Both must succeed.

    **2. `test_core_power_thermal.py`** — PowerThermalSampler tests:
    - `test_imports`: Verify module imports
    - `test_sampler_without_gpu`: Create sampler, start/stop without GPU, verify empty samples, verify `is_available == False`
    - `test_thermal_throttle_info_no_samples`: Verify `get_thermal_throttle_info()` returns all-false when no samples
    - `test_power_thermal_sample_dataclass`: Verify dataclass fields
    - `test_context_manager_pattern`: Use `with PowerThermalSampler() as s:` pattern, verify no crash

    **3. `test_core_baseline.py`** — Baseline measurement tests:
    - `test_adjust_energy_for_baseline`: 100J - 10W*5s = 50J
    - `test_adjust_energy_floor_at_zero`: 10J - 50W*5s = 0J (not negative)
    - `test_create_energy_breakdown_no_baseline`: Verify raw_j set, adjusted_j is None, method="unavailable"
    - `test_create_energy_breakdown_with_baseline`: Verify adjusted_j computed, baseline fields populated
    - `test_baseline_cache_invalidation`: Measure (mock), invalidate, verify cache cleared
    - `test_measure_baseline_caching`: Mock pynvml, measure twice, verify second call uses cache (not re-measured)
    - Mock pynvml using `unittest.mock.patch`

    **4. `test_core_warmup.py`** — Warmup convergence tests:
    - `test_disabled_warmup`: WarmupConfig(enabled=False), verify converged=True, iterations=0
    - `test_convergence_with_stable_latencies`: Callable returns ~100ms with <2% noise, verify converges before max
    - `test_non_convergence_with_noisy_latencies`: Callable returns 50-200ms random, cv_threshold=0.01, verify hits max_prompts
    - `test_fixed_mode`: WarmupConfig(convergence_detection=False, max_prompts=10), verify runs exactly 10 prompts
    - `test_exception_in_inference_continues`: Callable raises on 3rd call, verify warmup continues
    - `test_warmup_result_fields`: Verify all WarmupResult fields populated correctly

    **5. `test_core_environment.py`** — Environment metadata tests:
    - `test_imports`: Verify module imports
    - `test_collect_without_nvml`: Mock pynvml ImportError, verify returns degraded EnvironmentMetadata (not crash)
    - `test_environment_metadata_model`: Construct EnvironmentMetadata with all fields, verify
    - `test_summary_line_format`: Verify summary_line contains GPU name and CUDA version

    **6. `test_results_exporters_v3.py`** — Extended CSV export tests:
    - `test_aggregated_row_v3_fields`: Create mock AggregatedResult with energy_breakdown, thermal_throttle, environment; verify new columns in row dict
    - `test_aggregated_row_backwards_compat`: Create mock AggregatedResult WITHOUT v3 fields; verify no crash, None values for new columns
    - `test_column_ordering`: Verify energy_*, thermal_*, env_* columns grouped in output order
    - `test_csv_write_with_v3_fields`: Full CSV write to tmp_path, verify file content has new headers

    **7. `test_results_timeseries.py`** — Time-series export tests:
    - `test_export_and_load`: Create mock samples, export to tmp_path, load back, verify round-trip
    - `test_export_empty_samples`: Empty sample list, verify file created with sample_count=0
    - `test_summary_statistics`: Verify summary block has power_mean_w, memory_max_mb, etc.
    - `test_compact_keys`: Verify sample dict uses compact keys (t, power_w, mem_mb, temp_c, sm_pct, throttle)
    - `test_relative_timestamps`: Verify timestamps are relative (first sample at t=0)

    Follow existing patterns:
    - Use pytest fixtures for shared setup
    - Use `pytest.approx()` for float comparisons
    - Use `unittest.mock.patch` and `MagicMock` for pynvml mocking
    - Use `tmp_path` fixture for file operations
    - Group related tests in classes (e.g., `TestEnergyBreakdown`)
    - Run `ruff format` and `ruff check` on all test files
  </action>
  <verify>
    Run `python -m pytest tests/unit/test_domain_schema_v3.py tests/unit/test_core_power_thermal.py tests/unit/test_core_baseline.py tests/unit/test_core_warmup.py tests/unit/test_core_environment.py tests/unit/test_results_exporters_v3.py tests/unit/test_results_timeseries.py -v 2>&1 | tail -30` — ALL new tests must pass.
    Run `python -m pytest tests/unit/ -x -q 2>&1 | tail -5` — ALL unit tests (old + new) must pass.
    Run `ruff check tests/unit/test_core_power_thermal.py tests/unit/test_core_baseline.py tests/unit/test_core_warmup.py tests/unit/test_core_environment.py tests/unit/test_domain_schema_v3.py tests/unit/test_results_exporters_v3.py tests/unit/test_results_timeseries.py` — no lint errors.
  </verify>
  <done>
    7 new test files with comprehensive coverage of all Phase 1 modules. All tests pass without GPU. No regressions in existing tests.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Phase 1 measurement foundations: baseline power measurement, thermal throttling detection, environment metadata collection, time-series power/memory/utilisation sampling, warmup convergence detection, schema v3 with migration path, extended CSV export.

    Covers requirements: MEAS-01, MEAS-02, MEAS-03, MEAS-04, MEAS-05, MEAS-06, MEAS-07.
  </what-built>
  <how-to-verify>
    **UAT Round 1 (MEAS-09) — Quick validation steps:**

    1. Verify tests pass:
       ```bash
       cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool
       python -m pytest tests/unit/ -v --tb=short 2>&1 | tail -40
       ```

    2. Verify new modules import:
       ```bash
       python -c "
       from llenergymeasure.core.power_thermal import PowerThermalSampler
       from llenergymeasure.core.baseline import measure_baseline_power
       from llenergymeasure.core.warmup import warmup_until_converged
       from llenergymeasure.core.environment import collect_environment_metadata
       from llenergymeasure.results.timeseries import export_timeseries
       from llenergymeasure.domain.metrics import EnergyBreakdown, ThermalThrottleInfo, WarmupResult
       from llenergymeasure.domain.environment import EnvironmentMetadata
       print('All Phase 1 modules import successfully')
       "
       ```

    3. Verify config extensions:
       ```bash
       python -c "
       from llenergymeasure.config.models import ExperimentConfig
       c = ExperimentConfig(config_name='test', model_name='m')
       print(f'Warmup: cv_threshold={c.warmup.cv_threshold}, max_prompts={c.warmup.max_prompts}')
       print(f'Baseline: enabled={c.baseline.enabled}, duration={c.baseline.duration_sec}s')
       print(f'TimeSeries: save={c.timeseries.save}, interval={c.timeseries.sample_interval_ms}ms')
       "
       ```

    4. Verify schema version:
       ```bash
       python -c "from llenergymeasure.constants import SCHEMA_VERSION; print(f'Schema: v{SCHEMA_VERSION}')"
       ```

    5. (Optional, if GPU available) Run a quick experiment to see new features in action:
       ```bash
       lem experiment --preset quick-test --model gpt2 -d alpaca -n 5
       ```

    **Pain points to document:**
    - Any import errors or missing dependencies?
    - Any confusing error messages?
    - Does the CLI output look reasonable?
  </how-to-verify>
  <resume-signal>Type "approved" if tests pass and imports work, or describe any issues.</resume-signal>
</task>

</tasks>

<verification>
- All 7 new test files pass
- All existing unit tests pass (no regressions)
- All Phase 1 modules import cleanly
- Config extensions have correct defaults
- Schema version is 3.0.0
- (If GPU available) Quick experiment shows new features
</verification>

<success_criteria>
1. 30+ new unit tests covering all Phase 1 modules
2. Zero regressions in existing test suite
3. UAT validates: fresh imports, config defaults, schema version
4. All tests run without GPU
</success_criteria>

<output>
After completion, create `.planning/phases/01-measurement-foundations/01-06-SUMMARY.md`
</output>
