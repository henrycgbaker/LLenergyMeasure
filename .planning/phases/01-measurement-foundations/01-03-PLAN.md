---
phase: 01-measurement-foundations
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/llenergymeasure/core/warmup.py
autonomous: true

must_haves:
  truths:
    - "Warmup runs inference prompts until latency CV stabilises below configured threshold"
    - "Warmup has a configurable safety cap (max_prompts) to prevent runaway warmup"
    - "Progress bar shows current CV vs target CV during warmup"
    - "Non-convergence produces a warning flag (not an error) with final CV value"
    - "Warmup returns WarmupResult with convergence status and metrics"
  artifacts:
    - path: "src/llenergymeasure/core/warmup.py"
      provides: "warmup_until_converged() function with CV-based convergence detection"
      exports: ["warmup_until_converged"]
  key_links:
    - from: "src/llenergymeasure/core/warmup.py"
      to: "src/llenergymeasure/domain/metrics.py"
      via: "returns WarmupResult model"
      pattern: "WarmupResult"
    - from: "src/llenergymeasure/core/warmup.py"
      to: "src/llenergymeasure/config/models.py"
      via: "accepts WarmupConfig for parameters"
      pattern: "WarmupConfig"
---

<objective>
Implement CV-based warmup convergence detection that replaces fixed iteration warmup with adaptive convergence.

Purpose: MEAS-05 — warmup continues until latency CV stabilises, not a fixed count. More scientifically robust, adapts to hardware variance.
Output: `core/warmup.py` module with `warmup_until_converged()` function.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-CONTEXT.md
@.planning/phases/01-measurement-foundations/01-RESEARCH.md

Key source files:
@src/llenergymeasure/config/models.py  (WarmupConfig from Plan 01)
@src/llenergymeasure/domain/metrics.py  (WarmupResult from Plan 01)
@src/llenergymeasure/core/inference.py  (existing inference pattern)
@src/llenergymeasure/progress.py  (existing progress bar utilities)

Plan 01 summary (if available):
@.planning/phases/01-measurement-foundations/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Warmup convergence detection module</name>
  <files>
    src/llenergymeasure/core/warmup.py
  </files>
  <action>
    Create `core/warmup.py` implementing MEAS-05 (warmup convergence detection).

    1. Main function signature:
    ```python
    def warmup_until_converged(
        run_single_inference: Callable[[], float],
        config: WarmupConfig,
        *,
        show_progress: bool = True,
    ) -> WarmupResult:
    ```
    - `run_single_inference`: A callable that runs one warmup prompt and returns latency in milliseconds. This decouples warmup from specific inference implementation — the caller provides the inference function.
    - `config`: WarmupConfig from config/models.py (cv_threshold, max_prompts, window_size, min_prompts)
    - `show_progress`: Whether to display tqdm progress bar
    - Returns: WarmupResult from domain/metrics.py

    2. Algorithm:
    ```
    latencies = []
    converged = False
    final_cv = 1.0

    for i in range(config.max_prompts):
        latency_ms = run_single_inference()
        latencies.append(latency_ms)

        # Don't check convergence until minimum window reached
        if len(latencies) >= max(config.min_prompts, config.window_size):
            recent = latencies[-config.window_size:]
            mean = numpy.mean(recent)
            std = numpy.std(recent)
            cv = std / mean if mean > 0 else 0.0
            final_cv = float(cv)

            if cv < config.cv_threshold:
                converged = True
                break

        update progress bar (postfix: cv=f"{final_cv:.1%}", target=f"{config.cv_threshold:.1%}")
    ```

    3. Progress bar: Use `tqdm` (already a dependency). Show:
    - Total: `config.max_prompts`
    - Description: "Warmup"
    - Unit: "prompt"
    - Postfix: current CV value and target
    - If `show_progress=False`, skip tqdm entirely (for testing or quiet mode)

    4. Logging:
    - `logger.info(f"Warmup converged after {len(latencies)} prompts (CV={final_cv:.3f} < {config.cv_threshold})")` if converged
    - `logger.warning(f"Warmup did not converge after {config.max_prompts} prompts (final CV={final_cv:.3f}, target={config.cv_threshold})")` if not converged

    5. Return:
    ```python
    return WarmupResult(
        converged=converged,
        final_cv=final_cv,
        iterations_completed=len(latencies),
        target_cv=config.cv_threshold,
        max_prompts=config.max_prompts,
        latencies_ms=latencies,
    )
    ```

    6. Edge cases:
    - If `config.convergence_detection == False`: run exactly `config.max_prompts` iterations, return with `converged=True` (fixed mode)
    - If `config.enabled == False`: return immediately with `WarmupResult(converged=True, final_cv=0.0, iterations_completed=0, target_cv=config.cv_threshold, max_prompts=config.max_prompts)`
    - If `run_single_inference` raises an exception: catch, log warning, continue with remaining prompts (one failed prompt shouldn't abort warmup)

    7. Also add a convenience function for when you have model + tokenizer directly:
    ```python
    def create_warmup_inference_fn(
        model: Any,
        tokenizer: Any,
        prompt: str,
        max_new_tokens: int = 32,
    ) -> Callable[[], float]:
        """Create a single-inference callable for warmup.

        Returns a function that runs one inference and returns latency in ms.
        """
        def _run() -> float:
            import time
            import torch
            start = time.perf_counter()
            inputs = tokenizer(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            with torch.no_grad():
                model.generate(**inputs, max_new_tokens=max_new_tokens)
            return (time.perf_counter() - start) * 1000.0
        return _run
    ```

    Use `from __future__ import annotations` at top. Import numpy, tqdm, loguru. Import WarmupConfig and WarmupResult from their respective modules.
  </action>
  <verify>
    Run `python -c "from llenergymeasure.core.warmup import warmup_until_converged, create_warmup_inference_fn; print('warmup imports OK')"` — must succeed.
    Run `python -c "
from llenergymeasure.core.warmup import warmup_until_converged
from llenergymeasure.config.models import WarmupConfig

# Test with disabled warmup
config = WarmupConfig(enabled=False)
result = warmup_until_converged(lambda: 10.0, config, show_progress=False)
assert result.converged == True
assert result.iterations_completed == 0
print('Disabled warmup OK')
"` — must succeed.
    Run `python -c "
from llenergymeasure.core.warmup import warmup_until_converged
from llenergymeasure.config.models import WarmupConfig
import random

# Test convergence with stable latencies
config = WarmupConfig(cv_threshold=0.10, max_prompts=20, window_size=5, min_prompts=5)
# Stable latencies (low CV) should converge quickly
call_count = 0
def stable_inference():
    global call_count
    call_count += 1
    return 100.0 + random.uniform(-2, 2)  # ~2% variance

result = warmup_until_converged(stable_inference, config, show_progress=False)
assert result.converged == True
assert result.iterations_completed <= 20
assert result.final_cv < 0.10
print(f'Converged after {result.iterations_completed} prompts, CV={result.final_cv:.3f}')
"` — must converge.
    Run `python -c "
from llenergymeasure.core.warmup import warmup_until_converged
from llenergymeasure.config.models import WarmupConfig
import random

# Test non-convergence with very noisy latencies
config = WarmupConfig(cv_threshold=0.01, max_prompts=10, window_size=5, min_prompts=5)
def noisy_inference():
    return random.uniform(50, 200)  # Very noisy, won't converge at 1%

result = warmup_until_converged(noisy_inference, config, show_progress=False)
assert result.converged == False
assert result.iterations_completed == 10  # Hit cap
print(f'Non-convergence OK: iterations={result.iterations_completed}, CV={result.final_cv:.3f}')
"` — must hit cap without converging.
  </verify>
  <done>
    `warmup_until_converged()` function exists, takes a callable inference function and WarmupConfig, returns WarmupResult. CV-based convergence with safety cap. Progress bar shows CV vs target. Non-convergence produces warning, not error. `create_warmup_inference_fn()` convenience helper for PyTorch models.
  </done>
</task>

</tasks>

<verification>
- `warmup_until_converged` imports and runs without GPU
- Stable latencies converge before max_prompts cap
- Noisy latencies hit max_prompts cap with converged=False
- Disabled warmup returns immediately with converged=True
- Fixed mode (convergence_detection=False) runs all max_prompts
</verification>

<success_criteria>
1. CV-based convergence detection works with configurable threshold and window
2. Safety cap prevents runaway warmup
3. Progress bar shows current CV vs target (when enabled)
4. Non-convergence is a warning flag, not an error
5. Decoupled from specific inference implementation (callable-based)
</success_criteria>

<output>
After completion, create `.planning/phases/01-measurement-foundations/01-03-SUMMARY.md`
</output>
