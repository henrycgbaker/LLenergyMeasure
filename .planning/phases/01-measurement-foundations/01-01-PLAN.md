---
phase: 01-measurement-foundations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/domain/metrics.py
  - src/llenergymeasure/domain/experiment.py
  - src/llenergymeasure/domain/environment.py
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/constants.py
autonomous: true

must_haves:
  truths:
    - "Schema v3 models exist with nested energy breakdown fields (raw, adjusted, baseline)"
    - "Environment metadata model captures GPU, CUDA, driver, thermal, CPU, and container info"
    - "Configuration supports warmup convergence, baseline power, and time-series settings"
    - "Schema version bumped to 3.0.0"
    - "All new fields are additive (existing field access unchanged)"
  artifacts:
    - path: "src/llenergymeasure/domain/metrics.py"
      provides: "EnergyBreakdown model with raw/adjusted/baseline fields, ThermalThrottleInfo model"
      contains: "class EnergyBreakdown"
    - path: "src/llenergymeasure/domain/environment.py"
      provides: "EnvironmentMetadata Pydantic model with GPU, CUDA, thermal, CPU, container sub-models"
      exports: ["EnvironmentMetadata"]
    - path: "src/llenergymeasure/domain/experiment.py"
      provides: "RawProcessResult and AggregatedResult with environment, energy_breakdown, thermal fields"
      contains: "environment"
    - path: "src/llenergymeasure/config/models.py"
      provides: "WarmupConfig, BaselineConfig, TimeSeriesConfig sub-configurations"
      contains: "class WarmupConfig"
    - path: "src/llenergymeasure/constants.py"
      provides: "SCHEMA_VERSION bumped to 3.0.0"
      contains: "3.0.0"
  key_links:
    - from: "src/llenergymeasure/domain/experiment.py"
      to: "src/llenergymeasure/domain/metrics.py"
      via: "import EnergyBreakdown, ThermalThrottleInfo"
      pattern: "from llenergymeasure.domain.metrics import.*EnergyBreakdown"
    - from: "src/llenergymeasure/domain/experiment.py"
      to: "src/llenergymeasure/domain/environment.py"
      via: "import EnvironmentMetadata"
      pattern: "from llenergymeasure.domain.environment import EnvironmentMetadata"
    - from: "src/llenergymeasure/config/models.py"
      to: "WarmupConfig"
      via: "warmup field on ExperimentConfig"
      pattern: "warmup.*WarmupConfig"
---

<objective>
Create schema v3 domain models and configuration extensions for Phase 1 measurement foundations.

Purpose: All other Phase 1 plans depend on these models and config fields. This is the data layer foundation.
Output: Updated domain models (EnergyBreakdown, ThermalThrottleInfo, EnvironmentMetadata), config extensions (WarmupConfig, BaselineConfig, TimeSeriesConfig), schema version bump to 3.0.0.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-CONTEXT.md
@.planning/phases/01-measurement-foundations/01-RESEARCH.md

Key source files to read before implementing:
@src/llenergymeasure/domain/metrics.py
@src/llenergymeasure/domain/experiment.py
@src/llenergymeasure/config/models.py
@src/llenergymeasure/constants.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Schema v3 domain models (metrics + environment + experiment)</name>
  <files>
    src/llenergymeasure/domain/metrics.py
    src/llenergymeasure/domain/environment.py
    src/llenergymeasure/domain/experiment.py
    src/llenergymeasure/constants.py
  </files>
  <action>
    1. In `constants.py`: Bump `SCHEMA_VERSION` from `"2.0.0"` to `"3.0.0"`.

    2. In `domain/metrics.py`, add these new Pydantic models AFTER the existing `EnergyMetrics` class:

    `EnergyBreakdown(BaseModel)`:
    - `raw_j: float` — Total measured energy (Joules), required
    - `adjusted_j: float | None = None` — Baseline-adjusted energy (raw - baseline*duration)
    - `baseline_power_w: float | None = None` — Measured baseline idle power (Watts)
    - `baseline_method: str | None = None` — How baseline was obtained ("cached", "fresh", "unavailable")
    - `baseline_timestamp: datetime | None = None` — When baseline was measured
    - `baseline_cache_age_sec: float | None = None` — Age of cached baseline in seconds
    All fields with `Field(description=...)`.

    `ThermalThrottleInfo(BaseModel)`:
    - `detected: bool = False` — Whether any throttling occurred during experiment
    - `thermal: bool = False` — GPU thermal throttling
    - `power: bool = False` — Power brake throttling
    - `sw_thermal: bool = False` — Software thermal slowdown
    - `hw_thermal: bool = False` — Hardware thermal slowdown
    - `hw_power: bool = False` — Hardware power brake slowdown
    - `throttle_duration_sec: float = 0.0` — Estimated duration of throttling
    - `max_temperature_c: float | None = None` — Peak temperature during experiment
    - `throttle_timestamps: list[float] = Field(default_factory=list)` — Timestamps when throttle detected
    All fields with `Field(description=...)`.

    `WarmupResult(BaseModel)`:
    - `converged: bool` — Whether convergence was achieved
    - `final_cv: float` — Final coefficient of variation
    - `iterations_completed: int` — Number of warmup prompts run
    - `target_cv: float` — Configured CV threshold
    - `max_prompts: int` — Configured cap
    - `latencies_ms: list[float] = Field(default_factory=list)` — Warmup latencies for debugging
    All fields with `Field(description=...)`.

    3. Create new file `domain/environment.py` with these models:

    `GPUEnvironment(BaseModel)`:
    - `name: str` — GPU model name (e.g., "NVIDIA A100-SXM4-80GB")
    - `vram_total_mb: float` — Total VRAM in MB
    - `compute_capability: str | None = None` — e.g., "8.0"
    - `pcie_gen: int | None = None` — PCIe generation
    - `mig_enabled: bool = False`

    `CUDAEnvironment(BaseModel)`:
    - `version: str` — CUDA version (e.g., "12.4")
    - `driver_version: str` — Driver version string
    - `cudnn_version: str | None = None`

    `ThermalEnvironment(BaseModel)`:
    - `temperature_c: float | None = None` — GPU temp at experiment start
    - `power_limit_w: float | None = None` — Configured power limit
    - `default_power_limit_w: float | None = None` — Factory power limit
    - `fan_speed_pct: float | None = None`

    `CPUEnvironment(BaseModel)`:
    - `governor: str = "unknown"` — CPU frequency governor
    - `model: str | None = None` — CPU model string
    - `platform: str` — OS platform (Linux, etc.)

    `ContainerEnvironment(BaseModel)`:
    - `detected: bool = False` — Running in container
    - `runtime: str | None = None` — Docker, podman, etc.

    `EnvironmentMetadata(BaseModel)`:
    - `gpu: GPUEnvironment`
    - `cuda: CUDAEnvironment`
    - `thermal: ThermalEnvironment = Field(default_factory=ThermalEnvironment)`
    - `cpu: CPUEnvironment`
    - `container: ContainerEnvironment = Field(default_factory=ContainerEnvironment)`
    - `collected_at: datetime` — When metadata was collected
    - Add a `summary_line` property that returns a one-line string like `"A100 80GB | CUDA 12.4 | Driver 535.104 | 42C | container"`

    Use `from __future__ import annotations` at top. Follow existing codebase conventions: Pydantic BaseModel, Field with description, Google-style docstrings.

    4. In `domain/experiment.py`, add new optional fields to `RawProcessResult`:
    - `environment: EnvironmentMetadata | None = Field(default=None, description="...")` — Import from domain/environment.py
    - `energy_breakdown: EnergyBreakdown | None = Field(default=None, description="...")` — Import from domain/metrics.py
    - `thermal_throttle: ThermalThrottleInfo | None = Field(default=None, description="...")` — Import from domain/metrics.py
    - `warmup_result: WarmupResult | None = Field(default=None, description="...")` — Import from domain/metrics.py
    - `timeseries_path: str | None = Field(default=None, description="Path to time-series data file if saved")`

    Add same fields to `AggregatedResult` (minus warmup_result which is per-process only):
    - `environment: EnvironmentMetadata | None = Field(default=None, ...)` (from first process)
    - `energy_breakdown: EnergyBreakdown | None = Field(default=None, ...)`
    - `thermal_throttle: ThermalThrottleInfo | None = Field(default=None, ...)`
    - `timeseries_path: str | None = Field(default=None, ...)`

    IMPORTANT: All new fields must have `default=None` to maintain backwards compatibility with existing v2 results that lack these fields. Do NOT remove or rename any existing fields.
  </action>
  <verify>
    Run `python -c "from llenergymeasure.domain.metrics import EnergyBreakdown, ThermalThrottleInfo, WarmupResult; from llenergymeasure.domain.environment import EnvironmentMetadata; from llenergymeasure.domain.experiment import RawProcessResult, AggregatedResult; print('Schema v3 imports OK')"` — must succeed.
    Run `python -c "from llenergymeasure.constants import SCHEMA_VERSION; assert SCHEMA_VERSION == '3.0.0', f'Got {SCHEMA_VERSION}'"` — must succeed.
    Run `python -c "from llenergymeasure.domain.experiment import RawProcessResult; r = RawProcessResult(experiment_id='test', process_index=0, gpu_id=0, config_name='t', model_name='m', timestamps={'start': '2024-01-01T00:00:00', 'end': '2024-01-01T00:01:00', 'duration_sec': 60.0}, inference_metrics={'total_tokens': 100, 'input_tokens': 50, 'output_tokens': 50, 'inference_time_sec': 60.0, 'tokens_per_second': 1.67, 'latency_per_token_ms': 600.0}, energy_metrics={'total_energy_j': 100.0, 'duration_sec': 60.0}, compute_metrics={'flops_total': 1e12}); print('Backwards compat OK')"` — must succeed (no new required fields).
  </verify>
  <done>
    Schema v3 models exist: EnergyBreakdown, ThermalThrottleInfo, WarmupResult in metrics.py; EnvironmentMetadata (with sub-models) in environment.py. RawProcessResult and AggregatedResult have new optional fields. SCHEMA_VERSION is "3.0.0". All new fields have defaults, so existing v2 results still parse.
  </done>
</task>

<task type="auto">
  <name>Task 2: Configuration extensions (warmup, baseline, time-series)</name>
  <files>
    src/llenergymeasure/config/models.py
  </files>
  <action>
    Add three new sub-configuration Pydantic models to `config/models.py`, following the existing pattern of `TrafficSimulation`, `ScheduleConfig`, etc.

    1. `WarmupConfig(BaseModel)`:
    - `enabled: bool = Field(default=True, description="Enable warmup phase before inference")`
    - `convergence_detection: bool = Field(default=True, description="Use CV-based convergence detection (false = fixed iterations)")`
    - `cv_threshold: float = Field(default=0.05, gt=0.0, le=1.0, description="Target CV threshold (default 5%)")`
    - `max_prompts: int = Field(default=50, ge=1, le=200, description="Maximum warmup iterations (safety cap)")`
    - `window_size: int = Field(default=5, ge=3, le=20, description="Rolling window size for CV calculation")`
    - `min_prompts: int = Field(default=5, ge=1, description="Minimum warmup prompts before checking convergence")`

    2. `BaselineConfig(BaseModel)`:
    - `enabled: bool = Field(default=True, description="Enable baseline power measurement")`
    - `required: bool = Field(default=False, description="Fail experiment if baseline measurement fails (false = warn and continue)")`
    - `duration_sec: float = Field(default=30.0, ge=5.0, le=120.0, description="Baseline measurement duration in seconds")`
    - `cache_ttl_sec: float = Field(default=3600.0, ge=60.0, description="Cache validity in seconds (default 1 hour)")`
    - `sample_interval_ms: int = Field(default=100, ge=50, le=1000, description="Sampling interval in milliseconds")`

    3. `TimeSeriesConfig(BaseModel)`:
    - `enabled: bool = Field(default=False, description="Enable time-series data collection")`
    - `save: bool = Field(default=False, description="Save time-series to separate file (--save-timeseries)")`
    - `sample_interval_ms: int = Field(default=100, ge=50, le=5000, description="Sampling interval in ms (100ms = 10Hz, 1000ms = 1Hz)")`

    4. Add these as fields on `ExperimentConfig`:
    - `warmup: WarmupConfig = Field(default_factory=WarmupConfig, description="Warmup convergence configuration")`
    - `baseline: BaselineConfig = Field(default_factory=BaselineConfig, description="Baseline power measurement configuration")`
    - `timeseries: TimeSeriesConfig = Field(default_factory=TimeSeriesConfig, description="Time-series data collection configuration")`

    Place new models BEFORE the ExperimentConfig class definition, after the existing sub-configuration classes (TrafficSimulation, ScheduleConfig, IOConfig, DecoderConfig).

    Add a `model_validator` to WarmupConfig:
    ```python
    @model_validator(mode="after")
    def validate_window_size(self) -> "WarmupConfig":
        if self.window_size > self.max_prompts:
            raise ValueError(f"window_size ({self.window_size}) must be <= max_prompts ({self.max_prompts})")
        if self.min_prompts > self.max_prompts:
            raise ValueError(f"min_prompts ({self.min_prompts}) must be <= max_prompts ({self.max_prompts})")
        return self
    ```
  </action>
  <verify>
    Run `python -c "from llenergymeasure.config.models import ExperimentConfig, WarmupConfig, BaselineConfig, TimeSeriesConfig; c = ExperimentConfig(config_name='test', model_name='m'); print(f'warmup.cv_threshold={c.warmup.cv_threshold}, baseline.enabled={c.baseline.enabled}, timeseries.save={c.timeseries.save}')"` — must print `warmup.cv_threshold=0.05, baseline.enabled=True, timeseries.save=False`.
    Run `python -c "from llenergymeasure.config.models import WarmupConfig; from pydantic import ValidationError; try: WarmupConfig(window_size=100, max_prompts=50); assert False, 'Should have raised'; except ValidationError: print('Validation OK')"` — must print "Validation OK".
    Run existing unit tests: `cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_config_models.py -x -q 2>&1 | tail -5` — must pass (no regressions).
  </verify>
  <done>
    WarmupConfig, BaselineConfig, and TimeSeriesConfig exist as sub-configurations on ExperimentConfig with sensible defaults. Existing configs parse without errors (all new fields have defaults). Validation rules enforced.
  </done>
</task>

</tasks>

<verification>
- `python -c "from llenergymeasure.constants import SCHEMA_VERSION; assert SCHEMA_VERSION == '3.0.0'"` passes
- `python -c "from llenergymeasure.domain.environment import EnvironmentMetadata"` passes
- `python -c "from llenergymeasure.domain.metrics import EnergyBreakdown, ThermalThrottleInfo, WarmupResult"` passes
- `python -c "from llenergymeasure.config.models import WarmupConfig, BaselineConfig, TimeSeriesConfig"` passes
- `python -m pytest tests/unit/ -x -q` passes (no regressions)
- Existing RawProcessResult construction works unchanged (backwards compatible)
</verification>

<success_criteria>
1. Schema v3 domain models importable and functional
2. Configuration extensions on ExperimentConfig with defaults
3. All existing tests pass (no breaking changes)
4. SCHEMA_VERSION = "3.0.0"
</success_criteria>

<output>
After completion, create `.planning/phases/01-measurement-foundations/01-01-SUMMARY.md`
</output>
