---
phase: 08-testing-and-integration
plan: 02
type: execute
wave: 2
depends_on: [08-01]
files_modified:
  - tests/unit/test_state_machine.py
  - tests/unit/test_exceptions.py
  - tests/unit/test_protocols.py
  - tests/unit/test_config_schema.py
  - tests/unit/test_config_loader.py
  - tests/unit/test_config_introspection.py
  - tests/unit/test_config_user_config.py
autonomous: true
requirements: [STU-05, INF-10, INF-11]

must_haves:
  truths:
    - "State machine tests cover all 3 phases (INITIALISING, MEASURING, DONE), valid transitions, and invalid transition rejection"
    - "Exception hierarchy tests confirm LLEMError -> 5 subclasses + InvalidStateTransitionError under ExperimentError"
    - "Protocol conformance tests verify that FakeInferenceBackend, FakeEnergyBackend, FakeResultsRepository satisfy isinstance() checks"
    - "Config schema tests validate ExperimentConfig field renames, extra=forbid, cross-validators, backend section composition"
    - "Config loader tests cover YAML loading, ConfigError on unknown keys, Pydantic ValidationError passthrough, did-you-mean suggestions"
    - "Config introspection tests use SSOT to generate test values (INF-11) and verify schema export"
    - "User config tests cover XDG path, env var overrides, missing file graceful default"
  artifacts:
    - path: "tests/unit/test_state_machine.py"
      provides: "3-state machine coverage"
      contains: "ExperimentPhase"
    - path: "tests/unit/test_exceptions.py"
      provides: "Exception hierarchy coverage"
      contains: "LLEMError"
    - path: "tests/unit/test_protocols.py"
      provides: "Protocol conformance tests"
      contains: "isinstance"
    - path: "tests/unit/test_config_schema.py"
      provides: "ExperimentConfig validation tests"
      contains: "ExperimentConfig"
    - path: "tests/unit/test_config_loader.py"
      provides: "YAML loader tests"
      contains: "load_experiment_config"
    - path: "tests/unit/test_config_introspection.py"
      provides: "SSOT introspection tests"
      contains: "get_param_test_values"
    - path: "tests/unit/test_config_user_config.py"
      provides: "User config tests"
      contains: "load_user_config"
  key_links:
    - from: "tests/unit/test_protocols.py"
      to: "tests/fakes.py"
      via: "protocol isinstance check"
      pattern: "isinstance.*FakeInferenceBackend"
    - from: "tests/unit/test_config_introspection.py"
      to: "src/llenergymeasure/config/introspection.py"
      via: "SSOT-driven test value generation"
      pattern: "get_param_test_values"
---

<objective>
Write the missing subsystem unit tests for infrastructure (state machine, exceptions, protocols) and config (schema, loader, introspection, user config).

Purpose: Cover the remaining subsystems that don't have v2.0 tests yet. These tests use protocol injection fakes from `tests/fakes.py` (INF-10) and config introspection for SSOT-driven test generation (INF-11).
Output: 7 new test files covering all untested v2.0 subsystems.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-testing-and-integration/08-CONTEXT.md
@.planning/phases/08-testing-and-integration/08-RESEARCH.md
@.planning/phases/08-testing-and-integration/08-01-SUMMARY.md

<interfaces>
<!-- State machine API (src/llenergymeasure/core/state.py) -->
```python
class ExperimentPhase(str, Enum):
    INITIALISING = "initialising"
    MEASURING = "measuring"
    DONE = "done"

class ExperimentState(BaseModel):
    phase: ExperimentPhase = ExperimentPhase.INITIALISING
    failed: bool = False
    experiment_id: str
    config_hash: str
    started_at: datetime
    # ...

class StateManager:
    def __init__(self, state_dir: Path | None = None): ...
    def create(self, experiment_id: str, config_hash: str) -> ExperimentState: ...
    def transition(self, state: ExperimentState, to_phase: ExperimentPhase) -> ExperimentState: ...
    def mark_failed(self, state: ExperimentState) -> ExperimentState: ...
    def save(self, state: ExperimentState) -> Path: ...
    def load(self, experiment_id: str) -> ExperimentState: ...

def compute_config_hash(config_dict: dict) -> str: ...
```

<!-- Exceptions API (src/llenergymeasure/exceptions.py) -->
```python
class LLEMError(Exception): ...
class ConfigError(LLEMError): ...
class BackendError(LLEMError): ...
class PreFlightError(LLEMError): ...
class ExperimentError(LLEMError): ...
class StudyError(LLEMError): ...
class InvalidStateTransitionError(ExperimentError): ...
```

<!-- Protocols API (src/llenergymeasure/protocols.py) -->
```python
@runtime_checkable
class ModelLoader(Protocol): ...
class InferenceEngine(Protocol): ...
class MetricsCollector(Protocol): ...
class EnergyBackend(Protocol): ...
class ResultsRepository(Protocol): ...
```

<!-- Config loader API (src/llenergymeasure/config/loader.py) -->
```python
def load_experiment_config(source: str | Path, cli_overrides: dict | None = None) -> ExperimentConfig: ...
def deep_merge(base: dict, overlay: dict) -> dict: ...
```

<!-- Config introspection API (src/llenergymeasure/config/introspection.py) -->
```python
def get_backend_params(backend: str) -> dict[str, dict[str, Any]]: ...
def get_shared_params() -> dict[str, dict[str, Any]]: ...
def get_experiment_config_schema() -> dict[str, Any]: ...
def get_param_test_values(param_path: str) -> list[Any]: ...
def get_all_params() -> dict[str, dict[str, dict[str, Any]]]: ...
def list_all_param_paths(backend: str | None = None) -> list[str]: ...
def get_validation_rules() -> list[dict[str, str]]: ...
```

<!-- User config API (src/llenergymeasure/config/user_config.py) -->
```python
class UserConfig(BaseModel): ...
def get_user_config_path() -> Path: ...
def load_user_config(config_path: Path | None = None) -> UserConfig: ...
```

<!-- Fakes from tests/fakes.py (created in Plan 01) -->
```python
class FakeInferenceBackend: ...
class FakeEnergyBackend: ...
class FakeResultsRepository: ...
```

<!-- Factories from tests/conftest.py (created in Plan 01) -->
```python
def make_config(**overrides) -> ExperimentConfig: ...
def make_result(**overrides) -> ExperimentResult: ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write infrastructure unit tests (state machine, exceptions, protocols)</name>
  <files>
    tests/unit/test_state_machine.py
    tests/unit/test_exceptions.py
    tests/unit/test_protocols.py
  </files>
  <action>
    **Create `tests/unit/test_state_machine.py`** — tests for the 3-state machine in `core/state.py`:

    Tests to write:
    - `test_initial_phase_is_initialising`: new ExperimentState has phase=INITIALISING
    - `test_transition_initialising_to_measuring`: valid transition succeeds
    - `test_transition_measuring_to_done`: valid transition succeeds
    - `test_invalid_transition_initialising_to_done`: raises InvalidStateTransitionError (can't skip MEASURING)
    - `test_invalid_transition_done_to_measuring`: raises InvalidStateTransitionError (can't go backwards)
    - `test_mark_failed_sets_flag`: mark_failed(state) sets failed=True
    - `test_mark_failed_preserves_phase`: marking failed does not change the current phase
    - `test_state_manager_create_and_load_roundtrip`: create state, save, load — fields match
    - `test_compute_config_hash_deterministic`: same dict produces same hash
    - `test_compute_config_hash_different_for_different_configs`: different dicts produce different hashes

    Import from `llenergymeasure.core.state` — use `StateManager`, `ExperimentPhase`, `ExperimentState`, `compute_config_hash`.
    Use `tmp_path` fixture for StateManager's state_dir.

    **Create `tests/unit/test_exceptions.py`** — tests for the exception hierarchy in `exceptions.py`:

    Tests to write:
    - `test_llem_error_is_base`: LLEMError is subclass of Exception
    - `test_config_error_inherits_llem_error`: ConfigError is subclass of LLEMError
    - `test_backend_error_inherits_llem_error`: BackendError is subclass of LLEMError
    - `test_preflight_error_inherits_llem_error`: PreFlightError is subclass of LLEMError
    - `test_experiment_error_inherits_llem_error`: ExperimentError is subclass of LLEMError
    - `test_study_error_inherits_llem_error`: StudyError is subclass of LLEMError
    - `test_invalid_state_transition_inherits_experiment_error`: InvalidStateTransitionError is subclass of ExperimentError (not directly LLEMError)
    - `test_all_errors_catchable_via_llem_error`: catching LLEMError catches all 5 subclass instances
    - `test_error_messages_preserved`: constructing with a message preserves str(e)

    Import from `llenergymeasure.exceptions`.

    **Create `tests/unit/test_protocols.py`** — protocol conformance tests using fakes from `tests/fakes.py`:

    Tests to write (INF-10 requirement — protocol injection, not mock.patch):
    - `test_fake_inference_backend_satisfies_protocol`: `isinstance(FakeInferenceBackend(result=make_result()), InferenceBackend)` returns True
    - `test_fake_energy_backend_satisfies_protocol`: `isinstance(FakeEnergyBackend(), EnergyBackend)` returns True
    - `test_fake_results_repository_satisfies_protocol`: `isinstance(FakeResultsRepository(), ResultsRepository)` returns True
    - `test_fake_inference_backend_returns_result`: calling `fake.run(config)` returns the injected ExperimentResult
    - `test_fake_inference_backend_records_calls`: calling `fake.run(config)` records the config in `fake.run_calls`
    - `test_fake_energy_backend_lifecycle`: start_tracking() then stop_tracking() returns EnergyMeasurement
    - `test_fake_results_repository_save_load_roundtrip`: save then load returns the same result

    Import protocols from `llenergymeasure.protocols` and `llenergymeasure.core.backends.protocol`.
    Import fakes from `tests.fakes`.
    Import factories from `tests.conftest` — use `make_config()` and `make_result()`.
  </action>
  <verify>
    <automated>pytest tests/unit/test_state_machine.py tests/unit/test_exceptions.py tests/unit/test_protocols.py -v --tb=short -q 2>&1 | tail -10</automated>
  </verify>
  <done>3 new test files created. State machine tests cover all 3 phases and transition rules. Exception hierarchy tests confirm the flat hierarchy with InvalidStateTransitionError under ExperimentError. Protocol conformance tests use fakes (INF-10) — no unittest.mock.patch on internal modules. All tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Write config subsystem unit tests (schema, loader, introspection, user config)</name>
  <files>
    tests/unit/test_config_schema.py
    tests/unit/test_config_loader.py
    tests/unit/test_config_introspection.py
    tests/unit/test_config_user_config.py
  </files>
  <action>
    **Create `tests/unit/test_config_schema.py`** — ExperimentConfig Pydantic validation tests:

    Tests to write:
    - `test_minimal_valid_config`: `ExperimentConfig(model="gpt2", backend="pytorch")` succeeds
    - `test_extra_fields_forbidden`: `ExperimentConfig(model="gpt2", backend="pytorch", unknown_field="x")` raises ValidationError (extra="forbid")
    - `test_field_renames_accepted`: v2.0 field names work — `model` (not `model_name`), `precision` (not `fp_precision`), `n` (not `num_input_prompts`)
    - `test_invalid_backend_raises_validation_error`: `ExperimentConfig(model="gpt2", backend="invalid_backend")` raises ValidationError
    - `test_default_backend_is_pytorch`: `ExperimentConfig(model="gpt2")` has backend="pytorch"
    - `test_pytorch_config_section_composition`: config with `pytorch={...}` backend section accepted
    - `test_precision_validation`: valid precisions (float16, bfloat16, float32) accepted; invalid rejected
    - `test_cross_validator_precision_backend_mismatch`: if applicable, test that precision/backend cross-validation works (check the actual validators in models.py)
    - Schema-driven: use `from llenergymeasure.config.ssot import PRECISION_SUPPORT` to parametrise precision tests across all valid values for pytorch

    Use `make_config()` from `tests.conftest` for convenience. Use `pytest.raises(ValidationError)` for rejection tests.

    **Create `tests/unit/test_config_loader.py`** — YAML loading tests:

    Tests to write:
    - `test_load_valid_yaml`: write a minimal YAML to tmp_path, `load_experiment_config(path)` returns ExperimentConfig
    - `test_load_unknown_yaml_key_raises_config_error`: YAML with unknown key raises ConfigError (not ValidationError)
    - `test_pydantic_validation_error_passes_through`: YAML with valid keys but invalid values raises ValidationError (not ConfigError)
    - `test_load_nonexistent_file_raises_config_error`: loading a path that doesn't exist raises ConfigError
    - `test_cli_overrides_merged`: `load_experiment_config(path, cli_overrides={"model": "bert"})` merges overrides
    - `test_deep_merge_overlay_wins`: `deep_merge({"a": 1}, {"a": 2})` returns `{"a": 2}`
    - `test_deep_merge_nested`: `deep_merge({"a": {"b": 1}}, {"a": {"c": 2}})` returns `{"a": {"b": 1, "c": 2}}`

    Import `load_experiment_config`, `deep_merge` from `llenergymeasure.config.loader`.
    Import `ConfigError` from `llenergymeasure.exceptions`.
    Write YAML files to `tmp_path` using `Path.write_text()`.

    **Create `tests/unit/test_config_introspection.py`** — SSOT introspection tests (INF-11):

    Tests to write (schema-driven test generation):
    - `test_get_backend_params_returns_pytorch_params`: `get_backend_params("pytorch")` returns a dict with known fields (e.g., "batch_size")
    - `test_get_shared_params_returns_model_field`: `get_shared_params()` returns dict containing "model" key
    - `test_get_experiment_config_schema_is_valid_json_schema`: `get_experiment_config_schema()` returns a dict with "properties" key
    - `test_get_param_test_values_returns_list`: `get_param_test_values("pytorch.batch_size")` returns a non-empty list
    - `test_get_all_params_covers_all_backends`: `get_all_params()` returns dict with "pytorch" key (and others if present)
    - `test_list_all_param_paths_non_empty`: `list_all_param_paths()` returns a non-empty list of dotted paths
    - `test_all_precision_values_valid_config`: for each precision in `PRECISION_SUPPORT["pytorch"]`, `make_config(precision=precision)` succeeds (schema-driven per INF-11)
    - `test_get_validation_rules_returns_list`: `get_validation_rules()` returns a list of dicts

    Import from `llenergymeasure.config.introspection`.

    **Create `tests/unit/test_config_user_config.py`** — user config tests:

    Tests to write:
    - `test_get_user_config_path_returns_path`: `get_user_config_path()` returns a Path object
    - `test_load_user_config_missing_file_returns_defaults`: `load_user_config(config_path=tmp_path / "nonexistent.yaml")` returns a UserConfig with all defaults (no error)
    - `test_load_user_config_valid_file`: write a minimal user config YAML, `load_user_config(path)` returns UserConfig with overridden values
    - `test_env_var_overrides`: set env vars (e.g., LLEM_OUTPUT_DIR), load config, verify the env var takes effect. Use `monkeypatch.setenv()`.
    - `test_silent_ignore_invalid_float_env_vars`: set `LLEM_CARBON_INTENSITY` to "not_a_number", load config — no error, value treated as not set (per Phase 2 decision)

    Import from `llenergymeasure.config.user_config`.
    Use `tmp_path` and `monkeypatch` fixtures.
  </action>
  <verify>
    <automated>pytest tests/unit/test_config_schema.py tests/unit/test_config_loader.py tests/unit/test_config_introspection.py tests/unit/test_config_user_config.py -v --tb=short -q 2>&1 | tail -10</automated>
  </verify>
  <done>4 new test files created. Config schema tests validate composition model, extra=forbid, field renames, and cross-validators. Config loader tests cover YAML loading, ConfigError vs ValidationError boundary, and CLI override merging. Introspection tests use SSOT for schema-driven test generation (INF-11). User config tests cover XDG path, missing file defaults, and env var overrides. All tests pass.</done>
</task>

</tasks>

<verification>
1. `pytest tests/unit/test_state_machine.py tests/unit/test_exceptions.py tests/unit/test_protocols.py -v` — all infra tests pass
2. `pytest tests/unit/test_config_schema.py tests/unit/test_config_loader.py tests/unit/test_config_introspection.py tests/unit/test_config_user_config.py -v` — all config tests pass
3. `pytest tests/unit/ -v --tb=short` — entire unit suite passes (old + new tests)
4. `grep -r "unittest.mock.patch" tests/unit/test_protocols.py` — returns nothing (INF-10 compliance)
5. `grep "get_param_test_values\|PRECISION_SUPPORT" tests/unit/test_config_introspection.py tests/unit/test_config_schema.py` — SSOT used (INF-11)
</verification>

<success_criteria>
- 7 new test files exist and all pass
- Total unit test count increases by 30+ tests
- Protocol tests use fakes only (no mock.patch on internal modules)
- Introspection tests demonstrate SSOT-driven test value generation
- `pytest tests/unit/ -q` exits 0 with all tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/08-testing-and-integration/08-02-SUMMARY.md`
</output>
