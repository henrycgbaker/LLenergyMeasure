---
phase: 12-integration
plan: 03
type: execute
wave: 3
depends_on:
  - 12-02
files_modified:
  - src/llenergymeasure/cli/run.py
  - src/llenergymeasure/cli/_display.py
  - src/llenergymeasure/cli/__init__.py
  - tests/unit/test_cli_run.py
autonomous: true
requirements:
  - CLI-05
  - CLI-11

must_haves:
  truths:
    - "llem run study.yaml detects study mode (YAML has sweep: or experiments: keys) and routes to run_study()"
    - "--cycles 5 --order interleaved --no-gaps override execution block values from YAML"
    - "CLI effective defaults apply: n_cycles=3, cycle_order=shuffled when neither YAML nor CLI specifies"
    - "Terminal shows per-experiment progress line with model/backend/precision and elapsed time"
    - "Thermal gap countdown is visible during inter-experiment pauses (CLI-11)"
    - "--quiet suppresses CLI-side progress lines and summary; gap countdown suppression is a documented M2 limitation (subprocess-level)"
    - "Study summary table displays after completion with columns: Config, Cycle, Time, Energy, tok/s"
  artifacts:
    - path: "src/llenergymeasure/cli/run.py"
      provides: "Study detection, study flags, study execution path"
      contains: "--cycles"
    - path: "src/llenergymeasure/cli/_display.py"
      provides: "Study progress display and summary table"
      contains: "print_study_summary"
    - path: "tests/unit/test_cli_run.py"
      provides: "CLI study-mode tests"
  key_links:
    - from: "src/llenergymeasure/cli/run.py"
      to: "llenergymeasure._api.run_study"
      via: "study detection routes to run_study()"
      pattern: "run_study"
    - from: "src/llenergymeasure/cli/run.py"
      to: "llenergymeasure.config.loader.load_study_config"
      via: "study YAML detection checks for sweep/experiments keys"
      pattern: "load_study_config"
---

<objective>
Add study-mode detection and flags to `llem run`, implement study progress display and summary table, and integrate thermal gap countdown visibility.

Purpose: This is the user-facing integration — researchers can now run `llem run study.yaml` with `--cycles`, `--order`, and `--no-gaps` flags, see per-experiment progress during execution, and get a study summary table at the end.

Output: Study-aware CLI with flags, progress display, summary table, unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/12-integration/12-CONTEXT.md
@.planning/phases/12-integration/12-01-SUMMARY.md
@.planning/phases/12-integration/12-02-SUMMARY.md

<interfaces>
<!-- Key types from Plans 01 and 02. -->

From src/llenergymeasure/_api.py (after Plan 02):
```python
def run_study(config: str | Path | StudyConfig) -> StudyResult: ...
```

From src/llenergymeasure/config/loader.py:
```python
def load_study_config(path: Path | str, cli_overrides: dict | None = None) -> StudyConfig: ...
```

From src/llenergymeasure/domain/experiment.py (after Plan 01):
```python
class StudySummary(BaseModel):
    total_experiments: int
    completed: int = 0
    failed: int = 0
    total_wall_time_s: float = 0.0
    total_energy_j: float = 0.0
    warnings: list[str] = Field(default_factory=list)

class StudyResult(BaseModel):
    experiments: list[ExperimentResult]
    name: str | None = None
    study_design_hash: str | None = None
    measurement_protocol: dict[str, Any] = Field(default_factory=dict)
    result_files: list[str] = Field(default_factory=list)
    summary: StudySummary | None = None
```

From src/llenergymeasure/cli/_display.py (existing):
```python
def print_result_summary(result: ExperimentResult) -> None: ...
def print_experiment_header(config: ExperimentConfig) -> None: ...
def _format_duration(seconds: float) -> str: ...
def _sig3(value: float) -> str: ...
```

From src/llenergymeasure/study/grid.py:
```python
def format_preflight_summary(study_config: StudyConfig, skipped: list | None = None) -> str: ...
```

From src/llenergymeasure/cli/run.py (current):
```python
def run(config, model, backend, dataset, n, batch_size, precision, output, dry_run, quiet, verbose) -> None: ...
def _run_impl(...) -> None: ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add study detection and study flags to cli/run.py</name>
  <files>src/llenergymeasure/cli/run.py</files>
  <action>
  Modify `cli/run.py` to detect study YAML and add `--cycles`, `--order`, `--no-gaps` flags.

  **1. Add new CLI flags to the `run()` function signature:**

  After the existing `verbose` parameter, add:
  ```python
  cycles: Annotated[
      int | None,
      typer.Option("--cycles", help="Number of cycles (study mode)"),
  ] = None,
  order: Annotated[
      str | None,
      typer.Option("--order", help="Cycle ordering: sequential, interleaved, shuffled (study mode)"),
  ] = None,
  no_gaps: Annotated[
      bool,
      typer.Option("--no-gaps", help="Disable thermal gaps between experiments (study mode)"),
  ] = False,
  ```

  Pass these through to `_run_impl()`:
  ```python
  _run_impl(
      config=config, model=model, backend=backend, dataset=dataset,
      n=n, batch_size=batch_size, precision=precision, output=output,
      dry_run=dry_run, quiet=quiet, verbose=verbose,
      cycles=cycles, order=order, no_gaps=no_gaps,
  )
  ```

  **2. Add study detection to `_run_impl()`:**

  Add `cycles`, `order`, `no_gaps` parameters to `_run_impl()` signature.

  At the beginning of `_run_impl()`, after building `cli_overrides` and after the config/model validation check, add study detection:

  ```python
  # Study detection: YAML with sweep: or experiments: keys is a study
  is_study = False
  if config is not None:
      import yaml
      try:
          raw = yaml.safe_load(config.read_text())
          if isinstance(raw, dict) and ("sweep" in raw or "experiments" in raw):
              is_study = True
      except Exception:
          pass  # Fall through to normal experiment path — loader will raise if invalid
  ```

  **3. Study execution path:**

  After the study detection block and before the existing experiment path:

  ```python
  if is_study:
      _run_study_impl(
          config=config,
          cli_overrides=cli_overrides,
          cycles=cycles,
          order=order,
          no_gaps=no_gaps,
          quiet=quiet,
          verbose=verbose,
      )
      return
  ```

  **4. Create `_run_study_impl()` function:**

  ```python
  def _run_study_impl(
      config: Path,
      cli_overrides: dict[str, Any],
      cycles: int | None,
      order: str | None,
      no_gaps: bool,
      quiet: bool,
      verbose: bool,
  ) -> None:
      """Study execution path — separated for clean error handling."""
      from llenergymeasure import run_study
      from llenergymeasure.config.loader import load_study_config
      from llenergymeasure.cli._display import print_study_summary
      from llenergymeasure.study.grid import format_preflight_summary

      # Build execution overrides from CLI flags
      exec_overrides: dict[str, Any] = {}

      # CLI effective defaults: n_cycles=3, cycle_order="shuffled" when neither YAML nor CLI specifies
      # These are applied at CLI layer, not Pydantic layer (Pydantic defaults are conservative)
      # Load YAML first to check what's specified
      import yaml
      raw = yaml.safe_load(config.read_text()) or {}
      yaml_execution = raw.get("execution", {}) or {}

      if cycles is not None:
          exec_overrides["n_cycles"] = cycles
      elif "n_cycles" not in yaml_execution:
          exec_overrides["n_cycles"] = 3  # CLI effective default

      if order is not None:
          exec_overrides["cycle_order"] = order
      elif "cycle_order" not in yaml_execution:
          exec_overrides["cycle_order"] = "shuffled"  # CLI effective default

      if no_gaps:
          exec_overrides["experiment_gap_seconds"] = 0
          exec_overrides["cycle_gap_seconds"] = 0

      # Build full CLI overrides dict
      study_cli_overrides: dict[str, Any] = {}
      if cli_overrides:
          study_cli_overrides.update(cli_overrides)
      if exec_overrides:
          study_cli_overrides["execution"] = exec_overrides

      # Load study config with overrides
      study_config = load_study_config(
          path=config,
          cli_overrides=study_cli_overrides if study_cli_overrides else None,
      )

      # Pre-flight summary display
      if not quiet:
          summary = format_preflight_summary(study_config)
          print(summary, file=sys.stderr)
          print(file=sys.stderr)

      # Run the study
      result = run_study(study_config)

      # Display summary
      if not quiet:
          print_study_summary(result)

      # Show output path
      if result.result_files:
          # Derive study directory from first result file path
          from pathlib import PurePosixPath
          first = result.result_files[0]
          print(f"Study results: {first}", file=sys.stderr)
  ```

  **5. Add `StudyError` to the error handling in `run()`:**

  In the `try/except` block in `run()`, add:
  ```python
  from llenergymeasure.exceptions import StudyError
  ```
  And handle it alongside other errors:
  ```python
  except (PreFlightError, ExperimentError, BackendError, StudyError) as e:
      print(format_error(e, verbose=verbose), file=sys.stderr)
      raise typer.Exit(code=1) from None
  ```

  **6. Warning when CLI flags narrow a sweep:**

  In `_run_study_impl`, after loading the study config, check if CLI overrides narrowed the experiment count:
  ```python
  # Warning when CLI flags narrow a sweep (CONTEXT.md decision)
  if cli_overrides and not quiet:
      # Compare with un-overridden load
      try:
          unfiltered = load_study_config(path=config)
          if len(study_config.experiments) < len(unfiltered.experiments):
              narrowed_from = len(unfiltered.experiments)
              narrowed_to = len(study_config.experiments)
              print(
                  f"Warning: CLI flags narrowed sweep from {narrowed_from} to {narrowed_to} experiments",
                  file=sys.stderr,
              )
      except Exception:
          pass  # Best-effort warning
  ```

  Actually, this comparison won't work well because `load_study_config` applies cycles which changes the experiment count. And the CLI overrides affect execution, not experiment filtering. The narrowing happens when `--model` or `--backend` filter the sweep grid. For M2, this narrowing logic is complex (needs changes to `load_study_config` to accept experiment-level CLI overrides). This is discretionary per CONTEXT.md — implement a simple version or defer to Phase 13 docs.

  **Simpler approach:** Skip the narrowing warning for now. The CONTEXT.md says "Document this behaviour in user-facing docs (Phase 13)". The warning propagation into `StudyResult.summary.warnings` is handled in `_run()` (Plan 02). CLI-level narrowing comparison is non-trivial and not required by any requirement ID.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.cli.run import run, _run_study_impl
import inspect
sig = inspect.signature(run)
params = list(sig.parameters.keys())
assert 'cycles' in params, f'Missing --cycles flag: {params}'
assert 'order' in params, f'Missing --order flag: {params}'
assert 'no_gaps' in params, f'Missing --no-gaps flag: {params}'
print('CLI flags present:', [p for p in params if p in ('cycles', 'order', 'no_gaps')])
# Verify _run_study_impl exists and routes to run_study
src = inspect.getsource(_run_study_impl)
assert 'run_study' in src, '_run_study_impl does not call run_study()'
print('_run_study_impl routes to run_study: OK')
"</automated>
  </verify>
  <done>
    - llem run study.yaml detects study mode (sweep: or experiments: keys in YAML)
    - --cycles, --order, --no-gaps flags added to llem run
    - CLI effective defaults: n_cycles=3, cycle_order="shuffled" when unspecified
    - _run_study_impl() exists and calls run_study() (study routing verified)
    - Pre-flight summary displayed before execution (unless --quiet)
    - StudyError handled with exit code 1
  </done>
</task>

<task type="auto">
  <name>Task 2: Study progress display and summary table in _display.py, plus CLI tests</name>
  <files>
    src/llenergymeasure/cli/_display.py
    tests/unit/test_cli_run.py
  </files>
  <action>
  Add study-specific display functions to `_display.py` and tests to `test_cli_run.py`.

  **_display.py — Add study display functions:**

  Add these functions after the existing `print_experiment_header()`:

  ```python
  def print_study_progress(
      index: int,
      total: int,
      config: ExperimentConfig,
      status: str = "running",
      elapsed: float | None = None,
      energy: float | None = None,
  ) -> None:
      """Print a per-experiment progress line to stderr.

      Format: [3/12] <icon> model backend precision -- elapsed (energy)
      Icons: completed=OK, failed=FAIL, running=...

      Args:
          index: 1-based experiment index.
          total: Total experiments in study.
          config: ExperimentConfig for this experiment.
          status: "running", "completed", or "failed".
          elapsed: Elapsed time in seconds (None if not yet available).
          energy: Energy in joules (None if not yet available).
      """
      icons = {"running": "...", "completed": "OK", "failed": "FAIL"}
      icon = icons.get(status, "?")

      parts = [f"[{index}/{total}]", icon, config.model, config.backend, config.precision]

      if elapsed is not None:
          parts.append("--")
          parts.append(_format_duration(elapsed))
      if energy is not None:
          parts.append(f"({_sig3(energy)} J)")

      line = " ".join(parts)
      print(line, file=sys.stderr)


  def print_study_summary(result: StudyResult) -> None:
      """Print study summary table to stdout.

      Columns: #, Config, Status, Time, Energy, tok/s
      Failed experiments show error type instead of metrics.
      Footer with totals.

      Args:
          result: Completed StudyResult.
      """
      from llenergymeasure.domain.experiment import StudyResult

      print()
      print(f"Study: {result.name or 'unnamed'}")
      if result.study_design_hash:
          print(f"Hash:  {result.study_design_hash}")
      print()

      # Table header
      header = f"{'#':>3}  {'Config':<40}  {'Status':<8}  {'Time':>8}  {'Energy':>10}  {'tok/s':>8}"
      print(header)
      print("-" * len(header))

      # Table rows
      for i, exp in enumerate(result.experiments, 1):
          model_short = exp.effective_config.get("model", "unknown")
          if len(model_short) > 20:
              model_short = "..." + model_short[-17:]
          backend = exp.backend
          precision = getattr(exp, "precision", "?")
          if hasattr(exp, "effective_config"):
              precision = exp.effective_config.get("precision", precision)
          config_str = f"{model_short} / {backend} / {precision}"
          if len(config_str) > 40:
              config_str = config_str[:37] + "..."

          # Check if this is a failed experiment (has measurement_warnings with error info)
          # In M2, all returned experiments are ExperimentResult objects
          time_str = _format_duration(exp.duration_sec)
          energy_str = f"{_sig3(exp.total_energy_j)} J"
          toks_str = _sig3(exp.avg_tokens_per_second)

          print(f"{i:>3}  {config_str:<40}  {'OK':<8}  {time_str:>8}  {energy_str:>10}  {toks_str:>8}")

      print("-" * len(header))

      # Footer with totals
      if result.summary:
          s = result.summary
          print(
              f"Total: {s.completed}/{s.total_experiments} completed"
              f"  |  {_format_duration(s.total_wall_time_s)}"
              f"  |  {_sig3(s.total_energy_j)} J"
          )
          if s.failed > 0:
              print(f"Failed: {s.failed} experiment(s)")
          if s.warnings:
              for w in s.warnings:
                  print(f"  Warning: {w}")
      print()

      # Output paths
      if result.result_files:
          print(f"Results saved: {len(result.result_files)} file(s)")
          for path in result.result_files[:3]:
              print(f"  {path}")
          if len(result.result_files) > 3:
              print(f"  ... and {len(result.result_files) - 3} more")
  ```

  Add the necessary import at the top of `_display.py`:
  ```python
  from llenergymeasure.domain.experiment import ExperimentResult, StudyResult
  ```
  (ExperimentResult is already imported; add StudyResult to the existing import.)

  Note: The progress display (print_study_progress) is called by the CLI during study execution. The gap countdown is already handled by `study/gaps.py` (Phase 11) — CLI-11 is satisfied because the gap countdown display is always visible during study execution (it writes directly to stdout). The `--quiet` flag suppresses CLI output, but the gap countdown in `run_gap()` writes directly. To honour `--quiet`, the caller should set `experiment_gap_seconds=0` and `cycle_gap_seconds=0` (which `--no-gaps` already does). For `--quiet` without `--no-gaps`, we need to suppress gap output. This is handled by the fact that `_run()` creates the StudyRunner which calls `run_gap()` — there's no clean quiet mechanism through that path. For M2, document this limitation: `--quiet` does not suppress gap countdowns (they are subprocess-level). This is acceptable for M2.

  **tests/unit/test_cli_run.py — Add study CLI tests:**

  Append these tests to the existing file:

  ```python
  # === Study CLI tests (Phase 12) ===

  def test_study_detection_with_sweep_key(tmp_path):
      """YAML with sweep: key is detected as study mode."""
      study_yaml = tmp_path / "study.yaml"
      study_yaml.write_text('''
  name: test
  model: test/model
  sweep:
    precision: [fp32, fp16]
  ''')
      # Just verify detection logic, not full execution
      import yaml
      raw = yaml.safe_load(study_yaml.read_text())
      assert "sweep" in raw

  def test_study_detection_with_experiments_key(tmp_path):
      """YAML with experiments: key is detected as study mode."""
      study_yaml = tmp_path / "study.yaml"
      study_yaml.write_text('''
  name: test
  experiments:
    - model: test/model-a
    - model: test/model-b
  ''')
      import yaml
      raw = yaml.safe_load(study_yaml.read_text())
      assert "experiments" in raw

  def test_cli_flags_present():
      """llem run has --cycles, --order, --no-gaps flags."""
      from llenergymeasure.cli.run import run
      import inspect
      sig = inspect.signature(run)
      params = set(sig.parameters.keys())
      assert "cycles" in params
      assert "order" in params
      assert "no_gaps" in params

  def test_print_study_summary_basic():
      """print_study_summary runs without error on a minimal StudyResult."""
      from io import StringIO
      from unittest.mock import patch, MagicMock
      from llenergymeasure.cli._display import print_study_summary
      from llenergymeasure.domain.experiment import StudyResult, StudySummary

      # Create a minimal StudyResult with mocked experiments
      exp = MagicMock()
      exp.effective_config = {"model": "test/model", "precision": "fp16"}
      exp.backend = "pytorch"
      exp.duration_sec = 45.2
      exp.total_energy_j = 123.4
      exp.avg_tokens_per_second = 42.5

      summary = StudySummary(
          total_experiments=1,
          completed=1,
          failed=0,
          total_wall_time_s=50.0,
          total_energy_j=123.4,
      )
      result = StudyResult(
          experiments=[exp],
          name="test-study",
          study_design_hash="abcd1234",
          summary=summary,
          result_files=["results/exp1/result.json"],
      )

      # Capture stdout
      with patch("sys.stdout", new_callable=StringIO) as mock_stdout:
          print_study_summary(result)
      output = mock_stdout.getvalue()
      assert "test-study" in output
      assert "abcd1234" in output

  def test_print_study_progress():
      """print_study_progress produces a formatted line to stderr."""
      from io import StringIO
      from unittest.mock import patch
      from llenergymeasure.cli._display import print_study_progress
      from llenergymeasure.config.models import ExperimentConfig

      config = ExperimentConfig(model="test/model", backend="pytorch")
      with patch("sys.stderr", new_callable=StringIO) as mock_stderr:
          print_study_progress(1, 4, config, status="completed", elapsed=30.5, energy=100.0)
      output = mock_stderr.getvalue()
      assert "[1/4]" in output
      assert "OK" in output
      assert "test/model" in output
  ```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_cli_run.py -v --tb=short -k "study or progress or summary or flags" 2>&1 | tail -20</automated>
  </verify>
  <done>
    - llem run detects study YAML via sweep:/experiments: keys and routes to study path
    - --cycles, --order, --no-gaps flags added and passed through to study execution
    - CLI effective defaults: n_cycles=3, cycle_order="shuffled"
    - print_study_progress() shows per-experiment status line
    - print_study_summary() shows table with Config/Status/Time/Energy/tok/s columns
    - --quiet suppresses CLI-side progress lines and summary; gap countdown suppression deferred (M2 limitation, subprocess-level)
    - All CLI tests pass (existing + 5 new)
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/unit/test_cli_run.py -v` — all CLI tests pass (existing + new study tests)
2. `python -c "from llenergymeasure.cli.run import run; import inspect; sig = inspect.signature(run); assert 'cycles' in sig.parameters"` — study flags present
3. `python -c "from llenergymeasure.cli._display import print_study_summary, print_study_progress; print('imports OK')"` — display functions importable
4. `python -m pytest tests/unit/ -x --tb=short` — full unit test suite passes
</verification>

<success_criteria>
- llem run study.yaml detects study mode and routes to run_study() (CLI-05)
- --cycles, --order, --no-gaps flags work and override execution block values (CLI-05)
- CLI effective defaults apply: n_cycles=3, cycle_order="shuffled" when unspecified (CLI-05)
- Per-experiment progress line visible during study execution
- Thermal gap countdown visible during pauses (CLI-11, already from Phase 11)
- Study summary table displayed with Config/Status/Time/Energy/tok/s columns
- --quiet suppresses CLI-side progress lines and summary (gap countdown suppression is a documented M2 limitation)
- All existing CLI tests pass
- 5+ new CLI/display tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/12-integration/12-03-SUMMARY.md`
</output>
