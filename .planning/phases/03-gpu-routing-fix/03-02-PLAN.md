---
phase: 03-gpu-routing-fix
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/validation.py
  - src/llenergymeasure/config/loader.py
  - src/llenergymeasure/cli/experiment.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Invalid parallelism configs fail fast with clear error before container launch"
    - "tensor_parallel_size > len(gpus) raises ConfigurationError"
    - "tp_size > len(gpus) raises ConfigurationError for TensorRT"
    - "Error messages include remediation hints"
  artifacts:
    - path: "src/llenergymeasure/config/validation.py"
      provides: "validate_parallelism_constraints function"
      contains: "tensor_parallel_size"
    - path: "src/llenergymeasure/config/loader.py"
      provides: "Parallelism validation on config load"
      contains: "validate_parallelism_constraints"
  key_links:
    - from: "src/llenergymeasure/config/loader.py"
      to: "src/llenergymeasure/config/validation.py"
      via: "import and call validate_parallelism_constraints"
      pattern: "validate_parallelism_constraints"
---

<objective>
Add fail-fast validation for parallelism constraints at config load time.

Purpose: Prevent cryptic backend initialisation errors by validating that tensor_parallel_size/tp_size does not exceed available GPUs BEFORE container launch.

Output: Validation function in validation.py, wired into config loader with clear error messages.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-gpu-routing-fix/03-RESEARCH.md
@src/llenergymeasure/config/validation.py
@src/llenergymeasure/config/loader.py
@src/llenergymeasure/config/backend_configs.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add validate_parallelism_constraints function</name>
  <files>src/llenergymeasure/config/validation.py</files>
  <action>
Add a new function `validate_parallelism_constraints()` to validation.py:

```python
def validate_parallelism_constraints(
    config: "ExperimentConfig",
) -> ValidationWarnings:
    """Validate parallelism settings against available GPUs.

    Checks that tensor_parallel_size (vLLM) or tp_size (TensorRT) does not
    exceed the number of GPUs specified in config.gpus. This prevents cryptic
    backend initialisation errors.

    Args:
        config: The experiment configuration.

    Returns:
        List of ConfigWarning objects (severity="error" for violations).
    """
    from llenergymeasure.config.models import ExperimentConfig  # noqa: F811

    warnings: ValidationWarnings = []
    gpu_count = len(config.gpus) if config.gpus else 1

    # vLLM tensor parallelism
    if config.backend == "vllm" and config.vllm:
        tp = config.vllm.tensor_parallel_size
        if tp > gpu_count:
            warnings.append(ConfigWarning(
                field="vllm.tensor_parallel_size",
                message=(
                    f"tensor_parallel_size={tp} exceeds available GPUs ({gpu_count}). "
                    f"vLLM requires tensor_parallel_size <= len(gpus)."
                ),
                severity="error",
                suggestion=(
                    f"Either set gpus: [0, 1, ..., {tp-1}] to provide {tp} GPUs, "
                    f"or reduce tensor_parallel_size to {gpu_count} or less."
                ),
            ))
        # Also check pipeline parallelism
        pp = config.vllm.pipeline_parallel_size
        total_parallel = tp * pp
        if total_parallel > gpu_count:
            warnings.append(ConfigWarning(
                field="vllm.pipeline_parallel_size",
                message=(
                    f"tensor_parallel_size * pipeline_parallel_size = {total_parallel} "
                    f"exceeds available GPUs ({gpu_count})."
                ),
                severity="error",
                suggestion=(
                    f"Total parallelism (TP * PP) must not exceed GPU count. "
                    f"Reduce parallelism or add more GPUs."
                ),
            ))

    # TensorRT tensor parallelism
    if config.backend == "tensorrt" and config.tensorrt:
        tp = config.tensorrt.tp_size
        if tp > gpu_count:
            warnings.append(ConfigWarning(
                field="tensorrt.tp_size",
                message=(
                    f"tp_size={tp} exceeds available GPUs ({gpu_count}). "
                    f"TensorRT-LLM requires tp_size <= len(gpus)."
                ),
                severity="error",
                suggestion=(
                    f"Either set gpus: [0, 1, ..., {tp-1}] to provide {tp} GPUs, "
                    f"or reduce tp_size to {gpu_count} or less."
                ),
            ))
        # Also check pipeline parallelism
        pp = config.tensorrt.pp_size
        total_parallel = tp * pp
        if total_parallel > gpu_count:
            warnings.append(ConfigWarning(
                field="tensorrt.pp_size",
                message=(
                    f"tp_size * pp_size = {total_parallel} exceeds available GPUs ({gpu_count})."
                ),
                severity="error",
                suggestion=(
                    f"Total parallelism (TP * PP) must not exceed GPU count. "
                    f"Reduce parallelism or add more GPUs."
                ),
            ))

    # PyTorch data parallelism (less critical but good to validate)
    if config.backend == "pytorch" and config.pytorch:
        num_procs = config.pytorch.num_processes
        if num_procs > gpu_count:
            warnings.append(ConfigWarning(
                field="pytorch.num_processes",
                message=(
                    f"num_processes={num_procs} exceeds available GPUs ({gpu_count}). "
                    f"Each process requires its own GPU for data parallelism."
                ),
                severity="error",
                suggestion=(
                    f"Either set gpus: [0, 1, ..., {num_procs-1}] to provide {num_procs} GPUs, "
                    f"or reduce num_processes to {gpu_count} or less."
                ),
            ))

    return warnings
```

Add TYPE_CHECKING import and forward reference for ExperimentConfig to avoid circular imports.
  </action>
  <verify>
Run `python -c "from llenergymeasure.config.validation import validate_parallelism_constraints; print('OK')"` - should import without error.
  </verify>
  <done>
`validate_parallelism_constraints()` function exists in validation.py with checks for vLLM, TensorRT, and PyTorch parallelism settings.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire parallelism validation into config loader</name>
  <files>src/llenergymeasure/config/loader.py</files>
  <action>
Integrate parallelism validation into the config loading process:

1. Import the validation function:
   ```python
   from llenergymeasure.config.validation import (
       ConfigWarning,
       validate_parallelism_constraints,
   )
   ```

2. In `load_config()` or the appropriate validation entry point, add parallelism validation after config is parsed but before returning:
   ```python
   # Validate parallelism constraints
   parallelism_warnings = validate_parallelism_constraints(config)

   # Treat severity="error" warnings as blocking
   errors = [w for w in parallelism_warnings if w.severity == "error"]
   if errors:
       error_msgs = "\n".join(f"  - {w}" for w in errors)
       raise ConfigurationError(
           f"Parallelism validation failed:\n{error_msgs}"
       )

   # Add non-error warnings to config_warnings list
   config_warnings.extend(
       w for w in parallelism_warnings if w.severity != "error"
   )
   ```

3. Import ConfigurationError if not already imported:
   ```python
   from llenergymeasure.exceptions import ConfigurationError
   ```

WHY: Fail-fast validation catches parallelism misconfigurations before any container is launched or backend initialised, providing clear error messages with remediation hints.
  </action>
  <verify>
Run `python -c "from llenergymeasure.config.loader import load_config; print('OK')"` - should import without error.
  </verify>
  <done>
Config loader calls `validate_parallelism_constraints()` and raises ConfigurationError for severity="error" warnings.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add helpful error display in CLI</name>
  <files>src/llenergymeasure/cli/experiment.py</files>
  <action>
Ensure the CLI displays parallelism errors clearly:

1. In the experiment command error handling, catch ConfigurationError and display it nicely:
   ```python
   except ConfigurationError as e:
       console.print(f"[red]Configuration Error:[/red]\n{e}")
       raise typer.Exit(1)
   ```

2. If not already present, ensure validation errors from load_config are displayed with the suggestion field:
   ```python
   # When displaying ConfigWarning errors
   if warning.suggestion:
       console.print(f"  [dim]Hint: {warning.suggestion}[/dim]")
   ```

3. Consider adding a `--validate-only` or `--dry-run` mode that validates config without running (if not already present for experiment command - may already exist).

WHY: Clear error messages with remediation hints help users fix config issues quickly without digging through backend stack traces.
  </action>
  <verify>
Run `python -c "from llenergymeasure.cli.experiment import experiment_cmd; print('OK')"` - should import without error.
  </verify>
  <done>
CLI experiment command displays ConfigurationError with clear formatting and remediation hints.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `ruff check src/llenergymeasure/config/validation.py src/llenergymeasure/config/loader.py` passes
2. `pytest tests/unit/config/ -v --tb=short` passes
3. Create a test config with tensor_parallel_size=4 but gpus=[0] and verify it fails with clear error:
   ```bash
   echo "config_name: test
   model_name: gpt2
   backend: vllm
   gpus: [0]
   vllm:
     tensor_parallel_size: 4" > /tmp/bad_config.yaml
   lem experiment /tmp/bad_config.yaml --dry-run 2>&1 | grep -i "exceeds available GPUs"
   ```
</verification>

<success_criteria>
- validate_parallelism_constraints() validates vLLM tensor_parallel_size, TensorRT tp_size, PyTorch num_processes
- Config loader raises ConfigurationError for parallelism violations
- Error messages include the specific constraint violated and remediation hints
- CLI displays errors clearly with suggestions
</success_criteria>

<output>
After completion, create `.planning/phases/03-gpu-routing-fix/03-02-SUMMARY.md`
</output>
