---
phase: 04-pytorch-backend-pre-flight
plan: 02
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - src/llenergymeasure/core/backends/__init__.py
  - src/llenergymeasure/core/backends/protocol.py
  - src/llenergymeasure/core/backends/pytorch.py
  - tests/unit/test_backend_protocol.py
autonomous: true
requirements: [CM-01, CM-04, CM-05, CM-06]

must_haves:
  truths:
    - "InferenceBackend Protocol defines the contract all backends must satisfy"
    - "PyTorchBackend.run(config) returns ExperimentResult with environment_snapshot populated"
    - "model_kwargs are built AND passed to from_pretrained() directly — no intermediate loader"
    - "passthrough_kwargs from config are merged into model load kwargs (P0 fix)"
    - "Backend default detection returns 'pytorch' when transformers is installed"
  artifacts:
    - path: "src/llenergymeasure/core/backends/__init__.py"
      provides: "Package init with get_backend and backend detection"
      exports: ["get_backend", "detect_default_backend"]
    - path: "src/llenergymeasure/core/backends/protocol.py"
      provides: "InferenceBackend Protocol"
      contains: "class InferenceBackend"
    - path: "src/llenergymeasure/core/backends/pytorch.py"
      provides: "Rewritten PyTorch backend"
      contains: "class PyTorchBackend"
    - path: "tests/unit/test_backend_protocol.py"
      provides: "Protocol and backend detection tests"
  key_links:
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/config/models.py"
      via: "accepts ExperimentConfig"
      pattern: "config: ExperimentConfig"
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/domain/environment.py"
      via: "calls collect_environment_snapshot()"
      pattern: "collect_environment_snapshot"
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/domain/experiment.py"
      via: "returns ExperimentResult"
      pattern: "ExperimentResult"
---

<objective>
Create the InferenceBackend Protocol and rewrite the PyTorch inference backend from scratch using the v2.0 ExperimentConfig contract. Fix the P0 model_kwargs bug by eliminating the intermediate loader abstraction entirely.

Purpose: This is the core measurement entry point — the backend that loads a model, runs inference, and produces an ExperimentResult. The P0 bug fix is achieved structurally (not patched).

Output: `core/backends/` package with Protocol, PyTorch implementation, and backend detection. GPU-free protocol and detection tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/04-pytorch-backend-pre-flight/04-CONTEXT.md
@.planning/phases/04-pytorch-backend-pre-flight/04-RESEARCH.md
@.planning/phases/04-pytorch-backend-pre-flight/04-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts the executor needs from Plan 01 and existing code. -->

From src/llenergymeasure/config/models.py:
```python
class ExperimentConfig(BaseModel):
    model: str = Field(...)
    backend: Literal["pytorch", "vllm", "tensorrt"] = Field(default="pytorch")
    precision: Literal["fp32", "fp16", "bf16"] = Field(default="bf16")
    n: int = Field(default=100, ge=1)
    dataset: str | SyntheticDatasetConfig = Field(default="aienergyscore")
    max_input_tokens: int = Field(default=512, ge=1)
    max_output_tokens: int = Field(default=128, ge=1)
    decoder: DecoderConfig = Field(default_factory=DecoderConfig)
    warmup: WarmupConfig = Field(default_factory=WarmupConfig)
    baseline: BaselineConfig = Field(default_factory=BaselineConfig)
    pytorch: PyTorchConfig | None = Field(default=None)
    lora: LoRAConfig | None = Field(default=None)
    passthrough_kwargs: dict[str, Any] | None = Field(default=None)
    output_dir: str | None = Field(default=None)
    random_seed: int = Field(default=42)
```

From src/llenergymeasure/config/backend_configs.py:
```python
class PyTorchConfig(BaseModel):
    batch_size: int | None = Field(default=None, ge=1)
    attn_implementation: Literal["sdpa", "flash_attention_2", "eager"] | None = Field(default=None)
    torch_compile: bool | None = Field(default=None)
    load_in_4bit: bool | None = Field(default=None)
    load_in_8bit: bool | None = Field(default=None)
    num_processes: int | None = Field(default=None, ge=1)
```

From src/llenergymeasure/domain/experiment.py:
```python
class ExperimentResult(BaseModel):
    schema_version: str
    experiment_id: str
    backend: str
    total_tokens: int
    total_energy_j: float
    total_inference_time_sec: float
    avg_tokens_per_second: float
    avg_energy_per_token_j: float
    total_flops: float
    aggregation: AggregationMetadata
    process_results: list[RawProcessResult]
    start_time: datetime
    end_time: datetime
    environment_snapshot: EnvironmentSnapshot | None  # Added in Plan 01
    model_config = {"frozen": True}
```

From src/llenergymeasure/domain/environment.py (Plan 01 additions):
```python
class EnvironmentSnapshot(BaseModel): ...
def collect_environment_snapshot() -> EnvironmentSnapshot: ...
```

From src/llenergymeasure/exceptions.py:
```python
class BackendError(LLEMError): ...
```

From src/llenergymeasure/domain/experiment.py:
```python
class AggregationMetadata(BaseModel):
    method: str = Field(default="sum_energy_avg_throughput")
    num_processes: int
    temporal_overlap_verified: bool = Field(default=False)
    gpu_attribution_verified: bool = Field(default=False)
    warnings: list[str] = Field(default_factory=list)
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create InferenceBackend Protocol and PyTorch backend implementation</name>
  <files>
    src/llenergymeasure/core/backends/__init__.py
    src/llenergymeasure/core/backends/protocol.py
    src/llenergymeasure/core/backends/pytorch.py
  </files>
  <action>
**1. Create `src/llenergymeasure/core/backends/__init__.py`** — package init with backend detection and factory.

```python
"""Inference backends for llenergymeasure."""

from llenergymeasure.core.backends.protocol import InferenceBackend

def detect_default_backend() -> str:
    """Detect the default available backend. Returns 'pytorch' if transformers installed."""
    import importlib.util
    if importlib.util.find_spec("transformers") is not None:
        return "pytorch"
    # Future: check vllm, tensorrt_llm
    raise BackendError("No inference backend installed. pip install llenergymeasure[pytorch]")

def get_backend(name: str) -> InferenceBackend:
    """Get an inference backend by name."""
    if name == "pytorch":
        from llenergymeasure.core.backends.pytorch import PyTorchBackend
        return PyTorchBackend()
    raise BackendError(f"Unknown backend: {name!r}. Available: pytorch")
```

Import `BackendError` from `llenergymeasure.exceptions`.

**2. Create `src/llenergymeasure/core/backends/protocol.py`** — the InferenceBackend Protocol.

```python
from typing import Protocol, runtime_checkable
from llenergymeasure.config.models import ExperimentConfig
from llenergymeasure.domain.experiment import ExperimentResult

@runtime_checkable
class InferenceBackend(Protocol):
    """Contract for all inference backends (PyTorch, vLLM, TRT-LLM)."""

    @property
    def name(self) -> str: ...

    def run(self, config: ExperimentConfig) -> ExperimentResult: ...
```

Simple Protocol — one property, one method. The backend owns its entire lifecycle internally (model load, warmup, measurement, cleanup). This matches lm-eval's LM subclass pattern.

**3. Create `src/llenergymeasure/core/backends/pytorch.py`** — rewritten PyTorch backend.

**CRITICAL: This is a REWRITE from scratch.** Do NOT adapt the v1.x `core/inference_backends/pytorch.py`. Use it as reference only.

Class: `PyTorchBackend` with `name = "pytorch"` property and `run(config) -> ExperimentResult`.

Use stdlib logging: `import logging; logger = logging.getLogger(__name__)`.

The `run()` method structure:
```python
def run(self, config: ExperimentConfig) -> ExperimentResult:
    """Run a complete PyTorch inference experiment."""
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    # 1. Environment snapshot (BEFORE model loading — CM-32)
    snapshot = collect_environment_snapshot()

    # 2. Load model + tokenizer
    model, tokenizer = self._load_model(config)

    # 3. Prepare prompts
    prompts = self._prepare_prompts(config, tokenizer)

    # 4. Warmup (config.warmup.n_warmup iterations)
    self._run_warmup(model, tokenizer, config, prompts)

    # 5. Measurement
    start_time = datetime.now()
    result_data = self._run_measurement(model, tokenizer, config, prompts)
    end_time = datetime.now()

    # 6. Build ExperimentResult
    result = self._build_result(config, result_data, snapshot, start_time, end_time)

    # 7. Cleanup
    self._cleanup(model)

    return result
```

**`_load_model(config)` — THE P0 FIX (CM-06):**

The P0 bug root cause: v1.x `_build_model_kwargs()` builds kwargs but `loader.load(config)` ignores them. The fix: call `AutoModelForCausalLM.from_pretrained()` directly with ALL kwargs. No intermediate loader class.

```python
def _load_model(self, config: ExperimentConfig):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    kwargs = self._model_load_kwargs(config)
    logger.info(f"Loading model {config.model} with kwargs: {list(kwargs.keys())}")
    tokenizer = AutoTokenizer.from_pretrained(config.model, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(config.model, **kwargs)
    model.eval()
    return model, tokenizer
```

**`_model_load_kwargs(config)` — build kwargs dict:**
```python
def _model_load_kwargs(self, config: ExperimentConfig) -> dict:
    kwargs: dict = {
        "torch_dtype": self._precision_to_dtype(config.precision),
        "device_map": "auto",
        "trust_remote_code": True,
    }
    if config.pytorch:
        if config.pytorch.attn_implementation is not None:
            kwargs["attn_implementation"] = config.pytorch.attn_implementation
        if config.pytorch.load_in_4bit:
            kwargs["load_in_4bit"] = True
        if config.pytorch.load_in_8bit:
            kwargs["load_in_8bit"] = True
    # passthrough_kwargs — the P0 fix: these are passed through directly
    if config.passthrough_kwargs:
        kwargs.update(config.passthrough_kwargs)
    return kwargs
```

**`_precision_to_dtype(precision)` — simple dict lookup:**
```python
@staticmethod
def _precision_to_dtype(precision: str):
    import torch
    return {"fp32": torch.float32, "fp16": torch.float16, "bf16": torch.bfloat16}[precision]
```

**`_prepare_prompts(config, tokenizer)`:**
- For M1, generate simple prompts: `["Hello, " * 10] * config.n` or similar placeholder.
- Phase 5 will add proper dataset loading. For now, generate `config.n` simple prompts.
- This is a placeholder that enables the backend to run end-to-end.

**`_run_warmup(model, tokenizer, config, prompts)`:**
- Run `config.warmup.n_warmup` inference iterations using the first prompt.
- Use `torch.inference_mode()` context.
- Log warmup progress.

**`_run_measurement(model, tokenizer, config, prompts)`:**
- Iterate over prompts in batches (batch_size from `config.pytorch.batch_size` or 1).
- Use `torch.inference_mode()` context.
- Tokenize → `model.generate()` with `max_new_tokens=config.max_output_tokens`.
- Apply decoder config: `temperature`, `do_sample`, `top_k`, `top_p`, `repetition_penalty` from `config.decoder`.
- Track: total tokens generated, total time, peak memory.
- Return a dict/dataclass with measurement results.
- Wrap all inference errors in `BackendError` with helpful message: "CUDA out of memory. Try: reduce batch_size, use precision=fp16, or use a smaller model" for OOM; generic "Inference failed: {error}" otherwise.

**`_build_result(config, result_data, snapshot, start_time, end_time)`:**
- Create `ExperimentResult` with:
  - `experiment_id`: `f"{config.model}_{start_time.strftime('%Y%m%d_%H%M%S')}"`
  - `backend`: "pytorch"
  - `total_tokens`, `total_energy_j` (0.0 placeholder — Phase 5 adds energy), `total_inference_time_sec`, `avg_tokens_per_second`, `avg_energy_per_token_j` (0.0 placeholder), `total_flops` (0.0 placeholder)
  - `aggregation`: `AggregationMetadata(num_processes=1, method="single_process")`
  - `start_time`, `end_time`
  - `environment_snapshot`: the captured snapshot
  - `process_results`: empty list (single-process M1, no per-process breakdown needed)
  - Energy fields (total_energy_j, avg_energy_per_token_j, total_flops) are 0.0 placeholders — Phase 5 populates them.

**`_cleanup(model)`:**
- `del model`
- `torch.cuda.empty_cache()` if CUDA available.
- Log cleanup.

**IMPORTANT constraints:**
- Use `import logging; logger = logging.getLogger(__name__)` — NOT loguru.
- All heavy imports (`torch`, `transformers`) inside method bodies, NOT at module level.
- Wrap `torch.cuda.OutOfMemoryError` (and `RuntimeError` with "CUDA out of memory") in `BackendError` with fix suggestions (per CONTEXT.md).
- No intermediate `HuggingFaceModelLoader` class — call `from_pretrained()` directly.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.core.backends.protocol import InferenceBackend
from llenergymeasure.core.backends import get_backend, detect_default_backend
print('Protocol import OK')
backend = get_backend('pytorch')
print(f'Backend: {backend.name}')
print(f'Is InferenceBackend: {isinstance(backend, InferenceBackend)}')
print('All imports OK')
"</automated>
  </verify>
  <done>
    - `InferenceBackend` Protocol exists with `name` property and `run(config)` method
    - `PyTorchBackend` satisfies `InferenceBackend` Protocol (isinstance check passes)
    - `_model_load_kwargs()` builds kwargs AND they are passed to `from_pretrained()` directly (P0 fix)
    - `passthrough_kwargs` are merged into model load kwargs
    - `get_backend("pytorch")` returns a `PyTorchBackend` instance
    - `detect_default_backend()` returns "pytorch" when transformers is installed
    - All new code uses stdlib logging
  </done>
</task>

<task type="auto">
  <name>Task 2: Write GPU-free Protocol and backend detection tests</name>
  <files>
    tests/unit/test_backend_protocol.py
  </files>
  <action>
Create `tests/unit/test_backend_protocol.py` — GPU-free tests for the Protocol, backend factory, and detection.

```python
# Test: PyTorchBackend satisfies InferenceBackend Protocol
def test_pytorch_backend_satisfies_protocol():
    from llenergymeasure.core.backends.protocol import InferenceBackend
    from llenergymeasure.core.backends.pytorch import PyTorchBackend
    backend = PyTorchBackend()
    assert isinstance(backend, InferenceBackend)
    assert backend.name == "pytorch"

# Test: get_backend returns correct backend
def test_get_backend_pytorch():
    from llenergymeasure.core.backends import get_backend
    backend = get_backend("pytorch")
    assert backend.name == "pytorch"

# Test: get_backend unknown raises BackendError
def test_get_backend_unknown():
    from llenergymeasure.core.backends import get_backend
    from llenergymeasure.exceptions import BackendError
    with pytest.raises(BackendError, match="Unknown backend"):
        get_backend("nonexistent")

# Test: detect_default_backend returns pytorch when transformers installed
def test_detect_default_backend_pytorch():
    from llenergymeasure.core.backends import detect_default_backend
    # transformers is installed in test env
    assert detect_default_backend() == "pytorch"

# Test: detect_default_backend raises when no backends
def test_detect_default_backend_none(monkeypatch):
    import importlib.util
    original_find_spec = importlib.util.find_spec
    def mock_find_spec(name, *args, **kwargs):
        if name == "transformers":
            return None
        return original_find_spec(name, *args, **kwargs)
    monkeypatch.setattr(importlib.util, "find_spec", mock_find_spec)
    from llenergymeasure.core.backends import detect_default_backend
    from llenergymeasure.exceptions import BackendError
    with pytest.raises(BackendError, match="No inference backend"):
        detect_default_backend()

# Test: _model_load_kwargs builds correct kwargs (P0 fix verification)
def test_model_load_kwargs_basic():
    from llenergymeasure.core.backends.pytorch import PyTorchBackend
    from llenergymeasure.config.models import ExperimentConfig
    backend = PyTorchBackend()
    config = ExperimentConfig(model="gpt2")
    kwargs = backend._model_load_kwargs(config)
    assert "torch_dtype" in kwargs
    assert kwargs["device_map"] == "auto"
    assert kwargs["trust_remote_code"] is True

# Test: _model_load_kwargs includes passthrough_kwargs (P0 fix)
def test_model_load_kwargs_passthrough():
    from llenergymeasure.core.backends.pytorch import PyTorchBackend
    from llenergymeasure.config.models import ExperimentConfig
    backend = PyTorchBackend()
    config = ExperimentConfig(model="gpt2", passthrough_kwargs={"custom_key": "value"})
    kwargs = backend._model_load_kwargs(config)
    assert kwargs["custom_key"] == "value"

# Test: _model_load_kwargs includes PyTorchConfig options
def test_model_load_kwargs_pytorch_config():
    from llenergymeasure.core.backends.pytorch import PyTorchBackend
    from llenergymeasure.config.models import ExperimentConfig
    from llenergymeasure.config.backend_configs import PyTorchConfig
    backend = PyTorchBackend()
    config = ExperimentConfig(
        model="gpt2",
        pytorch=PyTorchConfig(attn_implementation="sdpa", load_in_4bit=True),
    )
    kwargs = backend._model_load_kwargs(config)
    assert kwargs["attn_implementation"] == "sdpa"
    assert kwargs["load_in_4bit"] is True

# Test: _precision_to_dtype mapping
def test_precision_to_dtype():
    from llenergymeasure.core.backends.pytorch import PyTorchBackend
    import torch
    assert PyTorchBackend._precision_to_dtype("fp32") == torch.float32
    assert PyTorchBackend._precision_to_dtype("fp16") == torch.float16
    assert PyTorchBackend._precision_to_dtype("bf16") == torch.bfloat16
```

These tests validate the Protocol contract, backend detection, and the P0 fix (kwargs are built correctly) — all without requiring a GPU. The `_precision_to_dtype` test does import torch (for dtype comparison) but doesn't use CUDA.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_backend_protocol.py -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>
    - Protocol satisfaction test passes (isinstance check)
    - get_backend factory tests pass
    - detect_default_backend tests pass (with and without transformers mocked)
    - _model_load_kwargs tests verify P0 fix: passthrough_kwargs are included
    - _precision_to_dtype tests pass
    - All tests run without a GPU
  </done>
</task>

</tasks>

<verification>
- `python -c "from llenergymeasure.core.backends import get_backend; b = get_backend('pytorch'); print(b.name)"` prints "pytorch"
- `python -c "from llenergymeasure.core.backends.protocol import InferenceBackend; from llenergymeasure.core.backends.pytorch import PyTorchBackend; assert isinstance(PyTorchBackend(), InferenceBackend)"` passes
- `pytest tests/unit/test_backend_protocol.py -x` all pass
- No `from loguru import logger` in any newly created file
- `grep -r "loader.load" src/llenergymeasure/core/backends/` returns nothing (no intermediate loader)
</verification>

<success_criteria>
1. `InferenceBackend` Protocol exists with `name` property and `run(config) -> ExperimentResult` method
2. `PyTorchBackend` satisfies the Protocol and has `name == "pytorch"`
3. `_model_load_kwargs()` builds kwargs and they are passed directly to `from_pretrained()` — no intermediate loader class (P0 fix CM-06)
4. `passthrough_kwargs` from config are merged into model load kwargs
5. `get_backend("pytorch")` returns a working `PyTorchBackend` instance
6. `detect_default_backend()` returns "pytorch" when transformers is installed (CM-05)
7. All tests pass without a GPU
</success_criteria>

<output>
After completion, create `.planning/phases/04-pytorch-backend-pre-flight/04-02-SUMMARY.md`
</output>
