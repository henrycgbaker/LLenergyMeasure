---
phase: 18-docker-pre-flight
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/infra/docker_preflight.py
  - src/llenergymeasure/exceptions.py
  - src/llenergymeasure/orchestration/preflight.py
  - src/llenergymeasure/cli/run.py
  - src/llenergymeasure/config/models.py
  - tests/unit/test_docker_preflight.py
autonomous: true
requirements:
  - DOCK-07
  - DOCK-08
  - DOCK-09

must_haves:
  truths:
    - "Running llem run with runner: docker on a host without NVIDIA Container Toolkit produces a clear, actionable error before any container starts"
    - "A container-level GPU visibility check confirms the GPU is accessible inside the container before the experiment starts"
    - "A CUDA/driver version mismatch between host and container image produces a descriptive error naming the versions"
    - "Passing --skip-preflight or docker.skip_preflight: true bypasses all Docker pre-flight checks"
    - "When all checks pass, no output is produced — execution proceeds silently"
    - "Multiple failures within the same tier are reported together as a numbered list"
    - "Missing host nvidia-smi does not hard-block — warns but allows container launch (remote Docker daemon scenario)"
  artifacts:
    - path: "src/llenergymeasure/infra/docker_preflight.py"
      provides: "Docker pre-flight check module with tiered execution"
      exports: ["run_docker_preflight"]
    - path: "tests/unit/test_docker_preflight.py"
      provides: "Unit tests for all pre-flight check paths"
      min_lines: 100
  key_links:
    - from: "src/llenergymeasure/orchestration/preflight.py"
      to: "src/llenergymeasure/infra/docker_preflight.py"
      via: "run_docker_preflight() called from run_study_preflight() when any runner is Docker"
      pattern: "run_docker_preflight"
    - from: "src/llenergymeasure/cli/run.py"
      to: "src/llenergymeasure/orchestration/preflight.py"
      via: "--skip-preflight flag passed through to preflight"
      pattern: "skip.preflight"
---

<objective>
Implement tiered Docker pre-flight checks that validate the host environment before any container is launched, giving researchers actionable error messages when Docker, NVIDIA Container Toolkit, GPU visibility, or CUDA/driver compatibility is misconfigured.

Purpose: Researchers using Docker runners must get clear, immediate feedback when their host environment cannot support GPU containers — rather than cryptic Docker errors after a long image pull.

Output: `docker_preflight.py` module with tiered checks, `--skip-preflight` CLI flag, config YAML bypass option, and comprehensive unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-docker-pre-flight/18-CONTEXT.md

@src/llenergymeasure/infra/docker_errors.py
@src/llenergymeasure/infra/runner_resolution.py
@src/llenergymeasure/exceptions.py
@src/llenergymeasure/orchestration/preflight.py
@src/llenergymeasure/cli/run.py
@src/llenergymeasure/_api.py
@src/llenergymeasure/study/runner.py
@tests/unit/test_docker_errors.py

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/exceptions.py:
```python
class LLEMError(Exception): ...
class PreFlightError(LLEMError): ...
class DockerError(LLEMError): ...
```

From src/llenergymeasure/infra/runner_resolution.py:
```python
@dataclass
class RunnerSpec:
    mode: Literal["local", "docker"]
    image: str | None
    source: str

def is_docker_available() -> bool: ...
def resolve_runner(backend: str, yaml_runners=None, user_config=None) -> RunnerSpec: ...
def resolve_study_runners(backends: list[str], ...) -> dict[str, RunnerSpec]: ...
```

From src/llenergymeasure/infra/docker_errors.py:
```python
class DockerGPUAccessError(DockerError): ...  # has fix_suggestion, stderr_snippet
def translate_docker_error(returncode: int, stderr: str, image: str) -> DockerError: ...
```

From src/llenergymeasure/orchestration/preflight.py:
```python
def run_preflight(config: ExperimentConfig) -> None: ...
def run_study_preflight(study: StudyConfig) -> None: ...
```

From src/llenergymeasure/cli/run.py:
```python
def run(config, model, backend, ..., no_gaps, ...) -> None: ...
# run() catches PreFlightError and exits with code 1
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Docker pre-flight check module with tiered execution and unit tests</name>
  <files>
    src/llenergymeasure/infra/docker_preflight.py
    src/llenergymeasure/exceptions.py
    tests/unit/test_docker_preflight.py
  </files>
  <action>
Create `src/llenergymeasure/infra/docker_preflight.py` implementing tiered Docker pre-flight checks.

**Error class (in exceptions.py):**
Add `DockerPreFlightError(PreFlightError)` subclass to `exceptions.py`. This keeps Docker pre-flight errors in the `PreFlightError` hierarchy (caught by existing CLI error handling) while distinguishing them from general pre-flight errors. Single class — no per-check subclasses needed.

**Module structure (`docker_preflight.py`):**

One public function:
```python
def run_docker_preflight(skip: bool = False) -> None:
```
If `skip` is True, log a warning and return immediately.

**Tier 1 checks (host-level, no container needed):**
1. **Docker CLI available** — `shutil.which("docker")`. If missing: hard error "Docker not found on PATH" with fix suggestion.
2. **NVIDIA Container Toolkit installed** — check for any of `nvidia-container-runtime`, `nvidia-ctk`, `nvidia-container-cli` on PATH (reuse logic from `is_docker_available()` in `runner_resolution.py` — import it or extract the check). If missing: hard error with link to https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
3. **Host nvidia-smi** — run `nvidia-smi --query-gpu=driver_version --format=csv,noheader` via `subprocess.run` with `timeout=10`. Parse driver version string. If nvidia-smi not found or fails: **warn** (do not hard-error) — log message explaining this may be a remote Docker daemon setup, and proceed to tier 2. Store parsed driver version for tier 2 CUDA compat check.

Collect all tier 1 failures. If any hard failures exist, raise `DockerPreFlightError` with a numbered list of all failures. Format:
```
Docker pre-flight failed: N issue(s) found
  1. Docker not found on PATH
     Fix: Install Docker Engine — https://docs.docker.com/engine/install/
  2. NVIDIA Container Toolkit not found
     Fix: Install NVIDIA Container Toolkit — https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

**Tier 2 checks (require a running container):**
Only reached if tier 1 passes.

4. **GPU visibility inside container** (DOCK-08) — Run a lightweight container probe:
   ```
   docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi --query-gpu=name --format=csv,noheader
   ```
   Use `subprocess.run` with `capture_output=True, text=True, timeout=30`.
   - If the container fails to start or nvidia-smi fails inside: raise `DockerPreFlightError` with message "GPU not accessible inside Docker container" and fix suggestion linking to NVIDIA Container Toolkit docs.
   - On success: the output contains GPU name(s) — proceed.

5. **CUDA/driver compatibility** (DOCK-09) — Run inside the same container (or parse the tier-2 probe output more carefully):
   ```
   docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi --query-gpu=driver_version --format=csv,noheader
   ```
   Parse the container-reported driver version. Compare with host driver version (from tier 1 step 3, if available). If host driver was obtained and container driver differs significantly (major version mismatch), warn. The primary CUDA/driver check is: if tier-2 `nvidia-smi` succeeds inside the container, the driver is compatible. If it fails with an error containing "CUDA" or "driver", raise `DockerPreFlightError` with both version numbers: "Host driver {host_ver} may be incompatible with container CUDA requirements. See https://docs.nvidia.com/deploy/cuda-compatibility/".

   Implementation note: Combine steps 4 and 5 into a single `docker run` invocation to avoid launching two containers. Run:
   ```
   docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi --query-gpu=name,driver_version --format=csv,noheader
   ```
   Parse both GPU name and driver version from the single output line.

Collect all tier 2 failures and raise `DockerPreFlightError` with numbered list if any.

**Constants:**
- `_PROBE_IMAGE = "nvidia/cuda:12.0.0-base-ubuntu22.04"` — lightweight CUDA base image for probes
- `_PROBE_TIMEOUT = 30` — seconds for container probe
- `_NVIDIA_TOOLKIT_INSTALL_URL = "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"`
- `_CUDA_COMPAT_URL = "https://docs.nvidia.com/deploy/cuda-compatibility/"`
- `_DOCKER_INSTALL_URL = "https://docs.docker.com/engine/install/"`

**Unit tests (`tests/unit/test_docker_preflight.py`):**

All tests must be GPU-free (mock subprocess.run and shutil.which).

Test classes:
- `TestSkipPreflight`: verify `run_docker_preflight(skip=True)` returns immediately, logs warning.
- `TestTier1DockerCheck`: mock `shutil.which("docker")` returning None → DockerPreFlightError with "Docker not found".
- `TestTier1NvidiaToolkit`: mock all three NVIDIA tools absent → DockerPreFlightError with toolkit message.
- `TestTier1HostNvidiaSmi`: mock nvidia-smi not found → warning logged but no error (proceeds to tier 2). Mock nvidia-smi returning driver version → version parsed correctly.
- `TestTier1MultipleFailures`: both Docker and NVIDIA CT missing → single DockerPreFlightError with 2 numbered items.
- `TestTier2GPUVisibility`: mock Docker available + NVIDIA CT present + container probe fails (returncode=125, stderr with "could not select device driver") → DockerPreFlightError with GPU access message.
- `TestTier2GPUVisibilitySuccess`: mock container probe succeeding → no error raised.
- `TestTier2CUDADriverCompat`: mock container probe failing with CUDA/driver error → DockerPreFlightError naming versions.
- `TestTier2SkippedWhenTier1Fails`: mock Docker absent → tier 2 container probe never invoked (assert subprocess.run not called with "docker run").
- `TestInheritance`: `DockerPreFlightError` inherits from both `PreFlightError` and can be caught as `LLEMError`.

Use `unittest.mock.patch` for `shutil.which` and `subprocess.run`. Use `pytest.raises` for error assertions. Check `fix_suggestion` or error message content for URL presence.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_docker_preflight.py -x -v 2>&1 | tail -30</automated>
  </verify>
  <done>
    - `DockerPreFlightError` exists in exceptions.py, inherits `PreFlightError`
    - `run_docker_preflight(skip=False)` performs tiered checks: tier 1 (Docker CLI, NVIDIA CT, host nvidia-smi) then tier 2 (container GPU probe)
    - All tier 1 failures collected and reported together before abort
    - Missing host nvidia-smi warns but does not block (remote daemon scenario)
    - Tier 2 only runs if tier 1 passes
    - All tests pass, covering skip, tier 1 failures, tier 2 failures, inheritance, multi-failure grouping
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire pre-flight into CLI and study execution path</name>
  <files>
    src/llenergymeasure/orchestration/preflight.py
    src/llenergymeasure/cli/run.py
    src/llenergymeasure/_api.py
    src/llenergymeasure/config/models.py
    tests/unit/test_docker_preflight.py
  </files>
  <action>
Wire Docker pre-flight into the execution path so it runs once per study before the first Docker dispatch.

**1. Add `--skip-preflight` CLI flag (`cli/run.py`):**
Add to the `run()` function signature:
```python
skip_preflight: Annotated[
    bool,
    typer.Option("--skip-preflight", help="Skip Docker pre-flight checks (GPU visibility, CUDA/driver compat)"),
] = False,
```
Pass it through to `_run_impl()` and `_run_study_impl()`. These functions pass it to `run_experiment()` / `run_study()` / `_run()` as a `skip_preflight` keyword argument.

**2. Add `docker.skip_preflight` to config models (`config/models.py`):**
If there is not already a Docker config section in `StudyConfig` or `ExperimentConfig`, add a minimal one. Check if `ExecutionConfig` is the right place, or if a new `DockerConfig` nested model is needed. The simplest approach: add `skip_preflight: bool = False` to `ExecutionConfig` (it's a study-level execution concern, not per-experiment). If `ExecutionConfig` does not exist as a field on `ExperimentConfig`, add `skip_preflight` as a top-level field on `StudyConfig` with default `False`. The CLI flag overrides the config value (CLI wins).

**3. Wire into `run_study_preflight()` (`orchestration/preflight.py`):**
Modify `run_study_preflight(study: StudyConfig, skip_preflight: bool = False)`:
- After the existing multi-backend check, determine if any experiment in the study will use a Docker runner. Import `resolve_study_runners` from `infra.runner_resolution`. Call it with the unique backends from the study.
- If any `RunnerSpec.mode == "docker"`, call `run_docker_preflight(skip=skip_preflight)` from `infra.docker_preflight`.
- If no Docker runners, skip Docker pre-flight entirely.

Note: `run_study_preflight()` is called from `_run()` in `_api.py`. Update the call site in `_run()` to pass `skip_preflight` through. The cleanest way: accept `skip_preflight: bool = False` as a parameter to `_run()`, and thread it from `run_experiment()` and `run_study()`.

**4. Thread `skip_preflight` through API (`_api.py`):**
- `run_experiment(..., skip_preflight=False)` → `_run(study, skip_preflight=skip_preflight)`
- `run_study(..., skip_preflight=False)` → `_run(study, skip_preflight=skip_preflight)`
- `_run(study, skip_preflight=False)` → `run_study_preflight(study, skip_preflight=skip_preflight)`

For the study config YAML path: if `study.execution.skip_preflight` is True (from YAML), OR `skip_preflight` parameter is True (from CLI), the pre-flight is skipped. CLI overrides config: `effective_skip = skip_preflight or getattr(study.execution, 'skip_preflight', False)`.

**5. Add wiring tests to `tests/unit/test_docker_preflight.py`:**
- `TestWiring`: Test that `run_study_preflight` calls `run_docker_preflight` when a Docker runner is resolved. Mock `resolve_study_runners` to return a spec with `mode="docker"`. Verify `run_docker_preflight` is called.
- `TestWiringSkipped`: Test that `run_study_preflight` does NOT call `run_docker_preflight` when all runners are local.
- `TestWiringSkipFlag`: Test that passing `skip_preflight=True` results in `run_docker_preflight(skip=True)` being called.
- `TestCLIFlag`: Test that `--skip-preflight` is a valid CLI option (use `typer.testing.CliRunner` or just verify the parameter exists in the function signature).

**Important:** The `DockerPreFlightError` inherits from `PreFlightError`, which is already caught by the CLI error handler in `run.py` (line 128: `except (PreFlightError, ...) as e:`). No additional CLI error handling needed.

**Error handling precedence for `--skip-preflight`:**
- CLI `--skip-preflight` flag → highest priority, always wins
- YAML `execution.skip_preflight: true` → second priority
- Default → `False` (pre-flight runs)
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_docker_preflight.py tests/unit/test_preflight.py -x -v 2>&1 | tail -30</automated>
  </verify>
  <done>
    - `--skip-preflight` flag exists on `llem run` command
    - `execution.skip_preflight` field exists in config model (StudyConfig or ExecutionConfig)
    - `run_study_preflight()` calls `run_docker_preflight()` when Docker runners are resolved
    - `run_study_preflight()` skips Docker pre-flight when all runners are local
    - CLI flag overrides config YAML value
    - Existing pre-flight tests still pass
    - Wiring tests confirm correct call chain
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/unit/test_docker_preflight.py -x -v` — all Docker pre-flight tests pass
2. `python -m pytest tests/unit/test_preflight.py -x -v` — existing pre-flight tests unbroken
3. `python -m pytest tests/unit/ -x --timeout=60` — full unit suite passes
4. `python -c "from llenergymeasure.infra.docker_preflight import run_docker_preflight; print('import ok')"` — module importable
5. `python -c "from llenergymeasure.exceptions import DockerPreFlightError; print(DockerPreFlightError.__mro__)"` — inheritance correct
6. `python -m mypy src/llenergymeasure/infra/docker_preflight.py --ignore-missing-imports` — type-checks clean
</verification>

<success_criteria>
- DockerPreFlightError raised when Docker or NVIDIA Container Toolkit is missing (DOCK-07)
- Container-level GPU visibility probe validates GPUs are accessible inside Docker (DOCK-08)
- CUDA/driver mismatch produces descriptive error with version numbers (DOCK-09)
- --skip-preflight CLI flag and execution.skip_preflight config option both bypass checks
- All error messages include actionable fix suggestions with upstream NVIDIA doc links
- Tier 1 failures block tier 2 execution (no cascade of irrelevant errors)
- Missing host nvidia-smi warns but does not block (remote Docker daemon support)
- Silent on success — no output when all checks pass
- Full unit test coverage, all existing tests unbroken
</success_criteria>

<output>
After completion, create `.planning/phases/18-docker-pre-flight/18-01-SUMMARY.md`
</output>
