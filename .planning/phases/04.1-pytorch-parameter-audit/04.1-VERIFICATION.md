---
phase: 04.1-pytorch-parameter-audit
verified: 2026-02-26T19:17:46Z
status: passed
score: 5/5 must-haves verified
re_verification: false
---

# Phase 04.1: PyTorch Parameter Audit Verification Report

**Phase Goal:** Every tuneable PyTorch parameter that a researcher would reasonably want to control is exposed as an ExperimentConfig / PyTorchConfig field — no hidden knobs that require passthrough_kwargs for common use cases.
**Verified:** 2026-02-26T19:17:46Z
**Status:** passed
**Re-verification:** No — initial verification

---

## Goal Achievement

### Observable Truths

| # | Truth | Status | Evidence |
|---|-------|--------|----------|
| 1 | PyTorchConfig fields cover all parameters accepted by `AutoModelForCausalLM.from_pretrained()` that affect inference behaviour (torch_dtype, attn_implementation, device_map, quantisation flags) | VERIFIED | `backend_configs.py` has 20 fields covering attention (incl. flash_attention_3), BitsAndBytes (4 bnb fields), device_map, max_memory, revision, trust_remote_code, torch_compile + 2 sub-options. All None-as-default. |
| 2 | DecoderConfig fields cover all `model.generate()` parameters that affect output (temperature, top_p, top_k, repetition_penalty, do_sample, min_p, min_new_tokens, etc.) | VERIFIED | `models.py` has all existing fields plus `min_p: float | None` and `min_new_tokens: int | None`. Beam search, KV cache, n-gram, and speculative decoding live in PyTorchConfig and are wired to generate(). |
| 3 | Any v1.x parameters that were dropped have a documented rationale (intentional removal, not oversight) | VERIFIED | SUMMARY 01 documents: beam search in PyTorchConfig (not DecoderConfig) because cross-backend semantics unverified; draft model speculative decoding via passthrough_kwargs (deliberate). RESEARCH.md covers all decisions. |
| 4 | SSOT introspection (`config/introspection.py`) reflects the updated field set | VERIFIED | `get_backend_specific_params()` lists 22 pytorch fields. `get_backend_capabilities()` includes torch_compile, beam_search, speculative_decoding, static_kv_cache. `get_validation_rules()` has 8 rules (3 new). `DECODER_PARAM_SUPPORT["pytorch"]` includes min_p and min_new_tokens. |
| 5 | Cross-validators updated for any new field interactions | VERIFIED | 3 new `@model_validator(mode="after")` methods on PyTorchConfig: `validate_torch_compile_options`, `validate_bnb_4bit_options`, `validate_cache_options`. All raise `ValidationError` on violation. All verified by 51-test suite (51/51 passed). |

**Score:** 5/5 truths verified

---

### Required Artifacts

| Artifact | Expected | Status | Details |
|----------|----------|--------|---------|
| `src/llenergymeasure/config/backend_configs.py` | Complete PyTorchConfig with ~20 additional fields | VERIFIED | 280 lines. 20 fields with None-as-default. 4 cross-validators. `extra="forbid"` intact. |
| `src/llenergymeasure/config/models.py` | DecoderConfig with min_p and min_new_tokens | VERIFIED | 397 lines. `min_p: float | None = Field(ge=0.0, le=1.0)` and `min_new_tokens: int | None = Field(ge=1)` present after `repetition_penalty`. |
| `src/llenergymeasure/core/backends/pytorch.py` | Full parameter wiring from config to HuggingFace APIs | VERIFIED | 569 lines. `_model_load_kwargs()` uses `BitsAndBytesConfig`, config-driven device_map/trust_remote_code/revision/max_memory. `_build_generate_kwargs()` passes all beam search, KV cache, n-gram, speculative, min_p, min_new_tokens params. `_load_model()` applies torch.compile post-load. |
| `src/llenergymeasure/config/ssot.py` | Updated DECODER_PARAM_SUPPORT with min_p, min_new_tokens | VERIFIED | 41 lines. `DECODER_PARAM_SUPPORT["pytorch"]` = `["temperature", "top_k", "top_p", "repetition_penalty", "min_p", "min_new_tokens"]`. |
| `src/llenergymeasure/config/introspection.py` | Updated SSOT introspection for expanded PyTorchConfig | VERIFIED | 820 lines. All 6 targeted functions updated: `get_backend_specific_params()`, `get_mutual_exclusions()`, `get_param_skip_conditions()`, `get_params_requiring_gpu_capability()`, `get_validation_rules()`, `get_backend_capabilities()`, `get_capability_matrix_markdown()`. |
| `tests/unit/test_config_backend_configs.py` | v2.0 backend config tests (replaces stale v1.x tests) | VERIFIED | 51 tests, 51 passed (0.24s). Covers all 20 PyTorchConfig fields, all 4 cross-validators, DecoderConfig new fields, VLLMConfig, TensorRTConfig, ExperimentConfig integration. No v1.x imports. |

---

### Key Link Verification

| From | To | Via | Status | Details |
|------|----|-----|--------|---------|
| `backend_configs.py::PyTorchConfig` | `pydantic model_validator` | `validate_torch_compile_options`, `validate_bnb_4bit_options`, `validate_cache_options` | WIRED | 3 new validators confirmed raising `ValidationError` on invalid combos |
| `pytorch.py::_model_load_kwargs()` | `transformers.AutoModelForCausalLM.from_pretrained()` | `BitsAndBytesConfig`, device_map, revision, trust_remote_code, attn_implementation | WIRED | Direct test confirmed: raw `load_in_4bit` absent, `quantization_config` present; device_map/trust_remote_code read from config |
| `pytorch.py::_build_generate_kwargs()` | `model.generate()` | num_beams, cache_implementation, min_p, min_new_tokens, use_cache, no_repeat_ngram_size, prompt_lookup_num_tokens | WIRED | Direct test confirmed: all 9 new kwargs passed through; None fields produce no entries; greedy mode strips min_p |
| `pytorch.py::_load_model()` | `torch.compile()` | post-load compilation step | WIRED | Lines 124-134 confirmed: `if config.pytorch.torch_compile:` → `_torch.compile(model, mode=mode, backend=backend)` with non-fatal exception handling |
| `introspection.py::get_backend_specific_params()` | `backend_configs.py::PyTorchConfig` | listing all 22 field paths | WIRED | All 22 paths confirmed present, including all 14 new fields |

---

### Requirements Coverage

No formal requirement IDs were declared for this phase (quality audit with no REQUIREMENTS.md entries).

---

### Anti-Patterns Found

| File | Line | Pattern | Severity | Impact |
|------|------|---------|----------|--------|
| `pytorch.py` | 236, 241, 246, 251 | `M1 placeholder` in `_prepare_prompts()` | Info | Out-of-scope for phase 4.1 — dataset loading is Phase 5 scope. Not a blocker. |
| `pytorch.py` | 508, 535, 538, 539 | `Phase 5 placeholder` in energy fields | Info | Out-of-scope for phase 4.1 — energy measurement is Phase 5 scope. Not a blocker. |

No blockers or warnings. The placeholder comments are correctly scoped to Phase 5, which is explicitly the next phase. Phase 4.1 does not own `_prepare_prompts()` or energy field population.

---

### Human Verification Required

None. All success criteria are structurally verifiable:

- Field existence: confirmed by Pydantic model introspection
- Cross-validators: confirmed by ValidationError tests
- Backend wiring: confirmed by direct method invocation (no GPU required — `_model_load_kwargs()` and `_build_generate_kwargs()` are pure dict builders)
- Test suite: 51/51 passed

The only human-requiring concern would be whether the specific HuggingFace API semantics (e.g., `BitsAndBytesConfig` with `load_in_4bit=True` actually quantising at runtime) behave correctly — but that is a runtime GPU test concern, not a phase 4.1 structural concern. The wiring is correct.

---

### Gaps Summary

No gaps. Phase goal is fully achieved.

All researcher-useful PyTorch parameters are now first-class config fields. The explicit boundary decisions are documented: draft-model speculative decoding stays in `passthrough_kwargs` (not a common use case, no standard interface), and beam search fields are in `PyTorchConfig` rather than `DecoderConfig` pending cross-backend semantic verification in a future phase.

---

## Commit Evidence

| Commit | Description |
|--------|-------------|
| `fc0431e` | feat(config): expand PyTorchConfig with complete researcher parameter set |
| `108179b` | feat(config): add min_p and min_new_tokens to DecoderConfig |
| `e72a813` | feat(pytorch): wire _model_load_kwargs with BitsAndBytesConfig and loading fields |
| `7754d17` | feat(pytorch): wire _build_generate_kwargs with beam search, KV cache, and new decoder fields |
| `a850440` | feat(config): update SSOT metadata for expanded PyTorchConfig |
| `0352cd8` | test(config): rewrite backend config tests for v2.0 schema |

All 6 commits verified present in git log.

---

_Verified: 2026-02-26T19:17:46Z_
_Verifier: Claude (gsd-verifier)_
