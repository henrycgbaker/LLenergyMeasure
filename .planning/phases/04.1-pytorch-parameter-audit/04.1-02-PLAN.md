---
phase: 04.1-pytorch-parameter-audit
plan: 02
type: execute
wave: 2
depends_on: ["04.1-01"]
files_modified:
  - src/llenergymeasure/core/backends/pytorch.py
autonomous: true
requirements: []

must_haves:
  truths:
    - "_model_load_kwargs() builds BitsAndBytesConfig when any quantization field is set (not raw load_in_4bit/load_in_8bit)"
    - "_model_load_kwargs() passes device_map, revision, trust_remote_code from PyTorchConfig (not hardcoded)"
    - "torch.compile is applied post-load when torch_compile=True (not silently ignored)"
    - "_build_generate_kwargs() passes beam search, KV cache, speculative decoding, min_p, min_new_tokens, no_repeat_ngram_size from config"
    - "All new field wiring uses conditional 'is not None' checks — None fields produce no kwargs"
  artifacts:
    - path: "src/llenergymeasure/core/backends/pytorch.py"
      provides: "Full parameter wiring from config to HuggingFace APIs"
      contains: "BitsAndBytesConfig"
  key_links:
    - from: "src/llenergymeasure/core/backends/pytorch.py::_model_load_kwargs"
      to: "transformers.AutoModelForCausalLM.from_pretrained"
      via: "kwargs dict with BitsAndBytesConfig, device_map, revision, trust_remote_code, attn_implementation"
      pattern: "quantization_config.*BitsAndBytesConfig"
    - from: "src/llenergymeasure/core/backends/pytorch.py::_build_generate_kwargs"
      to: "model.generate()"
      via: "kwargs dict with beam search, cache, and sampling parameters"
      pattern: "num_beams|cache_implementation|min_p"
    - from: "src/llenergymeasure/core/backends/pytorch.py::_load_model"
      to: "torch.compile"
      via: "post-load compilation step"
      pattern: "torch\\.compile"
---

<objective>
Wire all new PyTorchConfig and DecoderConfig fields through the PyTorch backend so they actually affect model loading and inference.

Purpose: Without backend wiring, the new config fields are accepted but silently ignored. This plan ensures that setting `torch_compile: true` actually compiles the model, `bnb_4bit_compute_dtype: bfloat16` actually uses bfloat16 compute, and `device_map: "cpu"` actually places the model on CPU.

Output: Updated `pytorch.py` with complete parameter wiring in `_model_load_kwargs()`, `_build_generate_kwargs()`, and a new `_apply_torch_compile()` step in `_load_model()`.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/04.1-pytorch-parameter-audit/04.1-RESEARCH.md
@.planning/phases/04.1-pytorch-parameter-audit/04.1-01-SUMMARY.md

<interfaces>
<!-- Target PyTorchConfig after Plan 01 -->
From src/llenergymeasure/config/backend_configs.py (after Plan 01):

```python
class PyTorchConfig(BaseModel):
    model_config = {"extra": "forbid"}

    # Batching
    batch_size: int | None = Field(default=None, ge=1)

    # Attention
    attn_implementation: Literal["sdpa", "flash_attention_2", "flash_attention_3", "eager"] | None = None

    # Compilation
    torch_compile: bool | None = None
    torch_compile_mode: str | None = None
    torch_compile_backend: str | None = None

    # BitsAndBytes quantization
    load_in_4bit: bool | None = None
    load_in_8bit: bool | None = None
    bnb_4bit_compute_dtype: Literal["float16", "bfloat16", "float32"] | None = None
    bnb_4bit_quant_type: Literal["nf4", "fp4"] | None = None
    bnb_4bit_use_double_quant: bool | None = None

    # KV caching
    use_cache: bool | None = None
    cache_implementation: Literal["static", "offloaded_static", "sliding_window"] | None = None

    # Beam search
    num_beams: int | None = Field(default=None, ge=1)
    early_stopping: bool | None = None
    length_penalty: float | None = None

    # N-gram repetition
    no_repeat_ngram_size: int | None = Field(default=None, ge=0)

    # Speculative decoding
    prompt_lookup_num_tokens: int | None = Field(default=None, ge=1)

    # Model loading
    device_map: str | None = None
    max_memory: dict | None = None
    revision: str | None = None
    trust_remote_code: bool | None = None

    # Data parallelism
    num_processes: int | None = Field(default=None, ge=1)
```

From src/llenergymeasure/config/models.py (DecoderConfig after Plan 01):

```python
class DecoderConfig(BaseModel):
    temperature: float = Field(default=1.0, ge=0.0, le=2.0)
    do_sample: bool = Field(default=True)
    top_k: int = Field(default=50, ge=0)
    top_p: float = Field(default=1.0, ge=0.0, le=1.0)
    repetition_penalty: float = Field(default=1.0, ge=0.1, le=10.0)
    min_p: float | None = Field(default=None, ge=0.0, le=1.0)
    min_new_tokens: int | None = Field(default=None, ge=1)
    preset: Literal[...] | None = None
```

Current backend methods to modify:

```python
# _model_load_kwargs() currently builds:
kwargs = {
    "torch_dtype": self._precision_to_dtype(config.precision),
    "device_map": "auto",          # HARDCODED — must read from config
    "trust_remote_code": True,     # HARDCODED — must read from config
}
# Then: attn_implementation, load_in_4bit, load_in_8bit as raw kwargs

# _build_generate_kwargs() currently builds:
kwargs = {
    "do_sample": decoder.do_sample,
    "temperature": decoder.temperature,
    "top_k": decoder.top_k,
    "top_p": decoder.top_p,
    "repetition_penalty": decoder.repetition_penalty,
}
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire _model_load_kwargs() with BitsAndBytesConfig and new loading fields</name>
  <files>src/llenergymeasure/core/backends/pytorch.py</files>
  <action>
Rewrite `_model_load_kwargs()` to wire all new PyTorchConfig fields into the kwargs dict for `from_pretrained()`. The key changes:

**1. Replace hardcoded device_map and trust_remote_code with config-driven values:**
```python
kwargs: dict = {
    "torch_dtype": self._precision_to_dtype(config.precision),
}

# Device placement — default "auto" unless researcher overrides
pt = config.pytorch
if pt is not None and pt.device_map is not None:
    kwargs["device_map"] = pt.device_map
else:
    kwargs["device_map"] = "auto"

# Trust remote code — default True unless researcher overrides
if pt is not None and pt.trust_remote_code is not None:
    kwargs["trust_remote_code"] = pt.trust_remote_code
else:
    kwargs["trust_remote_code"] = True
```

**2. Replace raw load_in_4bit/load_in_8bit with BitsAndBytesConfig:**
```python
if pt is not None and (pt.load_in_4bit or pt.load_in_8bit):
    from transformers import BitsAndBytesConfig  # noqa: PLC0415
    bnb_kwargs: dict = {}
    if pt.load_in_4bit:
        bnb_kwargs["load_in_4bit"] = True
        if pt.bnb_4bit_compute_dtype is not None:
            import torch as _torch  # noqa: PLC0415
            _dtype_map = {"float16": _torch.float16, "bfloat16": _torch.bfloat16, "float32": _torch.float32}
            bnb_kwargs["bnb_4bit_compute_dtype"] = _dtype_map[pt.bnb_4bit_compute_dtype]
        if pt.bnb_4bit_quant_type is not None:
            bnb_kwargs["bnb_4bit_quant_type"] = pt.bnb_4bit_quant_type
        if pt.bnb_4bit_use_double_quant is not None:
            bnb_kwargs["bnb_4bit_use_double_quant"] = pt.bnb_4bit_use_double_quant
    if pt.load_in_8bit:
        bnb_kwargs["load_in_8bit"] = True
    kwargs["quantization_config"] = BitsAndBytesConfig(**bnb_kwargs)
```
Do NOT pass `load_in_4bit`/`load_in_8bit` directly as raw kwargs anymore — the legacy API.

**3. Wire remaining from_pretrained() fields:**
```python
if pt is not None:
    if pt.attn_implementation is not None:
        kwargs["attn_implementation"] = pt.attn_implementation
    if pt.revision is not None:
        kwargs["revision"] = pt.revision
    if pt.max_memory is not None:
        kwargs["max_memory"] = pt.max_memory
```

**4. Wire trust_remote_code into tokenizer loading too:**
In `_load_model()`, change `AutoTokenizer.from_pretrained(config.model, trust_remote_code=True)` to use the config value:
```python
trust = True
if config.pytorch is not None and config.pytorch.trust_remote_code is not None:
    trust = config.pytorch.trust_remote_code
tokenizer = AutoTokenizer.from_pretrained(config.model, trust_remote_code=trust)
```

**5. Add torch.compile post-load step in _load_model():**
After `model.eval()` and before `return model, tokenizer`, add:
```python
# Apply torch.compile if configured (must be AFTER from_pretrained + eval)
if config.pytorch is not None and config.pytorch.torch_compile:
    import torch as _torch  # noqa: PLC0415
    mode = config.pytorch.torch_compile_mode or "default"
    backend = config.pytorch.torch_compile_backend or "inductor"
    try:
        model = _torch.compile(model, mode=mode, backend=backend)
        logger.info("torch.compile applied (mode=%s, backend=%s)", mode, backend)
    except Exception as e:  # noqa: BLE001
        logger.warning("torch.compile failed (non-fatal, continuing without): %s", e)
```
This is non-fatal — if compilation fails (e.g., unsupported model architecture), inference continues without compilation.

**passthrough_kwargs remains last** — preserve the existing merge-last behaviour.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && /usr/bin/python3 -c "
from llenergymeasure.core.backends.pytorch import PyTorchBackend
from llenergymeasure.config.models import ExperimentConfig
from llenergymeasure.config.backend_configs import PyTorchConfig

backend = PyTorchBackend()

# Test 1: default device_map and trust_remote_code
config1 = ExperimentConfig(model='gpt2')
kwargs1 = backend._model_load_kwargs(config1)
assert kwargs1['device_map'] == 'auto', f'Expected auto, got {kwargs1[\"device_map\"]}'
assert kwargs1['trust_remote_code'] is True

# Test 2: config-driven device_map and trust_remote_code
config2 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig(device_map='cpu', trust_remote_code=False))
kwargs2 = backend._model_load_kwargs(config2)
assert kwargs2['device_map'] == 'cpu', f'Expected cpu, got {kwargs2[\"device_map\"]}'
assert kwargs2['trust_remote_code'] is False

# Test 3: BitsAndBytesConfig replaces raw load_in_4bit
config3 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4'))
kwargs3 = backend._model_load_kwargs(config3)
assert 'quantization_config' in kwargs3, 'Should have quantization_config'
assert 'load_in_4bit' not in kwargs3, 'Should NOT have raw load_in_4bit'
assert 'load_in_8bit' not in kwargs3, 'Should NOT have raw load_in_8bit'

# Test 4: revision and max_memory passed through
config4 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig(revision='abc123', max_memory={0: '10GiB'}))
kwargs4 = backend._model_load_kwargs(config4)
assert kwargs4.get('revision') == 'abc123'
assert kwargs4.get('max_memory') == {0: '10GiB'}

# Test 5: None fields produce no kwargs
config5 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig())
kwargs5 = backend._model_load_kwargs(config5)
assert 'revision' not in kwargs5
assert 'max_memory' not in kwargs5
assert 'quantization_config' not in kwargs5
assert 'attn_implementation' not in kwargs5

print('All _model_load_kwargs checks passed')
"
    </automated>
  </verify>
  <done>
    _model_load_kwargs() builds BitsAndBytesConfig for quantization (not raw kwargs), reads device_map/trust_remote_code/revision/max_memory from config (not hardcoded), and torch.compile is applied post-load. None fields produce no kwargs entries.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire _build_generate_kwargs() with beam search, KV cache, and new DecoderConfig fields</name>
  <files>src/llenergymeasure/core/backends/pytorch.py</files>
  <action>
Expand `_build_generate_kwargs()` to pass all new parameters from PyTorchConfig and DecoderConfig into the `model.generate()` call. The method currently only passes decoder params.

**Add after the existing decoder kwargs block (before the greedy decoding guard):**

```python
# DecoderConfig new fields
if decoder.min_p is not None:
    kwargs["min_p"] = decoder.min_p
if decoder.min_new_tokens is not None:
    kwargs["min_new_tokens"] = decoder.min_new_tokens
```

**Add PyTorchConfig generate() fields after the decoder section:**

```python
# PyTorchConfig generate() fields
pt = config.pytorch
if pt is not None:
    # KV cache
    if pt.use_cache is not None:
        kwargs["use_cache"] = pt.use_cache
    if pt.cache_implementation is not None:
        kwargs["cache_implementation"] = pt.cache_implementation

    # Beam search
    if pt.num_beams is not None:
        kwargs["num_beams"] = pt.num_beams
    if pt.early_stopping is not None:
        kwargs["early_stopping"] = pt.early_stopping
    if pt.length_penalty is not None:
        kwargs["length_penalty"] = pt.length_penalty

    # N-gram repetition
    if pt.no_repeat_ngram_size is not None:
        kwargs["no_repeat_ngram_size"] = pt.no_repeat_ngram_size

    # Speculative decoding (prompt lookup)
    if pt.prompt_lookup_num_tokens is not None:
        kwargs["prompt_lookup_num_tokens"] = pt.prompt_lookup_num_tokens
```

**Greedy decoding guard update**: The existing guard strips temperature/top_k/top_p when greedy. Also strip `min_p` in greedy mode (it's a sampling parameter):
```python
if decoder.temperature == 0.0 or not decoder.do_sample:
    kwargs["do_sample"] = False
    kwargs.pop("temperature", None)
    kwargs.pop("top_k", None)
    kwargs.pop("top_p", None)
    kwargs.pop("min_p", None)
```

Do NOT modify the measurement loop (`_run_measurement`, `_run_batch`) or result assembly — those are unchanged. The `generate_kwargs` dict flows naturally into `model.generate(**inputs, max_new_tokens=..., **generate_kwargs)`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && /usr/bin/python3 -c "
from llenergymeasure.core.backends.pytorch import PyTorchBackend
from llenergymeasure.config.models import ExperimentConfig, DecoderConfig
from llenergymeasure.config.backend_configs import PyTorchConfig

backend = PyTorchBackend()

# Test 1: min_p and min_new_tokens passed
config1 = ExperimentConfig(model='gpt2', decoder=DecoderConfig(min_p=0.1, min_new_tokens=10))
kwargs1 = backend._build_generate_kwargs(config1)
assert kwargs1.get('min_p') == 0.1
assert kwargs1.get('min_new_tokens') == 10

# Test 2: beam search fields passed
config2 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig(num_beams=4, early_stopping=True, length_penalty=1.5))
kwargs2 = backend._build_generate_kwargs(config2)
assert kwargs2.get('num_beams') == 4
assert kwargs2.get('early_stopping') is True
assert kwargs2.get('length_penalty') == 1.5

# Test 3: KV cache fields passed
config3 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig(use_cache=True, cache_implementation='static'))
kwargs3 = backend._build_generate_kwargs(config3)
assert kwargs3.get('use_cache') is True
assert kwargs3.get('cache_implementation') == 'static'

# Test 4: prompt_lookup_num_tokens and no_repeat_ngram_size
config4 = ExperimentConfig(model='gpt2', pytorch=PyTorchConfig(prompt_lookup_num_tokens=3, no_repeat_ngram_size=2))
kwargs4 = backend._build_generate_kwargs(config4)
assert kwargs4.get('prompt_lookup_num_tokens') == 3
assert kwargs4.get('no_repeat_ngram_size') == 2

# Test 5: None fields produce no kwargs
config5 = ExperimentConfig(model='gpt2')
kwargs5 = backend._build_generate_kwargs(config5)
assert 'min_p' not in kwargs5
assert 'min_new_tokens' not in kwargs5
assert 'num_beams' not in kwargs5
assert 'cache_implementation' not in kwargs5

# Test 6: greedy mode strips min_p
config6 = ExperimentConfig(model='gpt2', decoder=DecoderConfig(temperature=0.0, min_p=0.1))
kwargs6 = backend._build_generate_kwargs(config6)
assert 'min_p' not in kwargs6
assert kwargs6.get('do_sample') is False

print('All _build_generate_kwargs checks passed')
"
    </automated>
  </verify>
  <done>
    _build_generate_kwargs() passes all new DecoderConfig fields (min_p, min_new_tokens) and PyTorchConfig generate() fields (beam search, KV cache, n-gram, speculative decoding). Greedy mode strips sampling params including min_p. None fields produce no kwargs.
  </done>
</task>

</tasks>

<verification>
1. `_model_load_kwargs()` returns `quantization_config=BitsAndBytesConfig(...)` when load_in_4bit=True (no raw load_in_4bit kwarg)
2. `_model_load_kwargs()` returns device_map from config (not hardcoded "auto")
3. `_model_load_kwargs()` returns trust_remote_code from config (not hardcoded True)
4. `_model_load_kwargs()` passes revision, max_memory, attn_implementation when set
5. `_load_model()` calls `torch.compile()` when torch_compile=True
6. `_build_generate_kwargs()` includes min_p, min_new_tokens, num_beams, use_cache, cache_implementation, no_repeat_ngram_size, prompt_lookup_num_tokens
7. None-valued fields produce no kwargs entries
8. Greedy mode strips min_p along with temperature/top_k/top_p
</verification>

<success_criteria>
Every new config field is wired to the HuggingFace API it controls. Setting a field in YAML/Python config actually changes model loading or inference behaviour. No field is silently ignored.
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-pytorch-parameter-audit/04.1-02-SUMMARY.md`
</output>
