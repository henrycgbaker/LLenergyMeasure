---
phase: 04.1-pytorch-parameter-audit
plan: 01
subsystem: config
tags: [pytorch, config, schema, pydantic]
dependency_graph:
  requires: []
  provides: [complete-pytorch-config, decoder-config-min-p]
  affects: [core/backends/pytorch.py]
tech_stack:
  added: []
  patterns: [None-as-default, pydantic-model-validators, cross-field-validation]
key_files:
  created: []
  modified:
    - src/llenergymeasure/config/backend_configs.py
    - src/llenergymeasure/config/models.py
decisions:
  - "torch_compile_mode/torch_compile_backend use str|None (not Literal) — PyTorch may add modes in future versions"
  - "num_beams and beam search fields go in PyTorchConfig (not DecoderConfig) — cross-backend semantics unverified until M3"
  - "min_p and min_new_tokens go in DecoderConfig — universal: both PyTorch and vLLM support them"
metrics:
  duration: ~8 min
  completed_date: "2026-02-26"
  tasks_completed: 2
  files_modified: 2
---

# Phase 4.1 Plan 01: PyTorch Parameter Audit - Config Expansion Summary

**One-liner:** PyTorchConfig expanded from 6 to 20 fields covering all researcher-useful from_pretrained() and generate() parameters; DecoderConfig gains min_p and min_new_tokens.

## What Was Built

Closed the gap between the M1 minimal PyTorchConfig schema (6 fields) and the complete researcher-useful parameter set identified in the Phase 4.1 audit. All new fields follow the None-as-default pattern. Three new cross-validators enforce field dependencies.

### Task 1 — Expand PyTorchConfig fields and cross-validators

**Commit:** `fc0431e`
**File:** `src/llenergymeasure/config/backend_configs.py`

New fields added to `PyTorchConfig` (all `None`-defaulted):

| Group | Fields Added |
|-------|-------------|
| Attention | `attn_implementation` updated: added `"flash_attention_3"` to Literal |
| Compilation | `torch_compile_mode` (`str\|None`), `torch_compile_backend` (`str\|None`) |
| BitsAndBytes | `bnb_4bit_compute_dtype`, `bnb_4bit_quant_type`, `bnb_4bit_use_double_quant` |
| KV caching | `use_cache`, `cache_implementation` |
| Beam search | `num_beams`, `early_stopping`, `length_penalty` |
| N-gram repetition | `no_repeat_ngram_size` |
| Speculative decoding | `prompt_lookup_num_tokens` |
| Model loading | `device_map`, `max_memory`, `revision`, `trust_remote_code` |

New cross-validators:
- `validate_torch_compile_options` — `torch_compile_mode`/`torch_compile_backend` require `torch_compile=True`
- `validate_bnb_4bit_options` — `bnb_4bit_*` fields require `load_in_4bit=True`
- `validate_cache_options` — `cache_implementation` with `use_cache=False` is a contradiction

### Task 2 — Add min_p and min_new_tokens to DecoderConfig

**Commit:** `108179b`
**File:** `src/llenergymeasure/config/models.py`

Added to `DecoderConfig` (after `repetition_penalty`, before `preset`):
- `min_p: float | None` — min probability filter, range `[0.0, 1.0]`
- `min_new_tokens: int | None` — minimum output token count, `ge=1`

Both use `None`-as-default. Preset expansion and `extra="forbid"` unchanged.

## Decisions Made

1. **`torch_compile_mode` uses `str | None`** — PyTorch adds new compile modes across versions (`"max-autotune-no-cudagraphs"` already exists). Literal would require frequent updates. Open string with validator on `torch_compile=True` dependency is the right trade-off.

2. **Beam search fields in `PyTorchConfig`** (not `DecoderConfig`) — vLLM and TensorRT-LLM both support beam search but with subtly different semantics (e.g., TRT-LLM's `num_beams` is a compile-time constant). Moving to `DecoderConfig` requires cross-backend semantic verification, deferred to M3.

3. **`min_p` and `min_new_tokens` in `DecoderConfig`** — Both PyTorch and vLLM support these with identical semantics. Universal placement avoids duplication.

## Deviations from Plan

None — plan executed exactly as written.

## Verification Results

All 10 plan verification checks passed:

1. `PyTorchConfig()` creates with all fields `None`
2. `PyTorchConfig(attn_implementation="flash_attention_3")` is valid
3. `PyTorchConfig(torch_compile_mode="reduce-overhead")` raises `ValidationError`
4. `PyTorchConfig(bnb_4bit_compute_dtype="bfloat16")` raises `ValidationError`
5. `PyTorchConfig(use_cache=False, cache_implementation="static")` raises `ValidationError`
6. `PyTorchConfig(load_in_4bit=True, bnb_4bit_compute_dtype="bfloat16")` is valid
7. `PyTorchConfig(torch_compile=True, torch_compile_mode="max-autotune")` is valid
8. `DecoderConfig(min_p=0.1, min_new_tokens=10)` is valid
9. `ExperimentConfig(model="gpt2", pytorch=PyTorchConfig(revision="abc123"))` is valid
10. `ExperimentConfig(model="gpt2", pytorch=PyTorchConfig(nonexistent=True))` raises `ValidationError`

## Self-Check: PASSED

Files modified exist:
- `src/llenergymeasure/config/backend_configs.py` — FOUND
- `src/llenergymeasure/config/models.py` — FOUND

Commits exist:
- `fc0431e` — FOUND (feat: expand PyTorchConfig)
- `108179b` — FOUND (feat: add min_p and min_new_tokens)
