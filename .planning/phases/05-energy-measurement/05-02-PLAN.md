---
phase: 05-energy-measurement
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/core/warmup.py
  - src/llenergymeasure/core/flops.py
  - src/llenergymeasure/domain/metrics.py
  - src/llenergymeasure/domain/experiment.py
  - tests/unit/test_warmup_v2.py
  - tests/unit/test_flops_v2.py
autonomous: true
requirements: [CM-21, CM-22, CM-23, CM-24, CM-26, CM-27, CM-28]

must_haves:
  truths:
    - "WarmupConfig.n_warmup defaults to 5 (not 3)"
    - "WarmupConfig has convergence_detection (bool, default False), cv_threshold, max_warmup_prompts, window_size, min_prompts fields"
    - "WarmupResult has 7 fields: converged, final_cv, iterations_completed, target_cv, max_prompts, latencies_ms, thermal_floor_wait_s"
    - "warmup_until_converged() returns WarmupResult with all 7 fields populated (thermal_floor_wait_s defaults to 0.0, set by caller after sleep)"
    - "thermal_floor_seconds enforces ge=30.0 minimum"
    - "FlopsResult.method Literal includes 'palm_formula'"
    - "estimate_flops_palm() implements 2 * N_non_embedding_params * total_tokens with separate prefill/decode"
    - "_count_non_embedding_params() excludes embedding layers"
    - "Warmup tokens are excluded from FLOPs calculation by API design (caller passes measurement-only token counts)"
    - "ExperimentConfig has energy.backend field accepting 'auto' | 'nvml' | 'zeus' | 'codecarbon' | None"
    - "ExperimentResult has measurement_warnings: list[str] field"
  artifacts:
    - path: "src/llenergymeasure/config/models.py"
      provides: "Updated WarmupConfig with CV fields and n_warmup=5, plus EnergyConfig"
      contains: "convergence_detection"
    - path: "src/llenergymeasure/config/models.py"
      provides: "EnergyConfig sub-model on ExperimentConfig"
      contains: "class EnergyConfig"
    - path: "src/llenergymeasure/domain/experiment.py"
      provides: "measurement_warnings field on ExperimentResult"
      contains: "measurement_warnings"
    - path: "src/llenergymeasure/core/flops.py"
      provides: "PaLM FLOPs formula as primary estimation method"
      contains: "def estimate_flops_palm"
    - path: "src/llenergymeasure/domain/metrics.py"
      provides: "Updated FlopsResult with palm_formula method"
      contains: "palm_formula"
    - path: "tests/unit/test_warmup_v2.py"
      provides: "Unit tests for WarmupConfig v2 defaults and CV fields"
    - path: "tests/unit/test_flops_v2.py"
      provides: "Unit tests for PaLM FLOPs formula and non-embedding param counting"
  key_links:
    - from: "src/llenergymeasure/core/flops.py"
      to: "src/llenergymeasure/domain/metrics.py"
      via: "FlopsResult import"
      pattern: "from llenergymeasure.domain.metrics import FlopsResult"
    - from: "src/llenergymeasure/core/warmup.py"
      to: "src/llenergymeasure/config/models.py"
      via: "WarmupConfig import"
      pattern: "from llenergymeasure.config.models import WarmupConfig"
---

<objective>
Update WarmupConfig to v2.0 defaults (n_warmup=5, CV opt-in fields), add EnergyConfig to ExperimentConfig, add measurement_warnings to ExperimentResult, and rewrite the FLOPs estimator to use the PaLM formula as primary method. These are independent of energy backends and can run in parallel with Plan 01.

Purpose: Warmup config changes and EnergyConfig are needed before the measurement integration (Plan 03) can wire warmup and energy properly. The PaLM FLOPs formula is a locked design decision that replaces the old calflops fallback chain. This plan owns all config/models.py and domain/experiment.py modifications to avoid file ownership conflicts with Plan 01.
Output: Updated WarmupConfig, EnergyConfig, measurement_warnings, rewritten flops.py with PaLM formula, updated FlopsResult type, unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-energy-measurement/05-CONTEXT.md
@.planning/phases/05-energy-measurement/05-RESEARCH.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/config/models.py — WarmupConfig (CURRENT, needs updating):
```python
class WarmupConfig(BaseModel):
    model_config = {"extra": "forbid"}
    n_warmup: int = Field(default=3, ge=1, description="...")
    thermal_floor_seconds: float = Field(default=60.0, ge=0.0, description="...")
```

From src/llenergymeasure/domain/metrics.py — FlopsResult (CURRENT, needs updating):
```python
class FlopsResult(BaseModel):
    value: float = Field(..., description="Estimated FLOPs count")
    method: Literal["calflops", "architecture", "parameter_estimate"] = Field(...)
    confidence: Literal["high", "medium", "low"] = Field(...)
    precision: str = Field(...)
    notes: str | None = Field(default=None)
```

From src/llenergymeasure/domain/metrics.py — WarmupResult:
```python
class WarmupResult(BaseModel):
    converged: bool = Field(...)
    final_cv: float = Field(...)
    iterations_completed: int = Field(...)
    target_cv: float = Field(...)
    max_prompts: int = Field(...)
    latencies_ms: list[float] = Field(default_factory=list)
    # MISSING: thermal_floor_wait_s — must add in Task 1
```

From src/llenergymeasure/core/warmup.py:
```python
def warmup_until_converged(
    run_single_inference: Callable[[], float],
    config: WarmupConfig,
    *, show_progress: bool = True,
) -> WarmupResult: ...
```

From src/llenergymeasure/core/flops.py:
```python
class FlopsEstimator:
    def estimate(self, model, input_ids, config=None) -> FlopsResult: ...
def estimate_flops(model, input_ids, config=None) -> FlopsResult: ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update WarmupConfig and FlopsResult, rewrite flops.py with PaLM formula</name>
  <files>
    src/llenergymeasure/config/models.py
    src/llenergymeasure/domain/metrics.py
    src/llenergymeasure/core/flops.py
    src/llenergymeasure/core/warmup.py
  </files>
  <action>
  **Update `src/llenergymeasure/config/models.py` — EnergyConfig (NEW):**

  Add `EnergyConfig` sub-model between `BaselineConfig` and `SyntheticDatasetConfig`:

  ```python
  class EnergyConfig(BaseModel):
      """Energy measurement backend configuration."""
      model_config = {"extra": "forbid"}

      backend: Literal["auto", "nvml", "zeus", "codecarbon"] | None = Field(
          default="auto",
          description="Energy measurement backend. None (YAML null) disables energy measurement."
      )
  ```

  Note on Pydantic + YAML `null`: `backend: null` in YAML becomes `backend: None` in Python.

  Add `energy: EnergyConfig` field to `ExperimentConfig`, positioned after `baseline:`:
  ```python
  energy: EnergyConfig = Field(
      default_factory=EnergyConfig, description="Energy measurement backend configuration"
  )
  ```

  **Update `src/llenergymeasure/domain/experiment.py` — measurement_warnings (NEW):**

  Add `measurement_warnings: list[str]` field to `ExperimentResult`, positioned before `model_config = {"frozen": True}`:
  ```python
  measurement_warnings: list[str] = Field(
      default_factory=list,
      description="Measurement quality warnings (e.g., short duration, thermal drift)",
  )
  ```

  **Update `src/llenergymeasure/config/models.py` — WarmupConfig:**

  Change `WarmupConfig` to add CV convergence fields and fix defaults:

  1. Change `n_warmup` default from `3` to `5` (CM-21, HIGH confidence per DeepSpeed/Zeus).
  2. Change `thermal_floor_seconds` constraint from `ge=0.0` to `ge=30.0` (CM-22, 30s minimum enforced). Keep default 60.0.
  3. Add the following fields for CV convergence (CM-23):
     - `convergence_detection: bool = Field(default=False, description="Enable CV-based convergence detection (additive to n_warmup)")` — opt-in
     - `cv_threshold: float = Field(default=0.05, ge=0.01, le=0.5, description="CV target for convergence")` — only used when convergence_detection is True
     - `max_warmup_prompts: int = Field(default=20, ge=5, description="Maximum warmup prompts when CV mode is on")` — safety cap
     - `window_size: int = Field(default=5, ge=3, description="Window size for CV calculation")` — rolling window
     - `min_prompts: int = Field(default=5, ge=1, description="Minimum prompts before checking convergence")` — warm start
  4. Add `enabled: bool = Field(default=True, description="Enable warmup phase")` for completeness (warmup.py already checks `config.enabled`).
  5. Add a code comment documenting confidence levels:
     ```python
     # Confidence: n_warmup=5 HIGH (DeepSpeed 5-10, Zeus 10, AIEnergyScore 10)
     # Confidence: thermal_floor_seconds=60 HIGH (MLPerf Power mandates 60s minimum)
     ```

  **Update `src/llenergymeasure/domain/metrics.py` — WarmupResult:**

  Add `thermal_floor_wait_s` field to WarmupResult (CONTEXT.md locked decision: "WarmupResult captures everything: warmup run count, per-run latencies, total warmup duration, AND thermal floor wait duration"):
  ```python
  thermal_floor_wait_s: float = Field(
      default=0.0, ge=0.0,
      description="Seconds spent in thermal floor wait after warmup. Set by caller, not by warmup_until_converged()."
  )
  ```
  This brings WarmupResult to 7 fields. The field defaults to 0.0 and is populated by the caller (PyTorchBackend.run() in Plan 03) after the `time.sleep()` thermal floor wait.

  **Update `src/llenergymeasure/domain/metrics.py` — FlopsResult:**

  Add `"palm_formula"` to the `method` Literal type:
  ```python
  method: Literal["calflops", "architecture", "parameter_estimate", "palm_formula"] = Field(...)
  ```

  **Rewrite `src/llenergymeasure/core/flops.py`:**

  Replace the existing FlopsEstimator class with PaLM formula as the PRIMARY method while keeping the old methods as fallbacks (for backward compat and for cases when model object is not available).

  Key changes:
  1. Remove module-level `import torch` (CRITICAL: deferred imports only, no module-level torch).
  2. Remove module-level `from loguru import logger` — use `import logging; logger = logging.getLogger(__name__)` (base package uses stdlib logging).
  3. Add new primary function `estimate_flops_palm()`:

  ```python
  def estimate_flops_palm(
      model: Any,
      n_input_tokens: int,
      n_output_tokens: int,
      batch_size: int = 1,
  ) -> FlopsResult:
      """PaLM/Chinchilla inference FLOPs estimate (v2.0 primary method).

      Formula: FLOPs = 2 * N_non_embedding_params * total_tokens
      Split: prefill (input) + decode (output).
      Caller must exclude warmup tokens from n_input_tokens/n_output_tokens (CM-28).
      """
      n_params = _count_non_embedding_params(model)
      flops_prefill = 2 * n_params * batch_size * n_input_tokens
      flops_decode = 2 * n_params * batch_size * n_output_tokens
      flops_total = flops_prefill + flops_decode

      return FlopsResult(
          value=float(flops_total),
          method="palm_formula",
          confidence="high",
          precision="n/a",  # precision doesn't affect FLOPs (forward pass)
          notes=(
              f"PaLM formula: 2x{n_params:,}x({n_input_tokens}+{n_output_tokens}) tokens. "
              f"Attention FLOPs omitted (v2.0 limitation, significant only for seq_len>=2048)."
          ),
      )
  ```

  4. Add `_count_non_embedding_params()`:
  ```python
  def _count_non_embedding_params(model: Any) -> int:
      """Count non-embedding parameters. Embeddings are memory-bound lookups, not MAC ops."""
      total = 0
      for name, param in model.named_parameters():
          if "embed" not in name.lower():
              total += param.numel()
      return total
  ```

  5. Keep the existing `FlopsEstimator` class but update its docstring to note it is the LEGACY fallback. The `estimate_flops_palm()` function is the new v2.0 primary API.

  6. Keep the existing `estimate_flops()` convenience function as a backward-compat wrapper, but add `estimate_flops_palm` to `__all__` or module-level exports.

  **Update `src/llenergymeasure/core/warmup.py`:**

  The existing `warmup_until_converged()` already handles both fixed and CV modes, and references `config.enabled`, `config.convergence_detection`, `config.cv_threshold`, `config.max_prompts`, `config.min_prompts`, `config.window_size`. After WarmupConfig is updated, these references will resolve correctly.

  One required change: replace `from loguru import logger` with `import logging; logger = logging.getLogger(__name__)` (base package uses stdlib logging, loguru is not a base dependency).

  Also update the iteration range from `config.max_prompts` to `config.max_warmup_prompts` if the field name differs (check the existing code — the warmup.py currently uses `config.max_prompts` which should match the WarmupConfig field name. Use `max_warmup_prompts` in the config, and update warmup.py to match).

  IMPORTANT: Ensure the field names in WarmupConfig match what warmup.py references:
  - warmup.py uses `config.max_prompts` — rename field to `max_prompts` (not `max_warmup_prompts`) for consistency, OR update warmup.py. Choose the cleaner option: keep `max_warmup_prompts` in config (clearer for YAML) and update warmup.py references.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.config.models import WarmupConfig
w = WarmupConfig()
assert w.n_warmup == 5, f'Expected 5, got {w.n_warmup}'
assert w.thermal_floor_seconds == 60.0
assert w.convergence_detection == False
assert w.cv_threshold == 0.05
print('WarmupConfig OK')
" && python -c "
from llenergymeasure.domain.metrics import FlopsResult
r = FlopsResult(value=1e12, method='palm_formula', confidence='high', precision='n/a')
assert r.method == 'palm_formula'
print('FlopsResult OK')
" && python -c "
from llenergymeasure.core.flops import estimate_flops_palm
print('PaLM FLOPs import OK')
"</automated>
  </verify>
  <done>WarmupConfig defaults to n_warmup=5, has CV convergence fields (opt-in), thermal floor enforces 30s minimum. FlopsResult accepts "palm_formula" method. estimate_flops_palm() implements PaLM formula with non-embedding param counting. warmup.py uses stdlib logging.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for warmup config and PaLM FLOPs</name>
  <files>
    tests/unit/test_warmup_v2.py
    tests/unit/test_flops_v2.py
  </files>
  <action>
  **Create `tests/unit/test_warmup_v2.py`:**

  Write unit tests (no GPU required):

  1. `test_warmup_config_n_warmup_default_is_5()` — `WarmupConfig().n_warmup == 5`
  2. `test_warmup_config_thermal_floor_default_60()` — `WarmupConfig().thermal_floor_seconds == 60.0`
  3. `test_warmup_config_thermal_floor_minimum_30()` — `WarmupConfig(thermal_floor_seconds=29.0)` raises `ValidationError`
  4. `test_warmup_config_thermal_floor_30_ok()` — `WarmupConfig(thermal_floor_seconds=30.0)` succeeds
  5. `test_warmup_config_convergence_detection_default_false()` — `WarmupConfig().convergence_detection == False`
  6. `test_warmup_config_cv_threshold_default()` — `WarmupConfig().cv_threshold == 0.05`
  7. `test_warmup_config_cv_threshold_bounds()` — values outside [0.01, 0.5] raise ValidationError
  8. `test_warmup_config_extra_forbid()` — `WarmupConfig(unknown_field=1)` raises ValidationError
  9. `test_warmup_config_enabled_default_true()` — `WarmupConfig().enabled == True`
  10. `test_warmup_fixed_mode_returns_converged()` — call `warmup_until_converged()` with a mock inference fn and `convergence_detection=False`, assert result.converged is True and iterations_completed equals n_warmup
  11. `test_warmup_cv_mode_converges()` — mock inference fn returning stable latencies (all 10ms), assert converges within max_warmup_prompts
  12. `test_warmup_result_thermal_floor_wait_default()` — `WarmupResult(...).thermal_floor_wait_s == 0.0` (defaults to 0.0, set by caller)
  13. `test_warmup_result_thermal_floor_wait_settable()` — construct WarmupResult with `thermal_floor_wait_s=60.5`, assert field round-trips
  14. `test_energy_config_default()` — `EnergyConfig().backend == "auto"`
  13. `test_energy_config_null()` — `EnergyConfig(backend=None).backend is None`
  14. `test_experiment_config_has_energy()` — `ExperimentConfig(model="gpt2").energy.backend == "auto"`
  15. `test_experiment_result_has_measurement_warnings()` — verify field exists and defaults to empty list

  **Create `tests/unit/test_flops_v2.py`:**

  Write unit tests (no GPU required, mock model):

  1. `test_estimate_flops_palm_basic()` — create mock model with `named_parameters()` returning known params (no "embed" in names), assert `estimate_flops_palm(model, 100, 50)` returns `value == 2 * param_count * (100 + 50)`
  2. `test_estimate_flops_palm_excludes_embeddings()` — mock model with both embed and non-embed params, assert only non-embed params counted
  3. `test_estimate_flops_palm_method_is_palm()` — assert result.method == "palm_formula"
  4. `test_estimate_flops_palm_confidence_is_high()` — assert result.confidence == "high"
  5. `test_count_non_embedding_params()` — directly test `_count_non_embedding_params()` with mock
  6. `test_estimate_flops_palm_batch_size()` — verify batch_size multiplier: `flops == 2 * params * batch_size * total_tokens`
  7. `test_flops_result_palm_formula_literal()` — `FlopsResult(value=1.0, method="palm_formula", confidence="high", precision="n/a")` validates
  8. `test_legacy_estimate_flops_still_works()` — import `estimate_flops` from `core.flops`, verify it still exists (backward compat)

  For mock model: create a class with `named_parameters()` returning a list of `(name, param)` tuples where `param.numel()` returns a known count. Use simple objects, not real torch tensors.

  ```python
  class MockParam:
      def __init__(self, numel_val): self._numel = numel_val
      def numel(self): return self._numel

  class MockModel:
      def __init__(self, params):
          self._params = params  # list of (name, MockParam)
      def named_parameters(self):
          return iter(self._params)
  ```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_warmup_v2.py tests/unit/test_flops_v2.py -x -v 2>&1 | tail -25</automated>
  </verify>
  <done>13+ warmup tests pass confirming n_warmup=5 default, CV opt-in fields, thermal floor 30s minimum, thermal_floor_wait_s field, extra=forbid. 8+ FLOPs tests pass confirming PaLM formula, embedding exclusion, batch size multiplier, backward compat.</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.config.models import WarmupConfig; assert WarmupConfig().n_warmup == 5"` — CM-21
2. `python -c "from llenergymeasure.config.models import WarmupConfig; assert WarmupConfig().thermal_floor_seconds == 60.0"` — CM-22
3. `python -c "from llenergymeasure.config.models import WarmupConfig; assert WarmupConfig().convergence_detection == False"` — CM-23
4. `python -c "from llenergymeasure.domain.metrics import FlopsResult; FlopsResult(value=1.0, method='palm_formula', confidence='high', precision='n/a')"` — CM-27
5. `python -c "from llenergymeasure.core.flops import estimate_flops_palm"` — CM-26
6. `python -c "from llenergymeasure.config.models import ExperimentConfig; c = ExperimentConfig(model='gpt2'); assert c.energy.backend == 'auto'"` — EnergyConfig wired
7. `python -m pytest tests/unit/test_warmup_v2.py tests/unit/test_flops_v2.py -x -v` — all tests pass
</verification>

<success_criteria>
- WarmupConfig.n_warmup defaults to 5
- WarmupConfig has convergence_detection, cv_threshold, max_warmup_prompts, window_size, min_prompts fields
- thermal_floor_seconds enforces ge=30.0
- FlopsResult.method accepts "palm_formula"
- estimate_flops_palm() computes 2 * non_embedding_params * tokens
- warmup.py uses stdlib logging (no loguru)
- ExperimentConfig.energy.backend defaults to "auto"
- ExperimentResult.measurement_warnings defaults to empty list
- 25+ unit tests pass across both test files
</success_criteria>

<output>
After completion, create `.planning/phases/05-energy-measurement/05-02-SUMMARY.md`
</output>
