---
phase: 17-docker-runner-infrastructure
plan: "03"
type: execute
wave: 2
depends_on: ["17-01", "17-02"]
files_modified:
  - src/llenergymeasure/infra/docker_runner.py
  - tests/unit/test_docker_runner.py
autonomous: true
requirements: [DOCK-01, DOCK-04]

must_haves:
  truths:
    - "DockerRunner dispatches an experiment to an ephemeral Docker container via subprocess.run"
    - "Container receives config via mounted JSON file with LLEM_CONFIG_PATH env var"
    - "Container writes result to shared volume; DockerRunner reads it after process exit"
    - "Container is always ephemeral (docker run --rm) with --gpus all"
    - "Temp directory is cleaned on success, preserved on failure with debug path logged"
  artifacts:
    - path: "src/llenergymeasure/infra/docker_runner.py"
      provides: "DockerRunner class that dispatches experiments to Docker containers"
      contains: "DockerRunner"
    - path: "tests/unit/test_docker_runner.py"
      provides: "Unit tests for Docker dispatch lifecycle"
      contains: "test_"
  key_links:
    - from: "src/llenergymeasure/infra/docker_runner.py"
      to: "src/llenergymeasure/infra/docker_errors.py"
      via: "translate_docker_error on container failure"
      pattern: "translate_docker_error"
    - from: "src/llenergymeasure/infra/docker_runner.py"
      to: "src/llenergymeasure/infra/image_registry.py"
      via: "get_default_image for image resolution"
      pattern: "get_default_image"
    - from: "src/llenergymeasure/infra/docker_runner.py"
      to: "src/llenergymeasure/infra/container_entrypoint.py"
      via: "container invokes entrypoint to run experiment"
      pattern: "container_entrypoint"
---

<objective>
Create the DockerRunner class that dispatches a single experiment to an ephemeral Docker container, managing the full lifecycle: temp directory creation, config JSON write, docker run execution, result JSON read, temp cleanup.

Purpose: This is the core dispatch mechanism that StudyRunner will call instead of multiprocessing.spawn when runner=docker. It uses the foundation types from Plan 01 (errors, entrypoint, registry) and runner resolution from Plan 02.

Output: DockerRunner class with full unit test coverage for success, failure, timeout, and cleanup paths.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-docker-runner-infrastructure/17-CONTEXT.md
@.planning/phases/17-docker-runner-infrastructure/17-01-SUMMARY.md
@.planning/phases/17-docker-runner-infrastructure/17-02-SUMMARY.md

<interfaces>
<!-- Contracts from Plan 01 and Plan 02 that this plan consumes -->

From src/llenergymeasure/infra/docker_errors.py (Plan 01):
```python
class DockerError(LLEMError): ...
class DockerImagePullError(DockerError): ...
class DockerGPUAccessError(DockerError): ...
class DockerOOMError(DockerError): ...
class DockerTimeoutError(DockerError): ...
class DockerPermissionError(DockerError): ...
class DockerContainerError(DockerError): ...
def translate_docker_error(returncode: int, stderr: str, image: str) -> DockerError: ...
def capture_stderr_snippet(stderr: str, max_lines: int = 20) -> str: ...
```

From src/llenergymeasure/infra/container_entrypoint.py (Plan 01):
```python
def run_container_experiment(config_path: Path, result_dir: Path) -> Path: ...
# Container entrypoint reads LLEM_CONFIG_PATH, runs experiment, writes result JSON
# Error writes: {"type": "...", "message": "...", "traceback": "..."}
```

From src/llenergymeasure/infra/image_registry.py (Plan 01):
```python
def get_default_image(backend: str) -> str: ...
def parse_runner_value(value: str) -> tuple[str, str | None]: ...
```

From src/llenergymeasure/infra/runner_resolution.py (Plan 02):
```python
@dataclass
class RunnerSpec:
    mode: Literal["local", "docker"]
    image: str | None
    source: str

def resolve_runner(backend: str, yaml_runners: dict | None, user_config: UserRunnersConfig | None) -> RunnerSpec: ...
```

From src/llenergymeasure/domain/experiment.py:
```python
def compute_measurement_config_hash(config: ExperimentConfig) -> str: ...
class ExperimentResult(BaseModel): ...  # frozen, extra="forbid"
```

From 17-CONTEXT.md (locked decisions):
- Exchange dir: tempfile.mkdtemp(prefix='llem-')
- Config: {config_hash}_config.json in exchange dir
- Container mounts exchange dir as /run/llem, LLEM_CONFIG_PATH points to config file
- Container writes {config_hash}_result.json to same mount
- Cleanup: delete temp dir on success; keep on failure with logged debug path
- Docker run: --rm --gpus all -v /tmp/llem-{hash}:/run/llem -e LLEM_CONFIG_PATH=...
- Runner is NOT part of config hash
- Runner type, image tag, image digest recorded in effective_config
- Container ID recorded in EnvironmentSnapshot
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: DockerRunner class with full dispatch lifecycle</name>
  <files>
    src/llenergymeasure/infra/docker_runner.py
  </files>
  <action>
    Create `infra/docker_runner.py` with:

    ```python
    class DockerRunner:
        """Dispatches a single experiment to an ephemeral Docker container.

        Lifecycle:
        1. Create temp exchange dir (tempfile.mkdtemp(prefix='llem-'))
        2. Write ExperimentConfig as JSON to {config_hash}_config.json
        3. docker run --rm --gpus all -v {exchange_dir}:/run/llem \
               -e LLEM_CONFIG_PATH=/run/llem/{config_hash}_config.json \
               --shm-size 8g {image} \
               python -m llenergymeasure.infra.container_entrypoint
        4. Read {config_hash}_result.json from exchange dir
        5. Clean up temp dir on success; preserve on failure
        """
    ```

    Key methods:

    - `__init__(self, image: str, timeout: int | None = None)`:
      - Store image tag, optional timeout (seconds, None = no timeout).

    - `run(self, config: ExperimentConfig) -> ExperimentResult | dict`:
      - Compute `config_hash = compute_measurement_config_hash(config)`
      - Create exchange dir via `tempfile.mkdtemp(prefix="llem-")`
      - Write config JSON to `{exchange_dir}/{config_hash}_config.json` using `config.model_dump_json()`
      - Build docker command:
        ```python
        cmd = [
            "docker", "run", "--rm",
            "--gpus", "all",
            "-v", f"{exchange_dir}:/run/llem",
            "-e", f"LLEM_CONFIG_PATH=/run/llem/{config_hash}_config.json",
            "--shm-size", "8g",
            image,
            "python", "-m", "llenergymeasure.infra.container_entrypoint",
        ]
        ```
      - Execute via `subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)`
      - On success (returncode 0): read result JSON from `{exchange_dir}/{config_hash}_result.json`, parse into `ExperimentResult`, clean up exchange dir, return result.
      - On timeout (`subprocess.TimeoutExpired`): raise `DockerTimeoutError`
      - On failure (returncode != 0): call `translate_docker_error(returncode, proc.stderr, image)`. Check if error JSON file exists in exchange dir (container may have written error payload before crashing). Log debug path: "Debug artifacts at {exchange_dir}". Raise the translated Docker error. Do NOT clean up exchange dir.
      - On success but missing result file: raise `DockerContainerError` with message "Container exited 0 but no result file found at {path}". Preserve exchange dir.

    - `_build_docker_cmd(self, config_hash: str, exchange_dir: str) -> list[str]`:
      Private helper to build the docker run command. Allows subclassing for custom flags. Propagate `HF_TOKEN` env var if set (for gated models inside container): add `-e HF_TOKEN={token}`.

    - `_read_result(self, exchange_dir: Path, config_hash: str) -> ExperimentResult | dict`:
      Read and parse the result JSON file. If the file contains an error payload (has "type" and "traceback" keys), return as dict (same format as StudyRunner worker errors). Otherwise parse as ExperimentResult.

    - `_cleanup_exchange_dir(self, exchange_dir: Path) -> None`:
      Remove exchange dir. Log warning on failure (never mask real errors — per CONTEXT.md).

    Runner metadata for effective_config (per CONTEXT.md — runner as metadata, not identity):
    - After successful result, inject runner metadata into result's effective_config:
      `runner_type: "docker"`, `runner_image: "{image}"`, `runner_source: "{source}"`
    - Since ExperimentResult is frozen, create a copy: `result.model_copy(update={"effective_config": {**result.effective_config, **runner_metadata}})`
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.infra.docker_runner import DockerRunner; print('DockerRunner importable')"</automated>
  </verify>
  <done>
    - DockerRunner class dispatches experiments to ephemeral containers
    - Uses subprocess.run with --rm --gpus all --shm-size 8g
    - Config/result transfer via mounted JSON files with LLEM_CONFIG_PATH
    - Container entrypoint is python -m llenergymeasure.infra.container_entrypoint
    - Error translation via translate_docker_error
    - Cleanup on success, preserve on failure
    - Runner metadata injected into effective_config
  </done>
</task>

<task type="auto">
  <name>Task 2: DockerRunner unit tests</name>
  <files>
    tests/unit/test_docker_runner.py
  </files>
  <action>
    Create `tests/unit/test_docker_runner.py` with comprehensive tests. All tests mock `subprocess.run` — no real Docker needed.

    Test cases:
    1. **Success path**: Mock subprocess.run returning 0. Write a valid ExperimentResult JSON to the exchange dir (mock the temp dir). Assert DockerRunner.run() returns ExperimentResult, exchange dir cleaned up.

    2. **Container failure**: Mock subprocess.run returning non-zero with stderr containing "No such image". Assert DockerImagePullError raised. Assert exchange dir preserved.

    3. **OOM error**: Mock subprocess.run returning 137 with "OOM" in stderr. Assert DockerOOMError.

    4. **Timeout**: Mock subprocess.run raising `subprocess.TimeoutExpired`. Assert DockerTimeoutError.

    5. **Permission error**: Mock subprocess returning 1 with "permission denied" in stderr. Assert DockerPermissionError.

    6. **Missing result file**: Mock subprocess returning 0 but no result JSON exists. Assert DockerContainerError with "no result file" message.

    7. **Error payload from container**: Mock subprocess returning 0, but result file contains error dict (type, message, traceback). Assert returned as dict, not ExperimentResult.

    8. **Docker command structure**: Assert the docker run command includes --rm, --gpus all, --shm-size 8g, -v mount, -e LLEM_CONFIG_PATH, and the entrypoint module path.

    9. **HF_TOKEN propagation**: Set HF_TOKEN env var, assert it appears in docker command as -e HF_TOKEN={token}.

    10. **Runner metadata in effective_config**: On success, assert result.effective_config contains runner_type, runner_image keys.

    11. **Cleanup warning**: Mock shutil.rmtree raising PermissionError. Assert warning logged, no exception raised.

    Use `tmp_path` fixture for exchange dir. Mock `tempfile.mkdtemp` to return a path within `tmp_path`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_docker_runner.py -x -v</automated>
  </verify>
  <done>
    - All 11 test scenarios pass
    - No real Docker invocations — fully mocked
    - Success, failure, timeout, permission, OOM, missing result all covered
    - Docker command structure verified
    - Cleanup behaviour verified for both success and failure
    - Runner metadata injection verified
  </done>
</task>

</tasks>

<verification>
```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool

# DockerRunner tests
python -m pytest tests/unit/test_docker_runner.py -x -v

# No regression
python -m pytest tests/unit/ -x --timeout=120

# Type check
python -m mypy src/llenergymeasure/infra/docker_runner.py --ignore-missing-imports

# Lint
python -m ruff check src/llenergymeasure/infra/docker_runner.py
```
</verification>

<success_criteria>
- DockerRunner.run() dispatches experiment to ephemeral container (docker run --rm) — DOCK-01
- Container completion signalled by subprocess.run blocking call — DOCK-04
- Config JSON written to temp dir, mounted as /run/llem — DOCK-02
- Result JSON read from same mount after container exit — DOCK-03
- Docker errors translated to categorised exceptions with fix suggestions
- Cleanup on success, preservation on failure with debug path
- All tests pass with no real Docker dependency
</success_criteria>

<output>
After completion, create `.planning/phases/17-docker-runner-infrastructure/17-03-SUMMARY.md`
</output>
