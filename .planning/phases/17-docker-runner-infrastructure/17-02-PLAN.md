---
phase: 17-docker-runner-infrastructure
plan: "02"
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/config/user_config.py
  - src/llenergymeasure/infra/runner_resolution.py
  - tests/unit/test_runner_resolution.py
autonomous: true
requirements: [DOCK-06]

must_haves:
  truths:
    - "Runner can be configured per-backend via study YAML runners: section"
    - "Environment variable LLEM_RUNNER_{BACKEND} overrides YAML config"
    - "Resolution precedence is: env var > study YAML > user config > built-in default"
    - "Docker availability (Docker + NVIDIA Container Toolkit) is detected correctly"
  artifacts:
    - path: "src/llenergymeasure/infra/runner_resolution.py"
      provides: "Runner resolution logic with precedence chain"
      contains: "resolve_runner"
    - path: "tests/unit/test_runner_resolution.py"
      provides: "Full coverage of resolution precedence and edge cases"
      contains: "test_"
  key_links:
    - from: "src/llenergymeasure/infra/runner_resolution.py"
      to: "src/llenergymeasure/config/user_config.py"
      via: "reads UserRunnersConfig for per-backend defaults"
      pattern: "UserRunnersConfig"
    - from: "src/llenergymeasure/infra/runner_resolution.py"
      to: "src/llenergymeasure/infra/image_registry.py"
      via: "uses parse_runner_value to parse config values"
      pattern: "parse_runner_value"
---

<objective>
Create the runner resolution module that determines whether an experiment runs locally or in Docker, respecting the full precedence chain (env var > study YAML > user config > Docker-first default).

Purpose: This is the decision logic consumed by Plan 03 (DockerRunner) and Plan 04 (StudyRunner integration). The runner resolution is independent of Docker dispatch mechanics and can be built and tested in parallel.

Output: Runner resolution module with full test coverage for all precedence scenarios.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-docker-runner-infrastructure/17-CONTEXT.md

<interfaces>
<!-- Key types from user config and study config -->

From src/llenergymeasure/config/user_config.py:
```python
class UserRunnersConfig(BaseModel):
    model_config = {"extra": "forbid"}
    pytorch: str = Field(default="local", ...)
    vllm: str = Field(default="local", ...)
    tensorrt: str = Field(default="local", ...)

class UserConfig(BaseModel):
    runners: UserRunnersConfig = Field(default_factory=UserRunnersConfig)
    # ...

def load_user_config(config_path: Path | None = None) -> UserConfig: ...
```

From src/llenergymeasure/config/user_config.py (_apply_env_overrides):
```python
# Already handles LLEM_RUNNER_{BACKEND} env vars:
for backend in ("pytorch", "vllm", "tensorrt"):
    env_key = f"LLEM_RUNNER_{backend.upper()}"
    if val := os.environ.get(env_key):
        runners_updates[backend] = val
```

From src/llenergymeasure/config/docker_detection.py:
```python
def is_inside_docker() -> bool: ...
def should_use_docker_for_campaign(backends: list[str]) -> bool: ...
```

From 17-CONTEXT.md (locked decisions):
- Docker-first when available: if Docker + NVIDIA Container Toolkit detected, default runner is `docker`
- Local fallback: if Docker not detected, default is `local` with nudge message
- User-set config always wins — explicit configuration is never overridden
- Resolution precedence: env var > study/experiment YAML > user config > built-in registry > default
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Runner resolution module with precedence chain</name>
  <files>
    src/llenergymeasure/infra/runner_resolution.py
    tests/unit/test_runner_resolution.py
  </files>
  <action>
    1. Create `infra/runner_resolution.py`:

       - `is_docker_available() -> bool`: check if `docker` CLI is on PATH (via `shutil.which("docker")`) AND `nvidia-container-toolkit` or `nvidia-docker2` is detectable (check for `nvidia-container-runtime` or `nvidia-ctk` on PATH). This is a quick host-level check; container-level GPU validation is Phase 18.

       - `@dataclass RunnerSpec`:
         ```python
         @dataclass
         class RunnerSpec:
             mode: Literal["local", "docker"]  # execution mode
             image: str | None  # Docker image (None for local)
             source: str  # where this came from: "env", "yaml", "user_config", "auto_detected", "default"
         ```

       - `resolve_runner(backend: str, yaml_runners: dict[str, str] | None = None, user_config: UserRunnersConfig | None = None) -> RunnerSpec`:
         Precedence chain (highest to lowest):
         1. **Env var**: `LLEM_RUNNER_{BACKEND.upper()}` — if set, parse with `parse_runner_value()`, source="env"
         2. **Study/experiment YAML**: `yaml_runners[backend]` — if provided and not None, parse, source="yaml"
         3. **User config**: `user_config.{backend}` — if not "local" (default), parse, source="user_config"
         4. **Auto-detection**: if `is_docker_available()`, return docker with default image, source="auto_detected". Log one-liner: "Docker detected. Using containerised execution for reproducible measurements."
         5. **Default**: return local, source="default". Log one-liner nudge: "Docker not detected. Install Docker + NVIDIA Container Toolkit for reproducible isolated measurements."

         When mode is "docker" and image is None, resolve image from `get_default_image(backend)` (from image_registry — import at call time to avoid circular deps).

       - `resolve_study_runners(study: StudyConfig, user_config: UserRunnersConfig | None = None) -> dict[str, RunnerSpec]`:
         Resolve runner for each unique backend in the study. Extract `runners:` section from raw YAML (this will be passed as `yaml_runners` dict). Returns `{backend_name: RunnerSpec}` mapping.

    2. Create `tests/unit/test_runner_resolution.py`:
       - Test env var override wins over everything: set `LLEM_RUNNER_VLLM=docker:custom/img`, assert source="env"
       - Test YAML runners win over user config: yaml_runners={"pytorch": "docker"}, user_config has local, assert source="yaml"
       - Test user config wins over auto-detection: user_config.pytorch="docker:myimg", assert source="user_config"
       - Test auto-detection when Docker available: mock `is_docker_available()=True`, no explicit config, assert source="auto_detected", mode="docker"
       - Test local fallback when Docker unavailable: mock `is_docker_available()=False`, no explicit config, assert source="default", mode="local"
       - Test `is_docker_available()` with mock `shutil.which` returning paths
       - Test `is_docker_available()` returns False when docker not on PATH
       - Test explicit "local" in user config is respected (not overridden by auto-detection)
       - Test `parse_runner_value` integration: "docker:ghcr.io/custom:v1" resolves to image="ghcr.io/custom:v1"
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_runner_resolution.py -x -v</automated>
  </verify>
  <done>
    - resolve_runner() implements full precedence chain: env > YAML > user_config > auto_detect > default
    - Docker-first default when Docker available (per CONTEXT.md locked decision)
    - Local fallback with nudge message when Docker unavailable
    - Explicit user config always wins — never overridden by auto-detection
    - All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Add runners section to study YAML parsing</name>
  <files>
    src/llenergymeasure/config/models.py
    src/llenergymeasure/config/user_config.py
  </files>
  <action>
    1. In `config/models.py`, add an optional `runners` field to `StudyConfig`:
       ```python
       runners: dict[str, str] | None = Field(
           default=None,
           description="Per-backend runner config: {'pytorch': 'local', 'vllm': 'docker:image'}. "
                       "None = use user config / auto-detection.",
       )
       ```
       This field is populated by the study YAML `runners:` section and passed through to runner resolution. The field is NOT part of ExperimentConfig (runner is metadata, not experiment identity — per CONTEXT.md locked decision that runner is not part of config hash).

    2. In `config/user_config.py`, update `UserRunnersConfig` to also accept bare `"docker"` (not just `"docker:<image>"`). The existing validator already handles this (`value != "local" and not value.startswith("docker:")` rejects "docker") — update the validator to accept bare "docker" as well:
       ```python
       if value != "local" and value != "docker" and not value.startswith("docker:"):
           raise ValueError(...)
       ```

    3. No changes to `config/loader.py` needed — the `runners:` key in study YAML will be passed through naturally by `expand_grid()` (which strips study-only keys) and picked up by `StudyConfig(**...)` construction. Verify this by checking that `expand_grid._extract_fixed()` does NOT strip `runners`. If it does, add "runners" to the pass-through list.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_config_models.py tests/unit/test_user_config.py -x -v</automated>
  </verify>
  <done>
    - StudyConfig accepts `runners: {"vllm": "docker"}` in YAML
    - UserRunnersConfig accepts bare "docker" (not just "docker:image")
    - Runner field is on StudyConfig, NOT ExperimentConfig (runner is metadata)
    - Existing tests pass with no regression
  </done>
</task>

</tasks>

<verification>
```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool

# New tests pass
python -m pytest tests/unit/test_runner_resolution.py -x -v

# No regression
python -m pytest tests/unit/ -x --timeout=120

# Type check
python -m mypy src/llenergymeasure/infra/runner_resolution.py --ignore-missing-imports

# Lint
python -m ruff check src/llenergymeasure/infra/runner_resolution.py src/llenergymeasure/config/models.py
```
</verification>

<success_criteria>
- Runner resolution correctly implements precedence: env var > YAML > user_config > auto_detect > default
- Docker-first default when Docker + NVIDIA Container Toolkit detected
- Local fallback with nudge when Docker not detected
- User-set config never overridden by auto-detection
- StudyConfig accepts runners: section from YAML
- All tests pass, no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/17-docker-runner-infrastructure/17-02-SUMMARY.md`
</output>
