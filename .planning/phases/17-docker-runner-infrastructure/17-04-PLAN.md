---
phase: 17-docker-runner-infrastructure
plan: "04"
type: execute
wave: 3
depends_on: ["17-03"]
files_modified:
  - src/llenergymeasure/study/runner.py
  - src/llenergymeasure/_api.py
  - src/llenergymeasure/orchestration/preflight.py
  - src/llenergymeasure/config/docker_detection.py
  - tests/unit/test_study_runner.py
  - tests/unit/test_api.py
autonomous: true
requirements: [DOCK-05]

must_haves:
  truths:
    - "StudyRunner dispatches to Docker when runner resolves to docker for a given backend"
    - "A multi-backend study auto-elevates all backends to Docker when Docker is available"
    - "Mixed runners in a study produce a warning but respect explicit user config"
    - "Incompatible backend locally (not installed, runner=local) logs error and skips experiment"
    - "Single experiment (llem run) also respects runner resolution"
  artifacts:
    - path: "src/llenergymeasure/study/runner.py"
      provides: "Docker dispatch path in _run_one() alongside existing subprocess path"
      contains: "DockerRunner"
    - path: "src/llenergymeasure/_api.py"
      provides: "Runner-aware _run() and _run_in_process() paths"
      contains: "resolve_runner"
    - path: "src/llenergymeasure/orchestration/preflight.py"
      provides: "Updated multi-backend guard with auto-elevation"
      contains: "auto_elevat"
  key_links:
    - from: "src/llenergymeasure/study/runner.py"
      to: "src/llenergymeasure/infra/docker_runner.py"
      via: "DockerRunner.run() called for docker-resolved experiments"
      pattern: "DockerRunner"
    - from: "src/llenergymeasure/study/runner.py"
      to: "src/llenergymeasure/infra/runner_resolution.py"
      via: "resolve_runner determines dispatch path"
      pattern: "resolve_runner"
    - from: "src/llenergymeasure/orchestration/preflight.py"
      to: "src/llenergymeasure/infra/runner_resolution.py"
      via: "auto-elevation uses is_docker_available"
      pattern: "is_docker_available"
---

<objective>
Wire DockerRunner into StudyRunner and _api._run(), implement auto-elevation for multi-backend studies, and update the multi-backend guard in preflight to auto-elevate instead of hard-error when Docker is available.

Purpose: This is the integration plan that connects all the Docker infrastructure from Plans 01-03 into the existing execution pipeline. After this plan, setting `runner: docker` in config actually causes experiments to run in containers.

Output: Updated StudyRunner, _api, and preflight modules with Docker dispatch path and auto-elevation logic.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-docker-runner-infrastructure/17-CONTEXT.md
@.planning/phases/17-docker-runner-infrastructure/17-01-SUMMARY.md
@.planning/phases/17-docker-runner-infrastructure/17-02-SUMMARY.md
@.planning/phases/17-docker-runner-infrastructure/17-03-SUMMARY.md

<interfaces>
<!-- Contracts from Plans 01-03 that this plan wires together -->

From src/llenergymeasure/infra/docker_runner.py (Plan 03):
```python
class DockerRunner:
    def __init__(self, image: str, timeout: int | None = None): ...
    def run(self, config: ExperimentConfig) -> ExperimentResult | dict: ...
```

From src/llenergymeasure/infra/runner_resolution.py (Plan 02):
```python
@dataclass
class RunnerSpec:
    mode: Literal["local", "docker"]
    image: str | None
    source: str

def resolve_runner(backend: str, yaml_runners: dict | None, user_config: UserRunnersConfig | None) -> RunnerSpec: ...
def is_docker_available() -> bool: ...
def resolve_study_runners(study: StudyConfig, user_config: UserRunnersConfig | None) -> dict[str, RunnerSpec]: ...
```

From src/llenergymeasure/study/runner.py (existing):
```python
class StudyRunner:
    def __init__(self, study: StudyConfig, manifest_writer: ManifestWriter, study_dir: Path): ...
    def run(self) -> list[Any]: ...
    def _run_one(self, config: ExperimentConfig, mp_ctx: Any, index: int, total: int) -> Any: ...
```

From src/llenergymeasure/_api.py (existing):
```python
def _run(study: StudyConfig) -> StudyResult: ...
def _run_in_process(study, manifest, study_dir) -> tuple: ...
def _run_via_runner(study, manifest, study_dir) -> tuple: ...
```

From src/llenergymeasure/orchestration/preflight.py (existing):
```python
def run_study_preflight(study: StudyConfig) -> None:
    """Raises PreFlightError for multi-backend studies."""
    # Currently: hard error for multi-backend
    # Phase 17: auto-elevate to Docker when available
```

From 17-CONTEXT.md (locked decisions):
- Auto-elevation: ALL backends in the study go to Docker, no hybrid
- Mixed runners: warn, then respect user config
- Incompatible backend locally: log error, skip experiment, continue
- Auto-elevation message: minimal one-liner warning, proceed automatically
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update preflight with auto-elevation logic</name>
  <files>
    src/llenergymeasure/orchestration/preflight.py
    src/llenergymeasure/config/docker_detection.py
  </files>
  <action>
    1. Update `run_study_preflight()` in `preflight.py`:
       - Current: hard error for multi-backend studies
       - New: check `is_docker_available()`. If Docker available, log info: "Multi-backend study detected ({backends}). Auto-elevating all backends to Docker for isolation." Return normally (no error).
       - If Docker NOT available, raise `PreFlightError` with updated message: "Multi-backend study requires Docker isolation. Found backends: {backend_list}. Install Docker + NVIDIA Container Toolkit, or use a single backend."
       - This replaces the current hard error with auto-elevation (DOCK-05).

    2. Update `should_use_docker_for_campaign()` in `docker_detection.py`:
       - This function is the v1.x equivalent. Update it to call `is_docker_available()` from runner_resolution for the Docker detection part (avoid duplicating detection logic). Keep the is_inside_docker() check (no nested containers).
       - Note: this function may not be actively called in v2.0 code paths. If not, leave it as-is but ensure it doesn't break.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_preflight.py -x -v</automated>
  </verify>
  <done>
    - Multi-backend study with Docker available: auto-elevates, no error
    - Multi-backend study without Docker: PreFlightError with install guidance
    - Auto-elevation log message is a minimal one-liner
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire Docker dispatch into StudyRunner and _api</name>
  <files>
    src/llenergymeasure/study/runner.py
    src/llenergymeasure/_api.py
    tests/unit/test_study_runner.py
    tests/unit/test_api.py
  </files>
  <action>
    1. Update `StudyRunner.__init__()`:
       - Add parameter `runner_specs: dict[str, RunnerSpec] | None = None` — pre-resolved runner specs per backend. If None, all experiments run via subprocess (backward compat).

    2. Update `StudyRunner._run_one()`:
       - Before the existing subprocess path, check runner spec for this experiment's backend:
         ```python
         spec = self._runner_specs.get(config.backend) if self._runner_specs else None
         if spec and spec.mode == "docker":
             return self._run_one_docker(config, spec, index=index, total=total)
         ```
       - If runner is "local" or no spec, fall through to existing subprocess dispatch.

    3. Add `StudyRunner._run_one_docker()`:
       - Create `DockerRunner(image=spec.image, timeout=_calculate_timeout(config))`
       - Pre-dispatch GPU memory check (same as local path): `check_gpu_memory_residual()`
       - Mark manifest running: `self.manifest.mark_running(config_hash, cycle)`
       - Call `docker_runner.run(config)` — blocking
       - On success (ExperimentResult): save result, mark manifest completed (same as local path)
       - On failure (dict with "type" key): mark manifest failed (same as local path)
       - On DockerError exception: create failure dict from error, mark manifest failed, continue study
       - Progress display: emit "running" and "completed"/"failed" events (same as local path, but without the progress queue/thread since Docker is blocking)

    4. Update `_api._run()`:
       - After `run_study_preflight(study)`, resolve runners:
         ```python
         from llenergymeasure.config.user_config import load_user_config
         from llenergymeasure.infra.runner_resolution import resolve_study_runners
         user_config = load_user_config()
         runner_specs = resolve_study_runners(study, user_config.runners)
         ```
       - Pass `runner_specs` to StudyRunner constructor in `_run_via_runner()`.
       - For `_run_in_process()` (single experiment, no subprocess): also resolve runner. If docker, use DockerRunner directly instead of in-process execution.

    5. Handle mixed runners (per CONTEXT.md):
       - In `_run()`, after resolving runners, check if there are mixed modes (some local, some docker):
         ```python
         modes = {spec.mode for spec in runner_specs.values()}
         if len(modes) > 1:
             import logging
             logging.getLogger(__name__).warning(
                 "Mixed runners detected. For consistent measurements, "
                 "consider running all backends in Docker."
             )
         ```

    6. Update tests:
       - `test_study_runner.py`: Add test that when runner_specs has docker for a backend, _run_one_docker is called instead of subprocess dispatch. Mock DockerRunner.run().
       - `test_study_runner.py`: Add test that when runner_specs is None, subprocess dispatch is used (backward compat).
       - `test_study_runner.py`: Add test that DockerError from DockerRunner is caught and converted to failure dict.
       - `test_api.py`: Add/update test that _run() resolves runners and passes them to StudyRunner. Mock resolve_study_runners.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_runner.py tests/unit/test_api.py tests/unit/test_preflight.py -x -v</automated>
  </verify>
  <done>
    - StudyRunner dispatches to DockerRunner when runner resolves to docker
    - StudyRunner falls back to subprocess dispatch when runner is local or unspecified
    - DockerErrors from container are caught and converted to non-fatal failure dicts
    - _api._run() resolves runners and passes them through
    - Auto-elevation in preflight allows multi-backend studies when Docker available
    - Mixed runner warning emitted but user config respected
    - All existing and new tests pass
  </done>
</task>

</tasks>

<verification>
```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool

# All integration tests
python -m pytest tests/unit/test_study_runner.py tests/unit/test_api.py tests/unit/test_preflight.py -x -v

# Full regression check
python -m pytest tests/unit/ -x --timeout=120

# Type check on modified files
python -m mypy src/llenergymeasure/study/runner.py src/llenergymeasure/_api.py src/llenergymeasure/orchestration/preflight.py --ignore-missing-imports

# Lint
python -m ruff check src/llenergymeasure/study/runner.py src/llenergymeasure/_api.py src/llenergymeasure/orchestration/preflight.py
```
</verification>

<success_criteria>
- Setting runner: docker causes experiment to execute inside container — DOCK-01 wired
- Multi-backend study auto-elevates to Docker when available — DOCK-05
- Auto-elevation message is a minimal one-liner, proceeds automatically — DOCK-05
- Mixed runners produce warning but respect explicit user config — DOCK-05
- Incompatible backend locally (not installed, runner=local) skips experiment — DOCK-05
- Single experiment (llem run) also respects runner resolution
- All existing tests pass (no regression in subprocess path)
- New tests cover Docker dispatch, auto-elevation, mixed runner warning
</success_criteria>

<output>
After completion, create `.planning/phases/17-docker-runner-infrastructure/17-04-SUMMARY.md`
</output>
