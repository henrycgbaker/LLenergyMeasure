---
phase: 16-gpu-memory-verification
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/study/gpu_memory.py
  - src/llenergymeasure/study/runner.py
  - tests/unit/test_gpu_memory.py
autonomous: true
requirements:
  - MEAS-01
  - MEAS-02

must_haves:
  truths:
    - "Before each experiment dispatch, NVML is queried for current GPU memory usage"
    - "If residual memory exceeds configured threshold, a warning is logged before the experiment starts"
    - "A clean-state experiment (no prior GPU use) produces no warning"
    - "If pynvml is unavailable or NVML query fails, the check is silently skipped (never blocks)"
  artifacts:
    - path: "src/llenergymeasure/study/gpu_memory.py"
      provides: "NVML residual GPU memory check function"
      exports: ["check_gpu_memory_residual"]
    - path: "tests/unit/test_gpu_memory.py"
      provides: "Unit tests for GPU memory check"
      min_lines: 60
  key_links:
    - from: "src/llenergymeasure/study/runner.py"
      to: "src/llenergymeasure/study/gpu_memory.py"
      via: "check_gpu_memory_residual() called in _run_one() before p.start()"
      pattern: "check_gpu_memory_residual"
---

<objective>
Add a pre-dispatch NVML GPU memory residual check to StudyRunner so researchers can be confident that leftover GPU state from a prior experiment does not contaminate the next measurement.

Purpose: Measurement quality — subprocess isolation via `spawn` guarantees a clean CUDA context per child, but driver-level memory leaks or third-party GPU processes could leave residual memory that inflates `peak_memory_mb` or affects allocation patterns. A warning before dispatch makes this visible.

Output: `gpu_memory.py` module with `check_gpu_memory_residual()`, wired into `StudyRunner._run_one()` before `p.start()`, with unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/study/runner.py (lines 341-376):
```python
def _run_one(self, config: ExperimentConfig, mp_ctx: Any, index: int, total: int) -> Any:
    """Spawn a subprocess for one experiment; collect result or failure dict."""
    from llenergymeasure.domain.experiment import compute_measurement_config_hash
    config_hash = compute_measurement_config_hash(config)
    # ... cycle tracking ...
    timeout = _calculate_timeout(config)
    child_conn, parent_conn = mp_ctx.Pipe(duplex=False)
    progress_queue = mp_ctx.Queue()
    p = mp_ctx.Process(...)
    # ... consumer thread setup ...
    self.manifest.mark_running(config_hash, cycle)
    self._active_process = p
    p.start()  # <-- GPU memory check goes BEFORE this line
```

From src/llenergymeasure/core/power_thermal.py (lines 131-137):
```python
# Pattern for NVML memory query:
mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
sample.memory_used_mb = mem_info.used / (1024 * 1024)
sample.memory_total_mb = mem_info.total / (1024 * 1024)
```

From src/llenergymeasure/orchestration/preflight.py (lines 88-108):
```python
# Pattern for graceful pynvml usage with init/shutdown:
def _warn_if_persistence_mode_off() -> None:
    try:
        import pynvml
        pynvml.nvmlInit()
        try:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            mode = pynvml.nvmlDeviceGetPersistenceMode(handle)
            if mode == pynvml.NVML_FEATURE_DISABLED:
                logger.warning("...")
        finally:
            pynvml.nvmlShutdown()
    except Exception:
        pass
```

Logging: Use `logging.getLogger(__name__)` — consistent with study/ modules (runner.py uses loguru in some modules, but study/ uses stdlib logging via runner.py patterns). Actually, check runner.py — it doesn't import a logger directly. Use `logging.getLogger(__name__)` as in preflight.py.
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create gpu_memory module with NVML residual check</name>
  <files>src/llenergymeasure/study/gpu_memory.py, tests/unit/test_gpu_memory.py</files>
  <action>
Create `src/llenergymeasure/study/gpu_memory.py` with a single public function:

```python
def check_gpu_memory_residual(
    device_index: int = 0,
    threshold_mb: float = 100.0,
) -> None:
```

**Behaviour:**
1. Try to import pynvml. If ImportError → `logger.debug("pynvml not available, skipping GPU memory check")` and return.
2. Call `pynvml.nvmlInit()`.
3. Get device handle via `pynvml.nvmlDeviceGetHandleByIndex(device_index)`.
4. Query `pynvml.nvmlDeviceGetMemoryInfo(handle)` → extract `used_mb = mem_info.used / (1024 * 1024)`.
5. Call `pynvml.nvmlShutdown()` in a `finally` block.
6. If `used_mb > threshold_mb`: `logger.warning("Residual GPU memory detected on device %d: %.0f MB used (threshold: %.0f MB). Prior process may not have released GPU resources. Measurement accuracy could be affected.", device_index, used_mb, threshold_mb)`.
7. If `used_mb <= threshold_mb`: no log output (clean state).
8. If any `pynvml.NVMLError` or other exception during steps 2-5: `logger.debug("GPU memory check failed: %s", e)` and return silently. Never raise.

**Key design points:**
- Use `logging.getLogger(__name__)` — matches preflight.py pattern.
- The threshold default of 100 MB is a reasonable baseline: NVML driver overhead is typically 50-80 MB on modern GPUs, so 100 MB accommodates that while catching real residual allocations (model weights, KV caches). This is NOT configurable via ExecutionConfig in this phase — hardcoded default is fine for M3. Configurability can be added later if researchers need it.
- The function is a void function — it only logs, never raises, never returns data. Warnings never block.
- `device_index=0` default — matches the single-GPU assumption in M2. Multi-GPU support follows naturally in future phases.

**Tests** (`tests/unit/test_gpu_memory.py`):

Write tests using `unittest.mock.patch` to mock pynvml:

1. `test_clean_state_no_warning`: Mock pynvml with `mem_info.used = 50 * 1024 * 1024` (50 MB, below 100 MB threshold). Assert no warning logged (use `caplog` fixture with level WARNING).

2. `test_residual_memory_warning`: Mock pynvml with `mem_info.used = 500 * 1024 * 1024` (500 MB, above threshold). Assert warning logged containing "Residual GPU memory detected" and "500 MB".

3. `test_custom_threshold`: Mock pynvml with `mem_info.used = 200 * 1024 * 1024` (200 MB). Call with `threshold_mb=150.0`. Assert warning logged. Call again with `threshold_mb=250.0`. Assert no warning.

4. `test_pynvml_not_available`: Patch `importlib.import_module` or use `patch.dict('sys.modules', {'pynvml': None})` to simulate ImportError. Assert function returns without error and no warning logged. Debug log should mention unavailability.

5. `test_nvml_error_graceful`: Mock pynvml but have `nvmlDeviceGetHandleByIndex` raise a mock `NVMLError`. Assert function returns without error and no warning logged.

6. `test_nvml_shutdown_called`: Mock pynvml with normal memory values. Assert `nvmlShutdown` was called exactly once (cleanup).

Mocking pattern: Create a mock pynvml module with `MagicMock()` for `nvmlInit`, `nvmlShutdown`, `nvmlDeviceGetHandleByIndex`. For memory, create a mock object with `.used` and `.total` attributes and patch `nvmlDeviceGetMemoryInfo` to return it.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_gpu_memory.py -x -v</automated>
  </verify>
  <done>
    - `check_gpu_memory_residual()` exists and is importable from `llenergymeasure.study.gpu_memory`
    - Clean state (low memory) produces no warning
    - High residual memory produces a descriptive warning
    - pynvml unavailability or NVML errors are handled gracefully (no exception, debug log only)
    - All 6 tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire GPU memory check into StudyRunner._run_one()</name>
  <files>src/llenergymeasure/study/runner.py, tests/unit/test_study_runner.py</files>
  <action>
**In `src/llenergymeasure/study/runner.py`:**

Add the GPU memory check call in `_run_one()`, immediately BEFORE `p.start()` (line 376) and AFTER `self._active_process = p` (line 374). The exact insertion point is between lines 374 and 376:

```python
        self.manifest.mark_running(config_hash, cycle)
        self._active_process = p

        # Pre-dispatch GPU memory residual check (MEAS-01, MEAS-02)
        from llenergymeasure.study.gpu_memory import check_gpu_memory_residual
        check_gpu_memory_residual()

        p.start()
```

Use a local import to avoid circular import issues and keep the import lazy (pynvml may not be installed).

**Why this location:**
- After `mark_running()`: the manifest reflects we're about to run, so the check is part of the dispatch flow.
- Before `p.start()`: the check runs in the parent process (host), not the child. This is correct because:
  - We want to check the host GPU state before spawning.
  - The child will have a clean CUDA context due to `spawn`.
  - Residual memory from a PRIOR child (or external process) is what we're detecting.

**Do NOT add configuration for the threshold** — use the default 100 MB. No changes to ExecutionConfig or any config models. Keep it simple for M3.

**In `tests/unit/test_study_runner.py`:**

Add one test to verify the check is called during `_run_one()`:

`test_gpu_memory_check_called_before_dispatch`: Patch `llenergymeasure.study.runner.check_gpu_memory_residual` (the import target, not the source module). Use the existing test infrastructure pattern from the file — the `FakeProcess` / mock mp_ctx pattern already used in the test file. Assert that `check_gpu_memory_residual` was called once. Also verify it's called BEFORE `p.start()` by using `mock.call_args_list` ordering or side effects.

If the existing test infrastructure in `test_study_runner.py` uses specific mock patterns for `_run_one`, follow those patterns. The goal is a lightweight assertion that the wiring exists, not a full integration test.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_runner.py -x -v -k "gpu_memory"</automated>
  </verify>
  <done>
    - `_run_one()` calls `check_gpu_memory_residual()` before `p.start()`
    - Existing StudyRunner tests still pass (no regression)
    - New test verifies the wiring
  </done>
</task>

</tasks>

<verification>
1. **All tests pass**: `python -m pytest tests/unit/test_gpu_memory.py tests/unit/test_study_runner.py -x -v`
2. **No import errors**: `python -c "from llenergymeasure.study.gpu_memory import check_gpu_memory_residual; print('OK')"`
3. **Type check**: `python -m mypy src/llenergymeasure/study/gpu_memory.py --ignore-missing-imports`
4. **Lint**: `python -m ruff check src/llenergymeasure/study/gpu_memory.py`
5. **Full test suite**: `python -m pytest tests/unit/ -x` (no regression)
</verification>

<success_criteria>
- MEAS-01 satisfied: NVML GPU memory is queried before each experiment dispatch via `check_gpu_memory_residual()` in `_run_one()`
- MEAS-02 satisfied: Warning logged when residual memory exceeds 100 MB threshold
- Clean state produces no warning (success criterion 3 from roadmap)
- Graceful degradation: pynvml unavailable or NVML errors → skip silently, never block
- All unit tests pass, no regression in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/16-gpu-memory-verification/16-01-SUMMARY.md`
</output>
