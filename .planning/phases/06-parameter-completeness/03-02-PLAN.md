---
phase: 03-parameter-completeness
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/backend_configs.py
autonomous: true

must_haves:
  truths:
    - "vLLM backend exposes 90%+ of energy-impactful engine parameters"
    - "New vLLM parameters have docstring-documented constraints"
    - "Introspection auto-discovers all new vLLM parameters"
  artifacts:
    - path: "src/llenergymeasure/config/backend_configs.py"
      provides: "Extended VLLMConfig with new engine parameters"
      contains: "tokenizer_mode"
  key_links:
    - from: "src/llenergymeasure/config/backend_configs.py"
      to: "introspection.py"
      via: "get_backend_params('vllm')"
      pattern: "get_backend_params"
---

<objective>
Add missing energy/throughput-impactful parameters to VLLMConfig Pydantic model.

Purpose: Expand vLLM parameter coverage from 81.9% to 90%+ by adding engine parameters from vLLM LLM() constructor that affect energy/throughput.
Output: Extended VLLMConfig with new fields that introspection auto-discovers.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-parameter-completeness/03-CONTEXT.md
@.planning/phases/03-parameter-completeness/03-RESEARCH.md
@src/llenergymeasure/config/backend_configs.py
@src/llenergymeasure/config/introspection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add missing vLLM engine parameters</name>
  <files>src/llenergymeasure/config/backend_configs.py</files>
  <action>
Add the following missing parameters to VLLMConfig. Group by functional category:

**Model Loading (after max_model_len):**
1. `dtype: Literal["auto", "half", "float16", "bfloat16", "float32"] = Field(default="auto", description="Model weight dtype. 'auto' uses model config. Affects memory usage and compute speed.")`
2. `tokenizer_mode: Literal["auto", "slow", "mistral"] = Field(default="auto", description="Tokenizer mode. 'slow' disables fast tokenizer. 'mistral' for Mistral models.")`
3. `trust_remote_code: bool = Field(default=False, description="Trust remote code in model configs. Required for some custom architectures.")`
4. `revision: str | None = Field(default=None, description="Model checkpoint revision (commit hash or tag).")`
5. `download_dir: str | None = Field(default=None, description="Directory for model downloads. Defaults to HuggingFace cache.")`

**Scheduler (after enable_chunked_prefill):**
6. `num_scheduler_steps: int = Field(default=1, ge=1, description="Number of scheduler steps per engine iteration. >1 enables multi-step scheduling.")`
7. `multi_step_stream_outputs: bool = Field(default=True, description="Stream outputs for multi-step scheduling. Only applies when num_scheduler_steps > 1.")`
8. `max_num_on_the_fly_requests: int | None = Field(default=None, ge=1, description="Maximum concurrent request processing. None = unlimited.")`

**Memory (after cpu_offload_gb):**
9. `enable_lora: bool = Field(default=False, description="Enable LoRA adapter support at engine level. See lora config for adapter settings.")`

**Disable attention sliding window (in attention section or near it):**
Add to VLLMAttentionConfig:
10. Update VLLMAttentionConfig to include `disable_sliding_window` if not already there.

Add these in appropriate sections following the existing organisation pattern. Use docstrings to document version requirements where known (e.g., "vLLM >= 0.5.0").
  </action>
  <verify>
Run `python -c "from llenergymeasure.config.backend_configs import VLLMConfig; print(len(VLLMConfig.model_fields))"` - should show increased field count.
Run `python -c "from llenergymeasure.config.introspection import get_backend_params; print('tokenizer_mode' in str(get_backend_params('vllm')))"` - should print True.
  </verify>
  <done>
VLLMConfig has 9+ new engine parameters. Introspection auto-discovers them. No import errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify extra escape hatch and update documentation</name>
  <files>src/llenergymeasure/config/backend_configs.py</files>
  <action>
Verify that VLLMConfig already has the `extra: dict[str, Any]` escape hatch field.

Update the description to be more explicit:
```python
extra: dict[str, Any] = Field(
    default_factory=dict,
    description="Escape hatch: kwargs passed directly to vLLM LLM() constructor without validation. "
    "Use for undocumented/niche parameters not yet in schema. "
    "Example: extra: {seed: 42, enable_prefix_caching_api: true}"
)
```

Ensure the escape hatch is positioned at the end of the class (before any validators).
  </action>
  <verify>
Run `python -c "from llenergymeasure.config.backend_configs import VLLMConfig; print('extra' in VLLMConfig.model_fields)"` - should print True.
  </verify>
  <done>
VLLMConfig.extra field exists with clear passthrough documentation.
  </done>
</task>

</tasks>

<verification>
- [ ] `python -c "from llenergymeasure.config.backend_configs import VLLMConfig"` succeeds
- [ ] `ruff check src/llenergymeasure/config/backend_configs.py` passes
- [ ] `python -c "from llenergymeasure.config.introspection import get_backend_params; p=get_backend_params('vllm'); print(len(p))"` shows increased param count
- [ ] New params have test_values auto-generated by introspection
</verification>

<success_criteria>
VLLMConfig expanded with 9+ new engine parameters. All parameters have docstrings with constraints. Introspection auto-discovers all new fields. Extra escape hatch documented.
</success_criteria>

<output>
After completion, create `.planning/phases/03-parameter-completeness/03-02-SUMMARY.md`
</output>
