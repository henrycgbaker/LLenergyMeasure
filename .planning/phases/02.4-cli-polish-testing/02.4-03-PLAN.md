---
phase: 02.4-cli-polish-testing
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/runtime/test_all_params.py
  - tests/runtime/issues.yaml
autonomous: true

must_haves:
  truths:
    - "Smoke test captures ALL warnings and errors from stdout/stderr during inference"
    - "Test fails if ANY warning or error detected (strict mode)"
    - "Known issues can be tracked in issues.yaml for temporary exceptions"
    - "Smoke test uses SSOT parameter discovery from introspection.py"
  artifacts:
    - path: "tests/runtime/test_all_params.py"
      provides: "Extended smoke test with warning capture"
      contains: "capture_warnings"
    - path: "tests/runtime/issues.yaml"
      provides: "Known issues tracking file"
      contains: "known_issues"
  key_links:
    - from: "tests/runtime/test_all_params.py"
      to: "src/llenergymeasure/config/introspection.py"
      via: "SSOT parameter discovery"
      pattern: "get_backend_params"
---

<objective>
Extend the existing test_all_params.py smoke test infrastructure with comprehensive warning/error capture and strict pass/fail criteria.

Purpose: Ensure that ANY warning or error from ANY library/feature is captured, logged, and reported. The philosophy is "if anything is failing, I want to know about it" - comprehensive capture, not a predefined checklist.

Output: Enhanced smoke test suite that captures all output issues.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.4-cli-polish-testing/02.4-CONTEXT.md
@.planning/phases/02.4-cli-polish-testing/02.4-RESEARCH.md

@tests/runtime/test_all_params.py
@tests/CLAUDE.md
@src/llenergymeasure/config/introspection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add comprehensive warning/error capture to smoke tests</name>
  <files>tests/runtime/test_all_params.py</files>
  <action>
Extend the existing test_all_params.py with comprehensive output capture. The existing infrastructure already runs experiments and captures stdout/stderr - we need to add strict warning detection.

**Add new data class for captured issues:**
```python
@dataclass
class CapturedIssue:
    """A warning or error captured during test execution."""
    category: str  # "warning", "error", "deprecation"
    source: str    # e.g., "vllm", "torch", "transformers", "ray"
    message: str   # Full message text
    severity: str  # "warning", "error", "critical"
```

**Add new function for comprehensive issue extraction:**
```python
def extract_all_issues(stdout: str, stderr: str) -> list[CapturedIssue]:
    """Extract ALL warnings and errors from combined output.

    This function is deliberately comprehensive - it captures everything
    that might indicate a problem. The philosophy is "capture first,
    filter known issues later" rather than a predefined checklist.

    Patterns captured:
    - Python warnings (Warning:, DeprecationWarning, FutureWarning, UserWarning, etc.)
    - Library-specific warnings (vLLM, TensorRT, transformers, Ray)
    - Error messages (Error:, Exception:, Failed:)
    - Fallback notifications (e.g., "flash_attention_2 not available, using sdpa")
    - GPU/CUDA warnings
    """
    issues: list[CapturedIssue] = []
    combined = stdout + "\n" + stderr

    # Generic warning patterns (catch-all)
    warning_patterns = [
        (r"(?i)(warning|warn):\s*(.+)", "warning"),
        (r"(?i)deprecat\w*:\s*(.+)", "deprecation"),
        (r"(?i)futurewarning:\s*(.+)", "deprecation"),
        (r"(?i)userwarning:\s*(.+)", "warning"),
        (r"(?i)runtimewarning:\s*(.+)", "warning"),
    ]

    # Error patterns
    error_patterns = [
        (r"(?i)error:\s*(.+)", "error"),
        (r"(?i)exception:\s*(.+)", "error"),
        (r"(?i)failed:\s*(.+)", "error"),
        (r"(?i)traceback \(most recent call last\)", "error"),
    ]

    # Fallback patterns (indicate something didn't work as expected)
    fallback_patterns = [
        (r"(?i)falling back to\s*(.+)", "warning"),
        (r"(?i)using\s+(\w+)\s+instead of\s+(\w+)", "warning"),
        (r"(?i)not available.*using\s*(.+)", "warning"),
        (r"(?i)flash.?attention.*not.*available", "warning"),
        (r"(?i)cuda.*not.*available", "warning"),
    ]

    # Library-specific patterns
    library_patterns = [
        # vLLM
        (r"(?i)\[vllm\].*warning", "warning"),
        (r"(?i)ray.*warning", "warning"),
        # TensorRT
        (r"(?i)\[TensorRT\].*warning", "warning"),
        (r"(?i)engine.*warning", "warning"),
        # PyTorch/transformers
        (r"(?i)transformers.*warning", "warning"),
        (r"(?i)torch.*warning", "warning"),
    ]

    all_patterns = warning_patterns + error_patterns + fallback_patterns + library_patterns

    for pattern, severity in all_patterns:
        for match in re.finditer(pattern, combined, re.MULTILINE):
            # Determine source from context
            source = _identify_source(match.group(0))
            issues.append(CapturedIssue(
                category=severity if severity in ("warning", "error") else "warning",
                source=source,
                message=match.group(0).strip()[:500],  # Truncate long messages
                severity=severity,
            ))

    # Deduplicate by message
    seen = set()
    unique_issues = []
    for issue in issues:
        key = (issue.category, issue.message[:100])
        if key not in seen:
            seen.add(key)
            unique_issues.append(issue)

    return unique_issues


def _identify_source(message: str) -> str:
    """Identify the source library from a warning/error message."""
    message_lower = message.lower()
    if "vllm" in message_lower or "ray" in message_lower:
        return "vllm"
    if "tensorrt" in message_lower or "trt" in message_lower:
        return "tensorrt"
    if "transformers" in message_lower or "huggingface" in message_lower:
        return "transformers"
    if "torch" in message_lower or "cuda" in message_lower:
        return "torch"
    if "flash" in message_lower and "attention" in message_lower:
        return "flash_attention"
    return "unknown"
```

**Add known issues loading:**
```python
def load_known_issues() -> dict[str, dict[str, Any]]:
    """Load known issues from issues.yaml for exception handling."""
    issues_path = Path(__file__).parent / "issues.yaml"
    if not issues_path.exists():
        return {}
    try:
        import yaml
        with open(issues_path) as f:
            data = yaml.safe_load(f) or {}
        return data.get("known_issues", {})
    except Exception:
        return {}


def is_known_issue(issue: CapturedIssue, known_issues: dict) -> bool:
    """Check if an issue matches a known issue pattern."""
    for issue_id, info in known_issues.items():
        pattern = info.get("pattern", "")
        if pattern and re.search(pattern, issue.message, re.IGNORECASE):
            return True
    return False
```

**Update run_experiment to include strict mode:**
Add `--smoke` flag to main() argument parser:
```python
parser.add_argument(
    "--smoke",
    action="store_true",
    help="Run in smoke test mode: fail on ANY warning or error",
)
```

**Update TestResult with captured issues:**
Add field to TestResult dataclass:
```python
captured_issues: list[CapturedIssue] = field(default_factory=list)
```

**Update run_experiment to capture issues:**
After getting stdout/stderr, add:
```python
# Capture all issues from output
captured_issues = extract_all_issues(result.stdout, result.stderr)

# In smoke mode, fail if any issues detected (unless known)
if smoke_mode and captured_issues:
    known = load_known_issues()
    unknown_issues = [i for i in captured_issues if not is_known_issue(i, known)]
    if unknown_issues:
        status = "failed"
        error_summary = f"Smoke test: {len(unknown_issues)} unknown issues detected"
```

**Update report to include issue summary:**
Add to TestReport:
```python
issues_by_source: dict[str, int] = field(default_factory=dict)
```

Update generate_report to count issues by source.

Update print_summary to show:
```
  Issues by source:
    torch: 5
    transformers: 3
    vllm: 2
```
  </action>
  <verify>
```bash
# Run smoke test (will capture warnings but not fail unless --smoke)
python -m tests.runtime.test_all_params --backend pytorch --quick

# Run strict smoke test (fails on any issue)
python -m tests.runtime.test_all_params --backend pytorch --quick --smoke

# List params to verify SSOT still works
python -m tests.runtime.test_all_params --list-params --backend pytorch
```
  </verify>
  <done>
Smoke test captures ALL warnings/errors, categorises by source, reports comprehensively, and fails in `--smoke` mode if issues detected.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create issues.yaml for known issue tracking</name>
  <files>tests/runtime/issues.yaml</files>
  <action>
Create `tests/runtime/issues.yaml` to track known issues that are WIP fixes. This allows temporary exceptions while fixes are being developed.

```yaml
# Known Issues Tracking
#
# This file tracks known warnings/errors that are work-in-progress.
# Issues listed here will NOT cause smoke tests to fail.
#
# Format:
#   known_issues:
#     ISSUE_ID:
#       pattern: "regex pattern to match"
#       description: "What this issue is"
#       status: "wip" | "wontfix" | "external"
#       created: "YYYY-MM-DD"
#       ticket: "optional link to issue tracker"
#       notes: "optional additional context"
#
# When an issue is fixed, remove it from this file.

known_issues:
  # Example entry (uncomment when needed)
  # FLASH_ATTN_FALLBACK:
  #   pattern: "flash.?attention.*not.*available"
  #   description: "Flash Attention 2 falls back to SDPA when flash-attn not installed"
  #   status: wip
  #   created: "2026-02-04"
  #   notes: "Expected in environments without flash-attn package"

  # vLLM Ray initialization warnings
  # RAY_INIT_WARNING:
  #   pattern: "ray.*already.*initialized"
  #   description: "Ray shows warning when already initialized"
  #   status: external
  #   created: "2026-02-04"
  #   notes: "Upstream vLLM issue, harmless warning"

  # Transformers deprecation warnings (common in HF ecosystem)
  # HF_DEPRECATION:
  #   pattern: "transformers.*deprecated"
  #   description: "Transformers library deprecation warnings"
  #   status: external
  #   created: "2026-02-04"
  #   notes: "Upstream HuggingFace warnings, track for updates"

# Status values:
#   wip      - We're working on a fix
#   wontfix  - Known issue we can't/won't fix
#   external - Issue in upstream dependency, not our code
```
  </action>
  <verify>
```bash
# Verify YAML is valid
python -c "import yaml; yaml.safe_load(open('tests/runtime/issues.yaml'))"

# Verify issues.yaml is loaded by test
python -c "
import sys
sys.path.insert(0, 'tests/runtime')
from test_all_params import load_known_issues
issues = load_known_issues()
print(f'Loaded {len(issues)} known issues')
"
```
  </verify>
  <done>
issues.yaml exists with documented format for tracking known issues, and is loaded by smoke test infrastructure.
  </done>
</task>

</tasks>

<verification>
1. `extract_all_issues` captures warnings from stdout/stderr
2. `--smoke` flag added to test_all_params.py
3. In smoke mode, test fails if unknown issues detected
4. Known issues from issues.yaml are excluded from failure
5. Report shows issues categorised by source
6. SSOT parameter discovery still works (--discover mode)
</verification>

<success_criteria>
- All warnings/errors captured from combined output
- Issues categorised by source (vllm, torch, transformers, etc.)
- `--smoke` flag enforces strict pass/fail
- issues.yaml provides exception mechanism for WIP issues
- Report includes comprehensive issue summary
- No regression in existing test_all_params functionality
</success_criteria>

<output>
After completion, create `.planning/phases/02.4-cli-polish-testing/02.4-03-SUMMARY.md`
</output>
