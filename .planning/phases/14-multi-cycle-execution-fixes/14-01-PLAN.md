---
phase: 14-multi-cycle-execution-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/study/runner.py
  - src/llenergymeasure/study/manifest.py
  - src/llenergymeasure/_api.py
  - tests/unit/test_study_runner.py
  - tests/unit/test_study_manifest.py
autonomous: true
requirements:
  - STU-07
  - STU-08
  - STU-09

must_haves:
  truths:
    - "A 2-config x 3-cycle study runs exactly 6 experiments (not 18) — apply_cycles() called once, not twice"
    - "Manifest entries for cycles 2+ transition correctly through running -> completed/failed — per-config_hash cycle counter replaces hardcoded cycle=1"
    - "_build_entries() deduplicates study.experiments by config_hash before multiplying by n_cycles — a 2-config x 3-cycle study produces exactly 6 manifest entries (not 18)"
    - "StudyManifest.status is 'completed' after a successful study run — ManifestWriter.mark_study_completed() exists and is called by _run()"
    - "All existing multi-cycle and manifest tests pass with the fixes applied"
  artifacts:
    - path: "src/llenergymeasure/study/runner.py"
      provides: "apply_cycles() removed, per-config_hash cycle counters"
      contains: "_cycle_counters"
    - path: "src/llenergymeasure/study/manifest.py"
      provides: "_build_entries() deduplicates by config_hash; mark_study_completed() method"
      contains: "mark_study_completed"
    - path: "src/llenergymeasure/_api.py"
      provides: "_run() calls manifest.mark_study_completed() on success"
      contains: "mark_study_completed"
    - path: "tests/unit/test_study_runner.py"
      provides: "Tests for correct experiment count and cycle tracking"
      contains: "test_multi_cycle_correct_experiment_count"
    - path: "tests/unit/test_study_manifest.py"
      provides: "Tests for mark_study_completed() and _build_entries() correct entry count"
      contains: "test_build_entries_deduplicates_cycled_experiments"
  key_links:
    - from: "src/llenergymeasure/_api.py"
      to: "src/llenergymeasure/study/manifest.py"
      via: "ManifestWriter.mark_study_completed() called after successful run"
      pattern: "manifest\\.mark_study_completed"
    - from: "src/llenergymeasure/study/runner.py"
      to: "src/llenergymeasure/study/manifest.py"
      via: "mark_running/mark_completed/mark_failed with correct cycle numbers"
      pattern: "self\\.manifest\\.mark_(running|completed|failed).*cycle"
    - from: "src/llenergymeasure/study/manifest.py"
      to: "src/llenergymeasure/study/manifest.py"
      via: "_build_entries() deduplicates experiments by config_hash before cycling"
      pattern: "seen_hashes"
---

<objective>
Fix the 4 integration defects that break multi-cycle study execution: (1) double apply_cycles() in runner causing 3x experiment inflation, (1b) _build_entries() in manifest also double-multiplying by n_cycles (same root cause — experiments list already cycled), (2) hardcoded cycle=1 causing KeyError for cycles 2+, (3) missing mark_study_completed() leaving manifest status stuck at "running".

Purpose: Multi-cycle studies are the core M2 workflow. All bugs were found by M2 audit and together make multi-cycle execution non-functional. The fixes are targeted (removing code, deduplicating entries, adding counters, adding one method + one call).

Output: Working multi-cycle execution with correct experiment counts, correct manifest entry counts, per-cycle manifest tracking, and "completed" manifest status.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/llenergymeasure/config/loader.py (load_study_config — lines 184-198):
```python
# BUG 1 SITE: apply_cycles() called here during YAML loading
ordered = apply_cycles(
    valid_experiments,
    n_cycles=execution.n_cycles,
    cycle_order=CycleOrder(execution.cycle_order),
    study_design_hash=study_hash,
    shuffle_seed=execution.shuffle_seed,
)

return StudyConfig(
    experiments=ordered,  # <- already cycled: 2 configs x 3 cycles = 6 entries
    ...
)
```

From src/llenergymeasure/study/runner.py (StudyRunner.run — lines 237-245):
```python
# BUG 1 SITE: apply_cycles() called AGAIN on already-cycled list
ordered = apply_cycles(
    self.study.experiments,  # <- already 6 entries (2 configs x 3 cycles)
    self.study.execution.n_cycles,
    CycleOrder(self.study.execution.cycle_order),
    self.study.study_design_hash or "",
    self.study.execution.shuffle_seed,
)
# Result: 6 x 3 = 18 experiments instead of 6
```

From src/llenergymeasure/study/runner.py (_run_one — line 323):
```python
# BUG 2 SITE: cycle hardcoded to 1
cycle = 1  # cycle tracking deferred to Phase 12 wiring
# For the 2nd run of config A, this tries to mark_running(hash_A, cycle=1)
# but cycle=1 for hash_A is already "completed" -> KeyError
```

From src/llenergymeasure/study/manifest.py (ManifestWriter — no mark_study_completed):
```python
# BUG 3: No mark_study_completed() method exists
# ManifestWriter has mark_running, mark_completed, mark_failed, mark_interrupted
# But no method to set the overall study status to "completed"
```

From src/llenergymeasure/_api.py (_run — lines 152-212):
```python
# BUG 3 SITE: no call to mark_study_completed after runner finishes
# After _run_via_runner() returns, _run() assembles StudyResult but never
# tells the manifest writer the study is done. Manifest stays status="running".
```

From src/llenergymeasure/study/manifest.py (ManifestWriter._build_entries — lines 233-252):
```python
@staticmethod
def _build_entries(study: StudyConfig) -> list[ExperimentManifestEntry]:
    """Build pending entries for all (experiment, cycle) combinations."""
    entries: list[ExperimentManifestEntry] = []
    n_cycles = study.execution.n_cycles
    for exp in study.experiments:
        config_hash = compute_measurement_config_hash(exp)
        summary = build_config_summary(exp)
        for cycle in range(1, n_cycles + 1):
            entries.append(
                ExperimentManifestEntry(
                    config_hash=config_hash, config_summary=summary,
                    cycle=cycle, status="pending",
                )
            )
    return entries
```
Note: _build_entries iterates study.experiments (already cycled by loader) and then
multiplies by n_cycles AGAIN. This creates 6 * 3 = 18 entries instead of 6. This is
a consequence of Bug 1 — once runner.run() stops double-cycling, this method also
needs to work with the pre-cycled experiment list. Since the experiments are already
cycled, _build_entries should use the UNIQUE experiments (deduplicated by config_hash)
and n_cycles from execution to build entries.

From src/llenergymeasure/study/grid.py (apply_cycles — lines 164-191):
```python
def apply_cycles(experiments, n_cycles, cycle_order, study_design_hash, shuffle_seed=None):
    """Return ordered execution sequence for n_cycles repetitions."""
    if cycle_order == CycleOrder.SEQUENTIAL:
        return [exp for exp in experiments for _ in range(n_cycles)]
    if cycle_order == CycleOrder.INTERLEAVED:
        return experiments * n_cycles
    # shuffled: ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix double apply_cycles and cycle tracking in runner.py</name>
  <files>
    src/llenergymeasure/study/runner.py
    tests/unit/test_study_runner.py
  </files>
  <action>
    **Understanding the bug chain:** `load_study_config()` in `loader.py` already calls `apply_cycles()` on the unique experiments and stores the result in `StudyConfig.experiments`. So `study.experiments` arriving in `StudyRunner.run()` is ALREADY the fully-ordered execution sequence (e.g., 6 entries for 2 configs x 3 cycles). `runner.py` then calls `apply_cycles()` AGAIN, tripling the count to 18. Additionally, `_run_one()` hardcodes `cycle=1`, so manifest lookups fail for cycles 2+.

    **Fix 1 — Remove duplicate apply_cycles() from runner.py (STU-07):**

    In `StudyRunner.run()` (lines 237-245), remove the entire `apply_cycles()` call block:
    ```python
    # REMOVE THIS BLOCK:
    from llenergymeasure.study.grid import CycleOrder, apply_cycles
    ordered = apply_cycles(
        self.study.experiments,
        self.study.execution.n_cycles,
        CycleOrder(self.study.execution.cycle_order),
        self.study.study_design_hash or "",
        self.study.execution.shuffle_seed,
    )
    ```

    Replace with simply using the already-ordered list:
    ```python
    ordered = self.study.experiments
    ```

    Also update `n_unique` (line 274) which is used for cycle gap detection. Currently `n_unique = len(self.study.experiments)` — after the fix this should be the count of UNIQUE configs. Compute it as the number of distinct `config_hash` values:
    ```python
    from llenergymeasure.domain.experiment import compute_measurement_config_hash
    seen_hashes = {compute_measurement_config_hash(c) for c in self.study.experiments}
    n_unique = len(seen_hashes)
    ```
    This ensures cycle gap logic still fires after each complete round of unique configs.

    **Fix 2 — Per-config_hash cycle counters in _run_one() (STU-08):**

    Remove the hardcoded `cycle = 1` (line 323). Instead, add a `_cycle_counters` dict to `StudyRunner.__init__`:
    ```python
    self._cycle_counters: dict[str, int] = {}
    ```

    In `_run_one()`, replace `cycle = 1` with:
    ```python
    current = self._cycle_counters.get(config_hash, 0) + 1
    self._cycle_counters[config_hash] = current
    cycle = current
    ```

    This means the first time config_hash "abc" is seen, cycle=1. The second time, cycle=2. The third time, cycle=3. This matches exactly what `_build_entries` creates: entries for (config_hash, cycle=1), (config_hash, cycle=2), etc.

    **Add tests:**

    In `tests/unit/test_study_runner.py`, add the following tests:

    1. `test_multi_cycle_correct_experiment_count()`:
       Build a 2-config x 3-cycle `StudyConfig` (where `experiments` is already the cycled list from `apply_cycles`, i.e. 6 entries). Create a mock context like the existing tests. Run `StudyRunner.run()` and assert `len(results) == 6` (not 18). Assert `ctx.Process.call_count == 6`.

       ```python
       def test_multi_cycle_correct_experiment_count():
           """2-config x 3-cycle study runs exactly 6 experiments, not 18."""
           from llenergymeasure.study.grid import CycleOrder, apply_cycles

           exp_a = ExperimentConfig(model="model-a", backend="pytorch", n=10)
           exp_b = ExperimentConfig(model="model-b", backend="pytorch", n=10)
           ordered = apply_cycles([exp_a, exp_b], 3, CycleOrder.INTERLEAVED, "aabb0011", None)
           assert len(ordered) == 6  # sanity check

           study = StudyConfig(
               experiments=ordered,
               name="count-test",
               execution=ExecutionConfig(n_cycles=3, cycle_order="interleaved"),
               study_design_hash="aabb0011",
           )
           manifest = MagicMock()
           fake_result = {"status": "ok"}
           # ... (use _make_mock_process and _make_mock_context pattern from existing tests)
           # Assert len(results) == 6
           # Assert ctx.Process.call_count == 6
       ```

    2. `test_cycle_counter_increments_per_config_hash()`:
       Build a 2-config x 2-cycle interleaved study (A, B, A, B). Mock the manifest. Run. Assert that `manifest.mark_running` was called 4 times. Check the `cycle` argument in each call: first call for hash_A should have cycle=1, first call for hash_B should have cycle=1, second call for hash_A should have cycle=2, second call for hash_B should have cycle=2.

       ```python
       def test_cycle_counter_increments_per_config_hash():
           """Cycle counter increments per config_hash, not globally."""
           from llenergymeasure.domain.experiment import compute_measurement_config_hash
           from llenergymeasure.study.grid import CycleOrder, apply_cycles

           exp_a = ExperimentConfig(model="model-a", backend="pytorch", n=10)
           exp_b = ExperimentConfig(model="model-b", backend="pytorch", n=10)
           ordered = apply_cycles([exp_a, exp_b], 2, CycleOrder.INTERLEAVED, "aabb0011", None)

           study = StudyConfig(
               experiments=ordered,
               name="cycle-test",
               execution=ExecutionConfig(n_cycles=2, cycle_order="interleaved"),
               study_design_hash="aabb0011",
           )
           manifest = MagicMock()
           # ... (same mock pattern)
           # After runner.run():
           # Extract cycle args from manifest.mark_running.call_args_list
           # For hash_A: cycles should be [1, 2]
           # For hash_B: cycles should be [1, 2]
       ```

    Use the same mock infrastructure pattern from existing tests (`_make_mock_process`, `_make_mock_context`, `patch("multiprocessing.get_context")`). For multi-experiment tests, use a factory approach (like `test_interleaved_ordering`) where `ctx.Process.side_effect` creates fresh mock processes for each call, and `ctx.Pipe.side_effect` creates fresh pipe pairs.
  </action>
  <verify>
    <automated>pytest tests/unit/test_study_runner.py -x -q</automated>
  </verify>
  <done>
    - `apply_cycles()` is NOT called in `runner.py` — only `ordered = self.study.experiments`
    - `_cycle_counters` dict on `StudyRunner` tracks per-config_hash cycle numbers
    - `test_multi_cycle_correct_experiment_count` passes — 6 experiments, not 18
    - `test_cycle_counter_increments_per_config_hash` passes — cycles 1, 2 tracked correctly
    - All existing runner tests pass without changes
  </done>
</task>

<task type="auto">
  <name>Task 2: Add mark_study_completed() and wire it in _run()</name>
  <files>
    src/llenergymeasure/study/manifest.py
    src/llenergymeasure/_api.py
    tests/unit/test_study_manifest.py
  </files>
  <action>
    **Fix 3a — Fix _build_entries() over-inflation in manifest.py (consequence of Bug 1):**

    `_build_entries()` iterates `study.experiments` which — after the loader's `apply_cycles()` — is already the cycled list (e.g. 6 entries for 2 configs x 3 cycles). It then multiplies by `n_cycles` again in its inner loop, producing 18 entries instead of 6. This is a companion bug to Bug 1 in the runner.

    In `src/llenergymeasure/study/manifest.py`, rewrite `_build_entries()` (lines 233-252) to deduplicate `study.experiments` by `config_hash` before iterating:

    ```python
    @staticmethod
    def _build_entries(study: StudyConfig) -> list[ExperimentManifestEntry]:
        """Build pending entries for all (experiment, cycle) combinations."""
        from llenergymeasure.domain.experiment import compute_measurement_config_hash

        # study.experiments is already the cycled execution list from apply_cycles().
        # Deduplicate by config_hash to get unique configs, then multiply by n_cycles.
        seen: dict[str, ExperimentConfig] = {}
        for exp in study.experiments:
            h = compute_measurement_config_hash(exp)
            if h not in seen:
                seen[h] = exp

        entries: list[ExperimentManifestEntry] = []
        n_cycles = study.execution.n_cycles
        for config_hash, exp in seen.items():
            summary = build_config_summary(exp)
            for cycle in range(1, n_cycles + 1):
                entries.append(
                    ExperimentManifestEntry(
                        config_hash=config_hash,
                        config_summary=summary,
                        cycle=cycle,
                        status="pending",
                    )
                )
        return entries
    ```

    This ensures a 2-config x 3-cycle study produces exactly 6 manifest entries: (hash_A, cycle=1), (hash_A, cycle=2), (hash_A, cycle=3), (hash_B, cycle=1), (hash_B, cycle=2), (hash_B, cycle=3).

    **Fix 3b — Add mark_study_completed() to ManifestWriter (STU-09):**

    In `src/llenergymeasure/study/manifest.py`, add a `mark_study_completed()` method to `ManifestWriter` after `mark_interrupted()` (around line 188):

    ```python
    def mark_study_completed(self) -> None:
        """Set manifest status to 'completed' and record completion time.

        Called by _run() after all experiments finish successfully (no SIGINT).
        """
        self.manifest = self.manifest.model_copy(
            update={
                "status": "completed",
                "completed_at": datetime.now(timezone.utc),
            }
        )
        self._write()
    ```

    This follows the exact same pattern as `mark_interrupted()` — using `model_copy(update=...)` then `_write()`. It sets both `status` and `completed_at` (the `completed_at` field already exists on `StudyManifest` as `datetime | None = None`).

    **Wire mark_study_completed() in _run() (STU-09):**

    In `src/llenergymeasure/_api.py`, in the `_run()` function, after the runner returns and before assembling `StudyResult`, call `manifest.mark_study_completed()`. Add the call around line 182 (after `wall_time = ...`):

    ```python
    wall_time = time.monotonic() - wall_start

    # Mark manifest as completed (STU-09)
    manifest.mark_study_completed()

    completed = sum(...)
    ```

    This call is reached only if the study was NOT interrupted (SIGINT causes `sys.exit(130)` inside `StudyRunner.run()` before returning). So the manifest transitions: running -> completed on success, running -> interrupted on SIGINT.

    **Add tests:**

    In `tests/unit/test_study_manifest.py`, add:

    1. `test_build_entries_deduplicates_cycled_experiments()`:
       Build a 2-config x 3-cycle `StudyConfig` (where `experiments` is already the cycled list from `apply_cycles`, i.e. 6 entries). Create a `ManifestWriter`. Assert that `len(writer.manifest.experiments) == 6` (not 18). Assert that for each unique config_hash, cycles `[1, 2, 3]` are present.

       ```python
       def test_build_entries_deduplicates_cycled_experiments(tmp_path: Path) -> None:
           """_build_entries() produces exactly n_unique * n_cycles entries, not len(experiments) * n_cycles."""
           from llenergymeasure.study.grid import CycleOrder, apply_cycles

           exp_a = ExperimentConfig(model="model-a", backend="pytorch", n=10)
           exp_b = ExperimentConfig(model="model-b", backend="pytorch", n=10)
           ordered = apply_cycles([exp_a, exp_b], 3, CycleOrder.INTERLEAVED, "aabb0011", None)
           assert len(ordered) == 6  # sanity: already cycled

           study = StudyConfig(
               experiments=ordered,
               name="dedup-test",
               execution=ExecutionConfig(n_cycles=3, cycle_order="interleaved"),
               study_design_hash="aabb0011",
           )
           writer = ManifestWriter(study=study, study_dir=tmp_path)

           # Should be 6 entries (2 configs x 3 cycles), NOT 18
           assert len(writer.manifest.experiments) == 6

           # Each config_hash should have cycles [1, 2, 3]
           from collections import defaultdict
           cycles_by_hash: dict[str, list[int]] = defaultdict(list)
           for entry in writer.manifest.experiments:
               cycles_by_hash[entry.config_hash].append(entry.cycle)
           assert len(cycles_by_hash) == 2  # 2 unique configs
           for cycles in cycles_by_hash.values():
               assert sorted(cycles) == [1, 2, 3]
       ```

    2. `test_mark_study_completed_sets_status()`:
       Create a `ManifestWriter` with a small study. Call `mark_study_completed()`. Assert `writer.manifest.status == "completed"`. Read `manifest.json` from disk and assert `data["status"] == "completed"`. Assert `data["completed_at"] is not None`.

       ```python
       def test_mark_study_completed(tmp_path: Path) -> None:
           """mark_study_completed() sets status='completed' and records completed_at."""
           study = _make_study(n_experiments=1, n_cycles=1)
           writer = ManifestWriter(study=study, study_dir=tmp_path)

           writer.mark_study_completed()

           # In-memory check
           assert writer.manifest.status == "completed"
           assert writer.manifest.completed_at is not None

           # On-disk check
           data = json.loads((tmp_path / "manifest.json").read_text())
           assert data["status"] == "completed"
           assert data["completed_at"] is not None
       ```

    3. `test_manifest_status_after_all_experiments_complete()`:
       Create a writer, run through the full lifecycle (mark_running then mark_completed for each experiment entry), then call mark_study_completed(). Assert final status is "completed" and pending count is 0.

    Run the full manifest test suite to verify no regressions.
  </action>
  <verify>
    <automated>pytest tests/unit/test_study_manifest.py tests/unit/test_study_runner.py -x -q</automated>
  </verify>
  <done>
    - `_build_entries()` deduplicates `study.experiments` by `config_hash` before cycling — 2 configs x 3 cycles = 6 entries, not 18
    - `test_build_entries_deduplicates_cycled_experiments` passes — correct entry count and per-hash cycle lists
    - `ManifestWriter.mark_study_completed()` exists and sets status="completed" + completed_at
    - `_run()` in `_api.py` calls `manifest.mark_study_completed()` after runner finishes
    - `test_mark_study_completed` passes — status and completed_at are correct on disk
    - All existing manifest tests pass without changes
  </done>
</task>

</tasks>

<verification>
Run full unit test suite to confirm no regressions:

```bash
pytest tests/unit/ -x -q
```

Spot-check the 5 success criteria:

1. **Correct experiment count:** Grep `runner.py` to confirm NO `apply_cycles` import or call — only `ordered = self.study.experiments`
2. **Correct manifest entries:** Grep `manifest.py` for `seen` in `_build_entries` — deduplicates by config_hash before cycling
3. **Cycle counters:** Grep `runner.py` for `_cycle_counters` — dict that tracks per-config_hash cycle numbers
4. **mark_study_completed:** Grep `manifest.py` for `def mark_study_completed` — method exists
5. **Wired in _run():** Grep `_api.py` for `mark_study_completed` — called after runner returns

```bash
# Verification commands
grep -n "apply_cycles" src/llenergymeasure/study/runner.py  # should return 0 lines
grep -n "seen" src/llenergymeasure/study/manifest.py  # should find dedup dict in _build_entries
grep -n "_cycle_counters" src/llenergymeasure/study/runner.py  # should find dict init and usage
grep -n "mark_study_completed" src/llenergymeasure/study/manifest.py  # should find method def
grep -n "mark_study_completed" src/llenergymeasure/_api.py  # should find call
```
</verification>

<success_criteria>
- A 2-config x 3-cycle study runs exactly 6 experiments (not 18)
- ManifestWriter._build_entries() produces exactly 6 entries for a 2-config x 3-cycle study (not 18) — deduplicates by config_hash
- Manifest entries for cycles 2+ transition correctly through running -> completed/failed
- StudyManifest.status is "completed" after a successful study run
- All existing multi-cycle and manifest tests pass with the fixes applied
- No regressions in the full unit test suite
</success_criteria>

<output>
After completion, create `.planning/phases/14-multi-cycle-execution-fixes/14-01-SUMMARY.md`
</output>
