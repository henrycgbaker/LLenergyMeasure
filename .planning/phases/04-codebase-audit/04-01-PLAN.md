---
phase: 04-codebase-audit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/04-codebase-audit/04-01-REPORT-automated-analysis.md
autonomous: true

must_haves:
  truths:
    - "Vulture and/or deadcode have been run against the codebase, producing a list of unused code with confidence scores"
    - "Ruff complexity metrics have been collected for all modules, identifying high-complexity functions"
    - "Industry CLI surface comparison completed via web research of lm-eval-harness, vLLM benchmarks, nanoGPT, LLMPerf, GenAI-Perf"
    - "All findings documented with module-level references and severity classification (remove/simplify/keep)"
  artifacts:
    - path: ".planning/phases/04-codebase-audit/04-01-REPORT-automated-analysis.md"
      provides: "Automated dead code findings + industry CLI comparison"
  key_links: []
---

<objective>
Run automated analysis tools against the codebase and research industry CLI patterns to establish the evidence base for the full audit.

Purpose: Provides the data-driven foundation for all subsequent audit plans. Automated tools surface dead code and complexity hotspots objectively; industry research establishes the "industry does X, we do Y" comparison baseline.

Output: Report section covering (1) dead/unused code findings from vulture/deadcode, (2) complexity hotspots from ruff, (3) industry CLI surface comparison table, (4) stub detection results.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/agents/gsd-planner.md
</execution_context>

<context>
@.planning/phases/04-codebase-audit/04-CONTEXT.md
@.planning/phases/04-codebase-audit/04-RESEARCH.md
@.planning/ROADMAP.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run automated dead code and complexity analysis</name>
  <files>.planning/phases/04-codebase-audit/04-01-REPORT-automated-analysis.md</files>
  <action>
Run the following automated analysis tools against src/llenergymeasure/:

1. **Dead code detection (vulture):**
   ```bash
   pip install vulture 2>/dev/null
   vulture src/llenergymeasure/ --min-confidence 60
   ```
   Capture ALL output. Group findings by module directory (cli/, config/, core/, domain/, orchestration/, results/, state/, top-level).

2. **Dead code detection (deadcode):**
   ```bash
   pip install deadcode 2>/dev/null
   deadcode src/llenergymeasure/
   ```
   Capture output and cross-reference with vulture findings. Note items found by both tools (higher confidence).

3. **Complexity analysis (ruff):**
   ```bash
   ruff check src/llenergymeasure/ --select C901 --statistics
   ```
   List all functions exceeding complexity threshold. Also run:
   ```bash
   ruff check src/llenergymeasure/ --select C901 --output-format json
   ```
   To get exact locations.

4. **Stub detection:**
   Search for stub patterns across the codebase:
   - Functions with only `pass` body
   - Functions with only `raise NotImplementedError`
   - Functions with only `...` (Ellipsis) body
   - TODO/FIXME/HACK comments
   Use grep/ripgrep for these patterns.

5. **Unused imports:**
   ```bash
   ruff check src/llenergymeasure/ --select F401 --statistics
   ```

Document ALL findings in structured tables with columns: File | Finding | Tool | Confidence | Severity (remove/simplify/keep).

For each finding, note whether it's likely genuinely dead vs intentionally staged (e.g., protocol methods that must exist for interface compliance).
  </action>
  <verify>
Report file exists and contains sections for: vulture results, deadcode results, complexity hotspots, stub detection, unused imports. Each section has structured findings with module references.
  </verify>
  <done>Automated tools have been run, all output captured and classified by module and severity.</done>
</task>

<task type="auto">
  <name>Task 2: Industry CLI surface comparison via web research</name>
  <files>.planning/phases/04-codebase-audit/04-01-REPORT-automated-analysis.md</files>
  <action>
Research the CLI command surface of comparable tools. For each tool, search its GitHub repository and documentation to catalogue:

**Tools to research (search GitHub repos and docs):**
1. **lm-eval-harness** — Search https://github.com/EleutherAI/lm-evaluation-harness for CLI commands, entry points, subcommand structure
2. **vLLM benchmarks** — Search https://github.com/vllm-project/vllm/tree/main/benchmarks and https://docs.vllm.ai for benchmark CLI
3. **nanoGPT** — Search https://github.com/karpathy/nanoGPT for entry points and scripts
4. **LLMPerf** — Search https://github.com/ray-project/llmperf for CLI and entry points
5. **GenAI-Perf** — Search https://github.com/triton-inference-server/perf_analyzer for CLI subcommands

**For each tool, capture:**
- Number of CLI commands/subcommands
- Main entry points (what the user actually runs)
- Config format (YAML, CLI args, Python files)
- Whether it has campaign/grid/sweep built-in
- Whether it has result aggregation built-in
- Whether it has a doctor/diagnostic command
- Whether it has an init/setup wizard
- Docker usage pattern (for tool vs for serving)

**Then compare against our tool:**
Run `lem --help` and catalogue all our commands and subcommands. Count them.

**Produce a comparison table** in "industry does X, we do Y" format:
| Aspect | Industry Norm | Our Tool | Assessment |
|--------|---------------|----------|------------|
| Total commands | 2-4 main commands | ? | ? |
| Campaign/grid | External scripting | Built-in | Over-engineered? |
| etc. | ... | ... | ... |

Append this comparison table to the report file created in Task 1.
  </action>
  <verify>
Report contains an industry comparison table with at least 5 tools compared across at least 8 dimensions. Each row has a clear assessment.
  </verify>
  <done>Industry CLI comparison complete with evidence-based "industry does X, we do Y" format for all major CLI aspects.</done>
</task>

</tasks>

<verification>
- Report file exists at .planning/phases/04-codebase-audit/04-01-REPORT-automated-analysis.md
- Contains automated tool output (vulture, deadcode, ruff complexity, stub detection, unused imports)
- Contains industry CLI comparison table with 5+ tools
- All findings have severity classification (remove/simplify/keep)
- Module-level references (not exact line numbers)
</verification>

<success_criteria>
- Dead code detection tools have been run and output captured
- Complexity hotspots identified with function names and locations
- Industry comparison table provides evidence base for CLI simplification recommendations
- All findings are structured and ready for integration into final audit report
</success_criteria>

<output>
After completion, create `.planning/phases/04-codebase-audit/04-01-SUMMARY.md`
</output>
