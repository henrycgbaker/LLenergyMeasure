---
phase: 04-codebase-audit
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/04-codebase-audit/04-05-REPORT-infra-tests-xref.md
autonomous: true

must_haves:
  truths:
    - "Docker infrastructure (Dockerfiles, compose, scripts, entrypoints) assessed for correctness and necessity"
    - "Detection systems (docker_detection, backend_detection, env_setup) evaluated for overlap and unification potential"
    - "Test suite quality assessed (weak assertions, tests for removed features, coverage gaps)"
    - "Planning doc cross-reference completed (features promised in Phases 1-3 vs implementation)"
    - "Documentation staleness checked (CLAUDE.md files, READMEs, user docs)"
  artifacts:
    - path: ".planning/phases/04-codebase-audit/04-05-REPORT-infra-tests-xref.md"
      provides: "Infrastructure, tests, and planning cross-reference audit"
  key_links: []
---

<objective>
Audit Docker infrastructure, detection systems, test quality, documentation accuracy, and cross-reference planning docs against implementation.

Purpose: Infrastructure and tests form the reliability foundation. Documentation accuracy affects user trust. Planning cross-reference ensures no features were lost in translation across 10+ phases.

Output: Report section covering (1) Docker infrastructure assessment, (2) detection system overlap analysis, (3) test quality audit, (4) planning document cross-reference, (5) documentation staleness check.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/agents/gsd-planner.md
</execution_context>

<context>
@.planning/phases/04-codebase-audit/04-CONTEXT.md
@.planning/phases/04-codebase-audit/04-RESEARCH.md
@.planning/ROADMAP.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Docker infrastructure and detection systems audit</name>
  <files>.planning/phases/04-codebase-audit/04-05-REPORT-infra-tests-xref.md</files>
  <action>
1. **Docker infrastructure assessment:**
   Read and assess:
   - `docker/Dockerfile.base` — Base image
   - `docker/Dockerfile.pytorch` — PyTorch image
   - `docker/Dockerfile.vllm` — vLLM image
   - `docker/Dockerfile.tensorrt` — TensorRT image
   - `docker-compose.yml` — Service definitions
   - `scripts/entrypoint.sh` — Container entrypoint
   - `scripts/docker-experiment.sh` — Docker experiment runner
   - `scripts/dev-entrypoint.sh` — Dev entrypoint

   For each: Is it up-to-date? Does it reference correct package versions? Is it functional?

   **Industry comparison:**
   - How does vLLM structure its Docker setup? (web search vLLM Docker)
   - How does lm-eval-harness handle Docker? (web search)
   - "Industry does X, we do Y" for Docker patterns

   **Docker-only model evaluation** (from CONTEXT.md):
   - Would moving all backends to Docker-only simplify execution?
   - Count distinct execution paths (local vs Docker)
   - Assessment with evidence

2. **Detection systems overlap analysis:**
   Read these three modules and trace their usage:
   - `config/docker_detection.py` (58 lines) — Running-in-Docker detection
   - `config/backend_detection.py` (58 lines) — Backend availability detection
   - `config/env_setup.py` (68 lines) — Environment setup (.env generation)

   For each:
   - What does it detect/do?
   - Where is it imported and called?
   - Does it overlap with other detection modules?
   - Could these be unified into a single detection module?

   Also check:
   - `cli/doctor.py` (236 lines) — Diagnostic command (does it duplicate detection logic?)
   - `core/gpu_info.py` (482 lines) — GPU detection (does it overlap with backend detection?)

   **Produce a detection system flow diagram:**
   ```
   User action -> which detection runs -> what it checks -> outcome
   ```

   **Unification methodology:** For each detection module, document:
   (1) What it detects
   (2) When in the execution lifecycle it runs
   (3) What decisions depend on its output
   Create a dependency diagram showing if detection outputs feed into each other (sequential) or are consumed independently (parallel). Only recommend unification if modules are sequential in the same concern area — don't merge orthogonal detections.

   Identify overlapping checks and recommend unification where appropriate.

3. **Scripts directory audit:**
   - `scripts/generate_param_matrix.py` — Used? By what?
   - `scripts/generate_config_docs.py` — Used? By what?
   - `scripts/generate_invalid_combos_doc.py` — Used? By what?
   - `scripts/runtime-test-orchestrator.py` — Used? By what?
   - `scripts/run-runtime-tests.sh` — Used? By what?
   - `scripts/test_cuda_visible_devices.py` — Used? By what?
   - `scripts/test_multi_gpu_parallelization.py` — Used? By what?
   For each: Is it functional, documented, and maintained?

Document all findings with severity classifications.
  </action>
  <verify>
Report contains Docker infrastructure assessment with industry comparison, detection system flow diagram with overlap analysis, and scripts directory audit.
  </verify>
  <done>Docker infrastructure and detection systems assessed, overlap identified, unification recommendations made with evidence.</done>
</task>

<task type="auto">
  <name>Task 2: Test quality audit and planning cross-reference</name>
  <files>.planning/phases/04-codebase-audit/04-05-REPORT-infra-tests-xref.md</files>
  <action>
1. **Test quality audit:**
   Scan all test files in tests/ directory:

   a. **Weak assertion detection:**
      Search for tests with:
      - No assert statements at all (test that just runs code without checking)
      - `assert True` or `assert 1` (always-passing assertions)
      - Only `assertIsNotNone` or `assertIsInstance` (type-checking only, not behavior)
      - Tests that catch all exceptions and pass silently

   b. **Tests for removed/dead features:**
      Cross-reference test file names against source modules.
      If a test tests a module that was removed or substantially changed, flag it.

   c. **Test coverage gaps:**
      Check which source modules have NO corresponding test file.
      Use naming convention: src/.../module.py -> tests/unit/test_module.py

   d. **Test structure assessment:**
      - How many test files? How many test functions?
      - Are tests properly isolated (no shared state)?
      - Are fixtures appropriate (conftest.py)?
      - Runtime tests vs unit tests vs integration vs e2e — is the split sensible?

   Produce a test quality summary:
   | Category | Count | Examples |
   |----------|-------|---------|
   | Tests with no assertions | ? | ... |
   | Tests for dead code | ? | ... |
   | Untested modules | ? | ... |

2. **Planning document cross-reference:**
   Cross-reference ROADMAP.md Phase 1-3 success criteria against actual implementation:

   For each Phase (1, 2, 2.1, 2.2, 2.3, 2.4, 3), check each success criterion:
   - Is the feature implemented in code?
   - Is it wired end-to-end?
   - Does it work as described?

   Focus on finding:
   - Features promised but not implemented
   - Features partially implemented (stub or incomplete)
   - Features implemented but not matching spec
   - Features that were superseded by later decisions

   Produce a cross-reference table:
   | Phase | Success Criterion | Implemented? | Evidence |

3. **Documentation staleness check:**
   Quickly scan:
   - All CLAUDE.md files in src/ subdirectories
   - docs/*.md files
   - Module README.md files

   For each: Does it reference current module structure, or is it stale?
   Flag any docs that reference removed code, old command names, or outdated architecture.

Append all findings to the report.
  </action>
  <verify>
Report contains test quality summary with weak assertion counts, planning cross-reference table covering all Phase 1-3 success criteria, and documentation staleness flags.
  </verify>
  <done>Test quality assessed, planning promises cross-referenced against implementation, documentation staleness identified.</done>
</task>

</tasks>

<verification>
- Report file exists at .planning/phases/04-codebase-audit/04-05-REPORT-infra-tests-xref.md
- Docker infrastructure has per-file assessment
- Detection systems have overlap analysis with flow diagram
- Test quality has quantified findings (weak assertions, untested modules)
- Planning cross-reference covers all Phase 1-3 success criteria
- Documentation staleness flags present
</verification>

<success_criteria>
- Docker infrastructure assessed with industry comparison
- Detection system overlap quantified and unification recommendation made
- Test quality issues identified with specific examples
- No Phase 1-3 features lost in translation (or explicitly flagged as missing)
- Stale documentation identified for refresh
</success_criteria>

<output>
After completion, create `.planning/phases/04-codebase-audit/04-05-SUMMARY.md`
</output>
