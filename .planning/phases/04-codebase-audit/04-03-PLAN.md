---
phase: 04-codebase-audit
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/04-codebase-audit/04-03-REPORT-core-engine.md
autonomous: true

must_haves:
  truths:
    - "Each inference backend (PyTorch, vLLM, TensorRT) assessed for end-to-end functional completeness"
    - "Each backend compared against its native upstream patterns (vLLM benchmark_serving.py, TensorRT trtllm-bench, PyTorch HuggingFace pipeline)"
    - "Docker execution paths audited per-backend given all 3 backends are currently broken in Docker"
    - "Shared backend code evaluated for duplication and abstraction quality"
    - "Measurement primitives (energy backends, power/thermal, baseline, warmup) assessed for correctness and integration"
    - "Core utility modules (FLOPs, GPU info, distributed, traffic, prompts, dataset loader) evaluated for necessity and quality"
    - "All findings documented with industry comparison evidence"
  artifacts:
    - path: ".planning/phases/04-codebase-audit/04-03-REPORT-core-engine.md"
      provides: "Core engine audit covering backends, metrics, and measurement"
  key_links: []
---

<objective>
Audit the core inference engine, measurement primitives, and supporting modules for completeness, correctness, and unnecessary complexity.

Purpose: The core engine is where actual inference and measurement happens. Backend completeness determines whether the tool actually works end-to-end. Measurement accuracy is the tool's primary value proposition.

Output: Report section covering (1) per-backend completeness assessment, (2) measurement system audit, (3) core utility module assessment, (4) code duplication and abstraction quality analysis.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/agents/gsd-planner.md
</execution_context>

<context>
@.planning/phases/04-codebase-audit/04-CONTEXT.md
@.planning/phases/04-codebase-audit/04-RESEARCH.md
@.planning/ROADMAP.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Inference backend audit</name>
  <files>.planning/phases/04-codebase-audit/04-03-REPORT-core-engine.md</files>
  <action>
**CRITICAL CONTEXT:** As of Phase 4, all three backends are broken in Docker (PyTorch hangs with CUDA driver init failure, vLLM worker processes crash, TensorRT routes to wrong container). For each backend, explicitly audit the Docker execution path (container entry -> backend init -> inference) and distinguish Docker-specific failures from general backend issues.

**BACKEND-NATIVE LENS:** For each backend, research how it works natively in its upstream project and compare our wrapper against native patterns. The goal is to identify where our abstractions diverge from how the backend is designed to be used, so Phase 5 can make each backend closer to its "native self."

Audit each inference backend for functional completeness and code quality:

1. **Backend protocol assessment:**
   Read `core/inference_backends/protocols.py` (342 lines) and evaluate:
   - What does the protocol require?
   - Is it too broad (methods that backends don't need)?
   - Is it too narrow (missing methods backends implement differently)?
   - Compare against how lm-eval-harness / vLLM structure their backend abstractions

2. **PyTorch backend** (`core/inference_backends/pytorch.py`, 1155 lines):
   - Trace full inference path: model load -> tokenize -> generate -> decode -> metrics
   - Identify dead code paths (unreachable branches, unused methods)
   - Assess streaming implementation complexity
   - Check for model kwargs that are defined but not passed through
   - Note any TODO/FIXME/HACK comments
   - Compare size to nanoGPT's inference code (~200 lines) — is complexity justified?
   - **Native comparison:** Research HuggingFace transformers pipeline/generate patterns (web search). Compare our PyTorch inference wrapper against standard HF usage. Produce "HuggingFace does X natively, we do Y" for: model loading, generation call, tokenisation, parameter passthrough.

3. **vLLM backend** (`core/inference_backends/vllm.py`, 1006 lines):
   - Same trace as PyTorch
   - Check vLLM-specific: engine initialization, sampling params, async generation
   - Is pynvml thread safety concern documented in CONTEXT addressed?
   - Note areas where vLLM's own API could replace custom code
   - **Native comparison:** Research vLLM's own `benchmarks/benchmark_serving.py` and `benchmarks/benchmark_throughput.py` (web search vLLM benchmarks GitHub). Compare our vLLM wrapper against how vLLM's own benchmarks call the engine. Produce "vLLM benchmarks do X natively, we do Y" for: engine init, sampling params, generation loop, result collection.
   - **Docker execution path:** How does vLLM's official Docker image work? Compare our docker-compose vllm service against vLLM's recommended container usage.

4. **TensorRT backend** (`core/inference_backends/tensorrt.py`, 1171 lines):
   - Same trace as PyTorch
   - Check TensorRT-specific: engine build, optimization profiles, runtime config
   - Assess whether TensorRT integration is functional end-to-end or partially stubbed
   - Note TensorRT-LLM version compatibility concerns
   - **Native comparison:** Research TensorRT-LLM's `trtllm-bench` benchmarking tool and `examples/` patterns (web search TensorRT-LLM benchmarking). Compare our TRT wrapper against native TRT-LLM usage. Produce "TRT-LLM does X natively, we do Y" for: model conversion/build, engine loading, generation, parameter handling.
   - **Docker execution path:** How does NVIDIA's TensorRT-LLM Docker image work? Compare our docker-compose tensorrt service against NVIDIA's recommended container setup.

5. **Shared backend code** (`core/inference_backends/shared.py`, 248 lines + adapters.py, 209 lines):
   - What's shared? Is it actually used by all backends?
   - Could more code be shared to reduce per-backend duplication?
   - Is the adapter pattern (adapters.py) adding value or unnecessary indirection?

6. **Backend __init__.py** (`core/inference_backends/__init__.py`, 147 lines):
   - Factory/registry pattern assessment
   - Is backend selection logic clean?

**For each backend produce:**
| Aspect | Status | Notes |
|--------|--------|-------|
| Model loading | functional/stub/broken | ... |
| Inference (non-streaming) | functional/stub/broken | ... |
| Inference (streaming) | functional/stub/broken | ... |
| Metrics collection | functional/stub/broken | ... |
| Config param passthrough | complete/partial/broken | ... |
| Error handling | adequate/minimal/missing | ... |

Research how vLLM's own benchmarks structure inference calls (web search vLLM benchmarks GitHub) and compare complexity.

**Produce a backend-native comparison table:**
| Backend | Aspect | Upstream Native Pattern | Our Implementation | Divergence | Justified? |
|---------|--------|------------------------|-------------------|------------|------------|
| PyTorch | Model load | ... | ... | ... | ... |
| PyTorch | Generation | ... | ... | ... | ... |
| vLLM | Engine init | ... | ... | ... | ... |
| vLLM | Sampling | ... | ... | ... | ... |
| TensorRT | Engine build | ... | ... | ... | ... |
| TensorRT | Generation | ... | ... | ... | ... |
  </action>
  <verify>
Report contains per-backend completeness tables, protocol assessment, shared code evaluation, and industry comparison for backend abstraction patterns.
  </verify>
  <done>All three backends assessed for end-to-end completeness, dead code identified, and complexity compared against industry patterns.</done>
</task>

<task type="auto">
  <name>Task 2: Measurement and core utility audit</name>
  <files>.planning/phases/04-codebase-audit/04-03-REPORT-core-engine.md</files>
  <action>
Audit measurement primitives and core utility modules:

1. **Energy measurement system:**
   - `core/energy_backends/` (base.py 8 lines, codecarbon.py 245 lines, __init__.py 110 lines)
   - Is CodeCarbon the only energy backend? Are others planned?
   - Is the abstraction (base.py at 8 lines) justified for a single implementation?
   - How does lm-eval-harness handle energy measurement? (web search)

2. **Power/thermal measurement:**
   - `core/power_thermal.py` (280 lines) — PowerThermalSampler
   - `core/baseline.py` (199 lines) — Baseline power measurement
   - Are these integrated end-to-end? Trace from config -> measurement -> results
   - Is sampling working correctly?

3. **Warmup system:**
   - `core/warmup.py` (171 lines) — CV-based convergence
   - Is this used? Trace from config -> execution -> convergence decision

4. **Extended metrics:**
   - `core/extended_metrics.py` (274 lines)
   - What metrics are collected? Which appear in results?
   - Null handling concerns from STATE.md — are they addressed?

5. **Compute metrics:**
   - `core/compute_metrics.py` (301 lines)
   - Core metric calculations — are all used in results output?

6. **Core utility modules — assess necessity:**
   - `core/flops.py` (340 lines) — FLOPs estimation. Is this used? By what?
   - `core/gpu_info.py` (482 lines) — GPU topology. Is this over-engineered? Compare to nvidia-smi parsing in other tools
   - `core/gpu_utilisation.py` (183 lines) — GPU util sampling. Used?
   - `core/distributed.py` (230 lines) — Multi-GPU support. Functional?
   - `core/model_loader.py` (292 lines) — Model loading. Duplicated with backend loaders?
   - `core/traffic.py` (142 lines) — Traffic patterns. What is this?
   - `core/prompts.py` (180 lines) — Prompt handling. Purpose and usage?
   - `core/dataset_loader.py` (209 lines) — Dataset loading. Working end-to-end?
   - `core/inference.py` (36 lines) — What's in here?
   - `core/implementations.py` (86 lines) — What's in here?
   - `core/environment.py` (247 lines) — Environment capture. Used in results?

For each module: Is it imported? By what? Does the import chain reach an actual execution path? Or is it orphaned?

**Produce a module necessity table:**
| Module | Lines | Imported By | Used In Execution | Status |
|--------|-------|-------------|-------------------|--------|

Append all findings to the report.
  </action>
  <verify>
Report contains measurement system assessment, per-module necessity table for all core utilities, and identification of orphaned/unused modules.
  </verify>
  <done>All core modules assessed for necessity, measurement chain traced end-to-end, unused modules identified.</done>
</task>

</tasks>

<verification>
- Report file exists at .planning/phases/04-codebase-audit/04-03-REPORT-core-engine.md
- All three backends have completeness tables
- Backend-native comparison table present (upstream does X, we do Y)
- Docker execution paths audited per-backend
- All core modules have necessity assessments
- Measurement chain (config -> energy sampling -> results) traced
- Industry comparisons included for backend architecture
</verification>

<success_criteria>
- Backend completeness known for all three backends
- Backend-native divergence documented for each backend
- Docker-specific vs general failures distinguished
- Dead code in backends identified
- Core modules classified as necessary/unnecessary with evidence
- Measurement system integrity verified or gaps identified
</success_criteria>

<output>
After completion, create `.planning/phases/04-codebase-audit/04-03-SUMMARY.md`
</output>
