---
phase: 12-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/domain/experiment.py
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/config/user_config.py
  - src/llenergymeasure/config/loader.py
  - src/llenergymeasure/study/runner.py
  - src/llenergymeasure/study/grid.py
  - src/llenergymeasure/orchestration/preflight.py
  - tests/unit/test_study_result.py
  - tests/unit/test_study_preflight.py
autonomous: true
requirements:
  - RES-13
  - CM-10

must_haves:
  truths:
    - "StudyResult has study_design_hash, measurement_protocol dict, result_files list, and summary (StudySummary)"
    - "StudySummary has total_experiments, completed, failed, total_wall_time_s, total_energy_j, warnings list"
    - "A study YAML with backend: [pytorch, vllm] raises PreFlightError with Docker runner message before any subprocess is spawned"
    - "config_gap_seconds is renamed to experiment_gap_seconds throughout the codebase"
  artifacts:
    - path: "src/llenergymeasure/domain/experiment.py"
      provides: "StudyResult full schema with StudySummary"
      contains: "class StudySummary"
    - path: "src/llenergymeasure/orchestration/preflight.py"
      provides: "Multi-backend study pre-flight check"
      contains: "run_study_preflight"
    - path: "tests/unit/test_study_result.py"
      provides: "StudyResult schema tests"
    - path: "tests/unit/test_study_preflight.py"
      provides: "Multi-backend pre-flight tests"
  key_links:
    - from: "src/llenergymeasure/orchestration/preflight.py"
      to: "llenergymeasure.exceptions.PreFlightError"
      via: "run_study_preflight raises PreFlightError for multi-backend"
      pattern: "PreFlightError.*Multi-backend"
---

<objective>
Upgrade StudyResult to the full RES-13 schema, rename config_gap_seconds to experiment_gap_seconds across all touchpoints, and add multi-backend pre-flight guard (CM-10).

Purpose: These are the foundational schema and validation changes that Plan 02 (core wiring) and Plan 03 (CLI) build upon. Without the full StudyResult schema, the runner cannot assemble results; without the pre-flight guard, multi-backend studies would reach the subprocess layer and fail cryptically.

Output: Full `StudyResult` + `StudySummary` models, `run_study_preflight()` guard, `experiment_gap_seconds` rename, unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/12-integration/12-CONTEXT.md

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/llenergymeasure/domain/experiment.py (current StudyResult — M1 stub):
```python
class StudyResult(BaseModel):
    """Container for study results. M1 stub."""
    experiments: list[ExperimentResult] = Field(...)
    name: str | None = Field(default=None)
```

From src/llenergymeasure/config/models.py (ExecutionConfig — has config_gap_seconds to rename):
```python
class ExecutionConfig(BaseModel):
    n_cycles: int = Field(default=1, ge=1)
    cycle_order: Literal["sequential", "interleaved", "shuffled"] = Field(default="sequential")
    config_gap_seconds: float | None = Field(default=None, ge=0.0)
    cycle_gap_seconds: float | None = Field(default=None, ge=0.0)
    shuffle_seed: int | None = Field(default=None)
```

From src/llenergymeasure/orchestration/preflight.py:
```python
def run_preflight(config: ExperimentConfig) -> None:
    """Run all pre-flight checks for a single experiment config."""
```

From src/llenergymeasure/exceptions.py:
```python
class PreFlightError(LLEMError): ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Upgrade StudyResult schema and add StudySummary</name>
  <files>src/llenergymeasure/domain/experiment.py</files>
  <action>
  Upgrade the existing `StudyResult` stub to the full RES-13 schema. Add a new `StudySummary` model.

  In `domain/experiment.py`:

  1. Add `StudySummary` model (before `StudyResult`):
     ```python
     class StudySummary(BaseModel):
         """Aggregate summary statistics for a completed study."""
         total_experiments: int = Field(..., description="Total experiments in the study")
         completed: int = Field(default=0, description="Number of successfully completed experiments")
         failed: int = Field(default=0, description="Number of failed experiments")
         total_wall_time_s: float = Field(default=0.0, description="Total wall-clock time in seconds")
         total_energy_j: float = Field(default=0.0, description="Total energy consumed in joules")
         warnings: list[str] = Field(default_factory=list, description="Runtime warnings (CLI narrowing, failures, etc.)")
     ```

  2. Replace the `StudyResult` stub with the full schema:
     ```python
     class StudyResult(BaseModel):
         """Final return value of a study run (RES-13).

         Distinct from StudyManifest (the in-progress checkpoint). StudyResult is
         assembled once after all experiments complete (or after interrupt) and returned
         to the caller.
         """
         experiments: list[ExperimentResult] = Field(
             default_factory=list, description="Results for each experiment in the study"
         )
         name: str | None = Field(default=None, description="Study name")
         study_design_hash: str | None = Field(
             default=None, description="16-char SHA-256 hex of experiment list"
         )
         measurement_protocol: dict[str, Any] = Field(
             default_factory=dict,
             description="Flat dict from ExecutionConfig: n_cycles, cycle_order, experiment_gap_seconds, cycle_gap_seconds, shuffle_seed, experiment_timeout_seconds",
         )
         result_files: list[str] = Field(
             default_factory=list,
             description="Paths to per-experiment result.json files (RES-15: paths, not embedded)",
         )
         summary: StudySummary | None = Field(
             default=None, description="Aggregate summary statistics"
         )
     ```

  3. Keep the `AggregatedResult = ExperimentResult` alias at the end of the file.

  Important: `StudyResult` does NOT have `extra="forbid"` — it is internal, not user-visible output (unlike ExperimentResult which does have it).
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.domain.experiment import StudyResult, StudySummary; sr = StudyResult(experiments=[], name='test', study_design_hash='abc', measurement_protocol={'n_cycles': 3}, result_files=['a.json'], summary=StudySummary(total_experiments=1)); print('OK:', sr.study_design_hash, sr.summary.total_experiments)"</automated>
  </verify>
  <done>StudyResult has study_design_hash, measurement_protocol, result_files, and summary fields. StudySummary model exists with total_experiments, completed, failed, total_wall_time_s, total_energy_j, warnings.</done>
</task>

<task type="auto">
  <name>Task 2: Rename config_gap_seconds to experiment_gap_seconds and add multi-backend pre-flight</name>
  <files>
    src/llenergymeasure/config/models.py
    src/llenergymeasure/config/user_config.py
    src/llenergymeasure/config/loader.py
    src/llenergymeasure/study/runner.py
    src/llenergymeasure/study/grid.py
    src/llenergymeasure/orchestration/preflight.py
  </files>
  <action>
  Two changes: (A) rename `config_gap_seconds` -> `experiment_gap_seconds`, (B) add multi-backend study pre-flight.

  **(A) Rename `config_gap_seconds` → `experiment_gap_seconds`:**

  1. `config/models.py` — In `ExecutionConfig`, rename the field:
     - `config_gap_seconds` → `experiment_gap_seconds` (keep same type, default, ge constraint, update description)

  1b. `config/user_config.py` — In `UserExecutionConfig`, rename the field (line ~98):
     - `config_gap_seconds` → `experiment_gap_seconds` (keep same type, default, ge constraint, update description)
     - This is the user-visible YAML-facing model — must stay in sync with `ExecutionConfig`.

  2. `study/runner.py` — In `StudyRunner.run()`, update the reference:
     - Change `self.study.execution.config_gap_seconds` → `self.study.execution.experiment_gap_seconds`
     - Update the gap label from `"Config gap"` to `"Experiment gap"` for clarity

  3. `config/loader.py` — No code changes needed (loader passes raw YAML dicts; the field name change is in the Pydantic model). But check: if any code references `config_gap_seconds` as a string literal or dict key, update it.

  4. `study/grid.py` — Search for any reference to `config_gap_seconds`. The `format_preflight_summary` function reads `study_config.execution` — no explicit references to field names, so likely no change needed. Verify.

  5. Update any existing tests that reference `config_gap_seconds` (search `tests/` for the old name):
     - `tests/unit/test_study_runner.py` has `config_gap_seconds=60.0` in a fixture — update to `experiment_gap_seconds=60.0`
     - `tests/unit/test_study_grid.py` may reference it — check and update

  **(B) Add `run_study_preflight()` in `orchestration/preflight.py`:**

  Add a new function below `run_preflight()`:

  ```python
  def run_study_preflight(study: StudyConfig) -> None:
      """Pre-flight checks for a study configuration.

      Raises PreFlightError if the study uses multiple backends (CM-10).
      Single-backend studies pass through — per-experiment pre-flight
      runs later in the subprocess.

      Args:
          study: Resolved StudyConfig.

      Raises:
          PreFlightError: Multi-backend study without Docker runner.
      """
      backends = {exp.backend for exp in study.experiments}
      if len(backends) > 1:
          backend_list = ", ".join(sorted(backends))
          raise PreFlightError(
              f"Multi-backend studies require Docker isolation (available in M3). "
              f"Found backends: {backend_list}. Use a single backend for now."
          )
  ```

  Add the necessary import at the top of preflight.py:
  ```python
  from llenergymeasure.config.models import ExperimentConfig, StudyConfig
  ```
  (Note: `ExperimentConfig` is already imported; add `StudyConfig` to the existing import.)

  Also add `"run_study_preflight"` to any `__all__` if the module uses one (it doesn't currently, so skip).
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.config.models import ExecutionConfig
ec = ExecutionConfig(experiment_gap_seconds=30.0)
print('rename OK:', ec.experiment_gap_seconds)
assert not hasattr(ec, 'config_gap_seconds'), 'Old field name still exists'
from llenergymeasure.orchestration.preflight import run_study_preflight
from llenergymeasure.config.models import ExperimentConfig, StudyConfig
study = StudyConfig(experiments=[
    ExperimentConfig(model='m1', backend='pytorch'),
    ExperimentConfig(model='m2', backend='vllm'),
])
try:
    run_study_preflight(study)
    assert False, 'Should have raised PreFlightError'
except Exception as e:
    assert 'Multi-backend' in str(e), f'Wrong error: {e}'
    assert 'Docker' in str(e), f'Missing Docker message: {e}'
print('preflight OK')
"</automated>
  </verify>
  <done>
    - ExecutionConfig.experiment_gap_seconds replaces config_gap_seconds
    - UserExecutionConfig.experiment_gap_seconds replaces config_gap_seconds (user_config.py)
    - All references updated (models, user_config, runner, loader, grid, tests)
    - Zero matches for `config_gap_seconds` across all .py files including user_config.py
    - run_study_preflight() raises PreFlightError for multi-backend studies with Docker direction message
    - Single-backend studies pass through without error
  </done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for StudyResult schema and multi-backend pre-flight</name>
  <files>
    tests/unit/test_study_result.py
    tests/unit/test_study_preflight.py
  </files>
  <action>
  Create two new test files.

  **tests/unit/test_study_result.py:**

  ```python
  """Tests for StudyResult full schema (RES-13)."""
  from llenergymeasure.domain.experiment import StudyResult, StudySummary

  def test_study_result_has_full_schema():
      """StudyResult includes all RES-13 fields."""
      summary = StudySummary(
          total_experiments=4,
          completed=3,
          failed=1,
          total_wall_time_s=120.5,
          total_energy_j=450.0,
          warnings=["1 experiment failed"],
      )
      result = StudyResult(
          experiments=[],
          name="my-study",
          study_design_hash="abcdef0123456789",
          measurement_protocol={
              "n_cycles": 3,
              "cycle_order": "interleaved",
              "experiment_gap_seconds": 30,
              "cycle_gap_seconds": 60,
          },
          result_files=["exp1/result.json", "exp2/result.json"],
          summary=summary,
      )
      assert result.study_design_hash == "abcdef0123456789"
      assert result.measurement_protocol["n_cycles"] == 3
      assert len(result.result_files) == 2
      assert result.summary is not None
      assert result.summary.total_experiments == 4
      assert result.summary.failed == 1
      assert result.summary.warnings == ["1 experiment failed"]

  def test_study_result_backwards_compat():
      """StudyResult still works with M1-style minimal construction."""
      result = StudyResult(experiments=[], name="test")
      assert result.study_design_hash is None
      assert result.measurement_protocol == {}
      assert result.result_files == []
      assert result.summary is None

  def test_study_summary_defaults():
      """StudySummary defaults: completed=0, failed=0, energy=0, warnings=[]."""
      summary = StudySummary(total_experiments=5)
      assert summary.completed == 0
      assert summary.failed == 0
      assert summary.total_wall_time_s == 0.0
      assert summary.total_energy_j == 0.0
      assert summary.warnings == []

  def test_result_files_are_paths_not_embedded():
      """RES-15: result_files contains path strings, not ExperimentResult objects."""
      result = StudyResult(
          experiments=[],
          result_files=["study_2026/exp1/result.json", "study_2026/exp2/result.json"],
      )
      assert all(isinstance(f, str) for f in result.result_files)
      assert all(f.endswith("result.json") for f in result.result_files)
  ```

  **tests/unit/test_study_preflight.py:**

  ```python
  """Tests for study-level pre-flight checks (CM-10)."""
  import pytest
  from llenergymeasure.config.models import ExperimentConfig, StudyConfig
  from llenergymeasure.exceptions import PreFlightError
  from llenergymeasure.orchestration.preflight import run_study_preflight

  def test_single_backend_passes():
      """Single-backend study passes pre-flight without error."""
      study = StudyConfig(experiments=[
          ExperimentConfig(model="m1", backend="pytorch"),
          ExperimentConfig(model="m2", backend="pytorch"),
      ])
      run_study_preflight(study)  # should not raise

  def test_multi_backend_raises_preflight_error():
      """Multi-backend study raises PreFlightError with Docker direction."""
      study = StudyConfig(experiments=[
          ExperimentConfig(model="m1", backend="pytorch"),
          ExperimentConfig(model="m2", backend="vllm"),
      ])
      with pytest.raises(PreFlightError, match="Multi-backend"):
          run_study_preflight(study)

  def test_multi_backend_error_mentions_docker():
      """Error message directs user to Docker runner (M3)."""
      study = StudyConfig(experiments=[
          ExperimentConfig(model="m1", backend="pytorch"),
          ExperimentConfig(model="m2", backend="vllm"),
      ])
      with pytest.raises(PreFlightError, match="Docker"):
          run_study_preflight(study)

  def test_multi_backend_error_lists_backends():
      """Error message lists the detected backends."""
      study = StudyConfig(experiments=[
          ExperimentConfig(model="m1", backend="pytorch"),
          ExperimentConfig(model="m2", backend="vllm"),
      ])
      with pytest.raises(PreFlightError) as exc_info:
          run_study_preflight(study)
      assert "pytorch" in str(exc_info.value)
      assert "vllm" in str(exc_info.value)
  ```

  Run both test files to confirm they pass.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_result.py tests/unit/test_study_preflight.py -v --tb=short 2>&1 | tail -20</automated>
  </verify>
  <done>
    - 4 StudyResult schema tests pass (full schema, backwards compat, defaults, paths-not-embedded)
    - 4 multi-backend pre-flight tests pass (single passes, multi raises, Docker message, backend list)
    - Existing test suite still passes with experiment_gap_seconds rename
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/unit/test_study_result.py tests/unit/test_study_preflight.py -v` — all 8 new tests pass
2. `python -m pytest tests/unit/test_study_runner.py tests/unit/test_study_grid.py tests/unit/test_study_manifest.py -v` — existing study tests pass with rename
3. `python -c "from llenergymeasure.domain.experiment import StudyResult, StudySummary; print('import OK')"` — models importable
4. `python -c "from llenergymeasure.config.models import ExecutionConfig; assert hasattr(ExecutionConfig.model_fields, '__contains__') and 'experiment_gap_seconds' in ExecutionConfig.model_fields"` — rename complete
5. `grep -r 'config_gap_seconds' src/ tests/` — returns zero matches (rename fully propagated)
</verification>

<success_criteria>
- StudyResult has all RES-13 fields: study_design_hash, measurement_protocol, result_files, summary
- StudySummary model exists with total_experiments, completed, failed, total_wall_time_s, total_energy_j, warnings
- config_gap_seconds is renamed to experiment_gap_seconds in all source and test files
- run_study_preflight() raises PreFlightError for multi-backend studies with Docker direction
- All existing tests pass after the rename
- 8+ new unit tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/12-integration/12-01-SUMMARY.md`
</output>
