---
phase: 12-integration
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - src/llenergymeasure/_api.py
  - src/llenergymeasure/study/runner.py
  - tests/unit/test_api.py
  - tests/unit/test_study_runner.py
autonomous: true
requirements:
  - LA-02
  - LA-05
  - STU-NEW-01
  - RES-15

must_haves:
  truths:
    - "run_study() accepts str | Path | StudyConfig and returns StudyResult with populated result_files, measurement_protocol, and summary"
    - "_run() dispatches single-experiment in-process and multi-experiment to StudyRunner"
    - "_run_experiment_worker sends ExperimentResult through Pipe (no longer raises NotImplementedError)"
    - "run_study() always writes manifest.json to disk (LA-05 side-effect contract)"
    - "result_files in StudyResult contains path strings to per-experiment result.json files (RES-15)"
    - "mark_completed receives the actual result_file path (not empty string)"
  artifacts:
    - path: "src/llenergymeasure/_api.py"
      provides: "run_study() implementation and _run() dispatcher"
      contains: "run_study_preflight"
    - path: "src/llenergymeasure/study/runner.py"
      provides: "Wired _run_experiment_worker with real backend"
      contains: "get_backend"
    - path: "tests/unit/test_api.py"
      provides: "Tests for run_study() and _run() dispatch"
    - path: "tests/unit/test_study_runner.py"
      provides: "Updated runner tests with study_dir parameter"
  key_links:
    - from: "src/llenergymeasure/_api.py"
      to: "llenergymeasure.study.runner.StudyRunner"
      via: "_run() creates StudyRunner when len(experiments) > 1 or n_cycles > 1"
      pattern: "StudyRunner.*\\.run\\(\\)"
    - from: "src/llenergymeasure/_api.py"
      to: "llenergymeasure.orchestration.preflight.run_study_preflight"
      via: "_run() calls run_study_preflight before dispatching"
      pattern: "run_study_preflight"
    - from: "src/llenergymeasure/study/runner.py"
      to: "llenergymeasure.core.backends.get_backend"
      via: "_run_experiment_worker calls get_backend + backend.run()"
      pattern: "get_backend.*backend\\.run"
---

<objective>
Wire the core study execution pipeline: implement `run_study()`, rewrite `_run()` as the single vs multi dispatcher, wire `_run_experiment_worker` to the real inference backend, and propagate result_file paths through the manifest and StudyResult.

Purpose: This is the central integration — connecting the Phase 9 config system, Phase 10 manifest, and Phase 11 subprocess runner into a working pipeline that `run_study()` can invoke and that returns a fully populated `StudyResult`.

Output: Working `run_study()` public API, `_run()` dispatcher, wired worker subprocess, result_file path tracking, unit tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/12-integration/12-CONTEXT.md
@.planning/phases/12-integration/12-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts from Plan 01 and existing code. -->

From src/llenergymeasure/domain/experiment.py (after Plan 01):
```python
class StudySummary(BaseModel):
    total_experiments: int
    completed: int = 0
    failed: int = 0
    total_wall_time_s: float = 0.0
    total_energy_j: float = 0.0
    warnings: list[str] = Field(default_factory=list)

class StudyResult(BaseModel):
    experiments: list[ExperimentResult]
    name: str | None = None
    study_design_hash: str | None = None
    measurement_protocol: dict[str, Any] = Field(default_factory=dict)
    result_files: list[str] = Field(default_factory=list)
    summary: StudySummary | None = None
```

From src/llenergymeasure/_api.py (current — to be rewritten):
```python
def run_study(config: str | Path | StudyConfig) -> StudyResult:
    raise NotImplementedError(...)

def _run(study: StudyConfig) -> StudyResult:
    # Currently runs single experiments in-process (M1 implementation)
    from llenergymeasure.core.backends import get_backend
    from llenergymeasure.orchestration.preflight import run_preflight
    results = []
    for config in study.experiments:
        run_preflight(config)
        backend = get_backend(config.backend)
        result = backend.run(config)
        results.append(result)
    return StudyResult(experiments=results, name=study.name)
```

From src/llenergymeasure/study/runner.py (current):
```python
class StudyRunner:
    def __init__(self, study: StudyConfig, manifest_writer: ManifestWriter) -> None: ...
    def run(self) -> list[Any]: ...  # returns list of results or failure dicts

def _run_experiment_worker(config, conn, progress_queue) -> None:
    # Currently raises NotImplementedError — must wire to real backend
```

From src/llenergymeasure/study/manifest.py:
```python
def create_study_dir(name: str | None, output_dir: Path) -> Path: ...
def experiment_result_filename(model, backend, precision, config_hash, extension=".json") -> str: ...

class ManifestWriter:
    def __init__(self, study: StudyConfig, study_dir: Path) -> None: ...
    def mark_running(self, config_hash, cycle) -> None: ...
    def mark_completed(self, config_hash, cycle, result_file: str) -> None: ...
    def mark_failed(self, config_hash, cycle, error_type, error_message) -> None: ...
    def mark_interrupted(self) -> None: ...
```

From src/llenergymeasure/core/backends/__init__.py:
```python
def get_backend(backend_name: str) -> InferenceBackend: ...
```

From src/llenergymeasure/orchestration/preflight.py (after Plan 01):
```python
def run_preflight(config: ExperimentConfig) -> None: ...
def run_study_preflight(study: StudyConfig) -> None: ...
```

From src/llenergymeasure/config/loader.py:
```python
def load_study_config(path: Path | str, cli_overrides: dict | None = None) -> StudyConfig: ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement run_study() and _run() dispatcher with result saving</name>
  <files>
    src/llenergymeasure/_api.py
    src/llenergymeasure/study/runner.py
  </files>
  <action>
  Two files modified: the API layer and the runner.

  **(A) Implement `run_study()` in `_api.py` — replace the NotImplementedError stub:**

  ```python
  def run_study(config: str | Path | StudyConfig) -> StudyResult:
      """Run a multi-experiment study.

      Always writes manifest.json to disk (LA-05 — documented side-effect).
      Returns StudyResult with full schema (RES-13).

      Args:
          config: YAML file path or resolved StudyConfig.

      Returns:
          StudyResult with experiments, result_files, measurement_protocol, summary.

      Raises:
          ConfigError: Invalid config path or parse error.
          PreFlightError: Multi-backend study without Docker (CM-10).
          pydantic.ValidationError: Invalid field values (passes through unchanged).
      """
      if isinstance(config, (str, Path)):
          from llenergymeasure.config.loader import load_study_config
          study = load_study_config(path=Path(config))
      elif isinstance(config, StudyConfig):
          study = config
      else:
          raise ConfigError(
              f"Expected str, Path, or StudyConfig; got {type(config).__name__}"
          )
      return _run(study)
  ```

  **(B) Rewrite `_run()` as the single-vs-multi dispatcher:**

  Per CONTEXT.md dispatch logic:
  - `len(experiments) == 1` AND `n_cycles == 1`: run in-process (no subprocess overhead)
  - Otherwise: delegate to StudyRunner

  Both paths must:
  1. Call `run_study_preflight(study)` first (CM-10 multi-backend guard)
  2. Create study output directory and ManifestWriter (LA-05)
  3. Return a fully populated StudyResult (RES-13 fields + RES-15 result_files)

  Replace the entire `_run()` function body. Use deferred imports inside the function body to avoid circular imports (all heavy imports like `get_backend`, `run_preflight`, `StudyRunner`, `ManifestWriter`, `create_study_dir`, `save_result`, etc.).

  Key implementation points for the in-process path:
  - Call `run_preflight(config)` then `get_backend(config.backend).run(config)`
  - Save result to study directory via `save_result(result, study_dir)`
  - Build result_files list from saved path
  - Mark completed in manifest

  Key implementation points for the multi-experiment path:
  - Create `StudyRunner(study, manifest_writer, study_dir)` — note: study_dir is a new parameter (see part C)
  - Call `runner.run()` — returns list of results (ExperimentResult or failure dict)
  - After runner completes, collect result_files from `runner.result_files`
  - Build StudyResult from runner output

  Both paths build:
  - `measurement_protocol` as flat dict: `{n_cycles, cycle_order, experiment_gap_seconds, cycle_gap_seconds, shuffle_seed}`
  - `StudySummary` with totals (completed, failed, wall_time, energy, warnings)
  - `result_files` list of path strings

  For the default output directory, use `Path("results")` as the parent — `create_study_dir` creates `results/{name}_{timestamp}/` underneath it.

  Remove the `_to_study_config` function call from `run_experiment` and `_run` chain. Important: `run_experiment` still calls `_run()` via `_to_study_config` wrapping into a degenerate StudyConfig — this path now hits the in-process branch (single experiment, 1 cycle). Verify this works by checking that the existing `run_experiment` flow is not broken.

  **(C) Add `study_dir` parameter to `StudyRunner` and wire result saving in `_run_one`:**

  In `study/runner.py`:

  1. Add `from pathlib import Path` to runtime imports (not just TYPE_CHECKING)
  2. Update `__init__` signature: `def __init__(self, study, manifest_writer, study_dir: Path)`
  3. Store: `self.study_dir = study_dir` and `self.result_files: list[str] = []`
  4. In `_run_one`, after collecting a successful result (the `else` branch after `isinstance(result, dict)`):
     - Lazy import `save_result` from `llenergymeasure.results.persistence`
     - Save: `result_path = save_result(result, self.study_dir)`
     - Track: `self.result_files.append(str(result_path))`
     - Update manifest: `self.manifest.mark_completed(config_hash, cycle, str(result_path.relative_to(self.study_dir)))`
     - Wrap in try/except so save failure does not abort the study
  5. Replace the existing `self.manifest.mark_completed(config_hash, cycle, result_file="")` with the new code above

  **Note:** `save_result()` creates one level of nesting inside `study_dir` — `result_files` entries will be relative paths like `model_slug_pytorch_ts/result.json`, not flat filenames. `relative_to(self.study_dir)` is correct for this nesting.

  **(D) Wire `_run_experiment_worker` to real backend:**

  Replace the `raise NotImplementedError(...)` block in `_run_experiment_worker` with:

  ```python
  from llenergymeasure.core.backends import get_backend
  from llenergymeasure.orchestration.preflight import run_preflight

  # Pre-flight inside subprocess (CUDA availability check needs to run in process that will use GPU)
  run_preflight(config)

  # Run actual experiment
  backend = get_backend(config.backend)
  result = backend.run(config)

  # Send result back to parent via Pipe
  conn.send(result)
  progress_queue.put({"event": "completed", "config_hash": config_hash})
  ```

  Keep the existing: `signal.signal(signal.SIGINT, signal.SIG_IGN)` at top, error payload on exception, `finally: conn.close()`. Remove the NotImplementedError and the stale Phase 12 comment.

  **Important design notes:**
  - `run_experiment()` existing callers are NOT broken: `_to_study_config` wraps into degenerate StudyConfig, `_run()` hits in-process path
  - In-process path: preflight + backend.run happen in the current process (no spawn)
  - Multi-experiment path: preflight + backend.run happen in each subprocess via `_run_experiment_worker`
  - Saving results: in-process saves in `_run()`, multi-experiment saves in `_run_one()` via StudyRunner
  - Manifest is always written (LA-05): ManifestWriter.__init__ writes the initial manifest
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure._api import run_study, _run
from llenergymeasure.study.runner import StudyRunner, _run_experiment_worker
import inspect
# Verify run_study no longer raises NotImplementedError
src = inspect.getsource(run_study)
assert 'NotImplementedError' not in src, 'run_study still has NotImplementedError'
# Verify _run_experiment_worker no longer raises NotImplementedError
src2 = inspect.getsource(_run_experiment_worker)
assert 'NotImplementedError' not in src2, 'Worker still has NotImplementedError'
# Verify StudyRunner accepts study_dir
sig = inspect.signature(StudyRunner.__init__)
assert 'study_dir' in sig.parameters, 'StudyRunner missing study_dir parameter'
print('All signatures OK')
"</automated>
  </verify>
  <done>
    - run_study() accepts str | Path | StudyConfig, loads YAML if needed, delegates to _run()
    - _run() dispatches: single in-process, multi via StudyRunner (STU-NEW-01)
    - _run() calls run_study_preflight() before dispatch (CM-10)
    - _run() creates study directory and ManifestWriter (LA-05)
    - _run() returns fully populated StudyResult with result_files (RES-15)
    - _run_experiment_worker calls get_backend().run(config) and sends result via Pipe
    - StudyRunner saves results in _run_one and tracks result_files
    - run_experiment() still works via degenerate StudyConfig in-process path
  </done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for run_study(), _run() dispatcher, and worker wiring</name>
  <files>
    tests/unit/test_api.py
    tests/unit/test_study_runner.py
  </files>
  <action>
  Update existing test files — do NOT replace existing tests, append new ones.

  **tests/unit/test_api.py** — Add study API tests at the end of the file:

  1. `test_run_study_accepts_study_config` — Mock `get_backend`, `run_preflight`, `run_study_preflight`. Create a single-experiment StudyConfig. Call `run_study(study)`. Assert result is StudyResult with populated summary.

  2. `test_run_study_accepts_path` — Create a minimal study YAML in tmp_path. Mock backends/preflight. Call `run_study(str(yaml_path))`. Assert returns StudyResult.

  3. `test_run_study_invalid_type_raises_config_error` — Call `run_study(42)`. Assert raises ConfigError.

  4. `test_run_dispatches_single_in_process` — Single experiment + n_cycles=1. Mock everything. Patch `StudyRunner` class. Assert StudyRunner was NOT called (in-process path used). Assert `get_backend().run()` was called directly.

  5. `test_run_study_returns_study_result_type` — Inspect `run_study` return annotation. Confirm it is `StudyResult` (not a union).

  Mock patterns:
  - All tests must mock `get_backend` and `run_preflight` since no GPU available
  - Mock `run_study_preflight` since it validates backend availability
  - For the mock ExperimentResult returned by `get_backend().run()`, use a MagicMock with `total_energy_j`, `model_dump_json.return_value`, `effective_config`, `backend`, `timeseries` attributes
  - Mock `save_result` from `llenergymeasure.results.persistence` to avoid real file I/O, OR use tmp_path and let it save (but result model may not validate)

  **tests/unit/test_study_runner.py** — Update existing tests for study_dir parameter:

  1. Search for all `StudyRunner(` constructor calls and add `study_dir` parameter (use `Path("/tmp/test-study")` with MagicMock manifest, or `tmp_path` fixture)

  2. Add `test_worker_no_longer_stub` — inspect source of `_run_experiment_worker`, assert "NotImplementedError" not in source

  3. Add `test_worker_calls_get_backend` — Mock multiprocessing context, create real Pipe, call `_run_experiment_worker` with mocked `get_backend` and `run_preflight`. Assert `get_backend` was called with the config's backend name.

  For updating existing tests: the main change is adding `Path("/tmp/test-study")` or similar as the third argument to `StudyRunner()`. The existing mock infrastructure (patched multiprocessing context) should continue to work since `study_dir` is only used in the success path of `_run_one` which is already mocked in most tests.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_api.py tests/unit/test_study_runner.py -v --tb=short 2>&1 | tail -30</automated>
  </verify>
  <done>
    - test_run_study_accepts_study_config passes: run_study(StudyConfig) returns StudyResult
    - test_run_study_accepts_path passes: run_study(str) loads and returns StudyResult
    - test_run_study_invalid_type_raises_config_error passes: run_study(42) raises ConfigError
    - test_run_dispatches_single_in_process passes: single experiment bypasses StudyRunner
    - test_run_study_returns_study_result_type passes: return type check
    - test_worker_no_longer_stub passes: NotImplementedError removed
    - test_worker_calls_get_backend passes: worker calls backend correctly
    - All existing runner tests pass with study_dir parameter added
  </done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure import run_study; print(run_study.__doc__[:50])"` — run_study is importable and documented
2. `python -m pytest tests/unit/test_api.py -v --tb=short` — all API tests pass including new run_study tests
3. `python -m pytest tests/unit/test_study_runner.py -v --tb=short` — all runner tests pass with study_dir parameter
4. `grep -r "NotImplementedError" src/llenergymeasure/study/runner.py src/llenergymeasure/_api.py` — zero matches
5. `python -m pytest tests/unit/ -x --tb=short` — full unit suite passes (no regressions)
</verification>

<success_criteria>
- run_study() accepts str | Path | StudyConfig and returns StudyResult (LA-02)
- run_study() always creates manifest.json on disk (LA-05)
- _run() dispatches: single in-process, multi via StudyRunner (STU-NEW-01)
- _run_experiment_worker runs real backend (no more NotImplementedError)
- StudyResult.result_files contains paths to saved result.json files (RES-15)
- mark_completed receives actual result_file path
- All existing + new tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/12-integration/12-02-SUMMARY.md`
</output>
