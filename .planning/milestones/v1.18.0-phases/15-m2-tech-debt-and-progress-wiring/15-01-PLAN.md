---
phase: 15-m2-tech-debt-and-progress-wiring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/study/runner.py
  - tests/unit/test_study_runner.py
autonomous: true
requirements: []

must_haves:
  truths:
    - "_consume_progress_events() forwards events to print_study_progress() — progress display visible during study execution"
    - "experiment_timeout_seconds phantom getattr reference is removed from runner.py — _calculate_timeout() is the sole timeout source"
    - "All existing unit tests pass with the changes applied"
  artifacts:
    - path: "src/llenergymeasure/study/runner.py"
      provides: "_consume_progress_events() calls print_study_progress(); getattr phantom removed"
      contains: "print_study_progress"
    - path: "tests/unit/test_study_runner.py"
      provides: "Test for progress event forwarding"
      contains: "test_progress_events_forwarded"
  key_links:
    - from: "src/llenergymeasure/study/runner.py"
      to: "src/llenergymeasure/cli/_display.py"
      via: "_consume_progress_events() imports and calls print_study_progress()"
      pattern: "print_study_progress"
---

<objective>
Wire the orphaned progress display and remove the phantom experiment_timeout_seconds field reference — two targeted fixes that close 1 broken flow and 2 tech debt items from the M2 audit.

Purpose: `print_study_progress()` was created in Phase 12-03 but never wired into `_consume_progress_events()`, which was left as a drain-only stub from Phase 11. The `experiment_timeout_seconds` field is referenced via `getattr` in `runner.py` but never existed on `ExecutionConfig` (the field has `extra="forbid"`, so adding it would be the wrong fix — remove the phantom reference and rely on the existing `_calculate_timeout()` heuristic).

Output: Progress lines visible on stderr during study execution; clean code with no phantom field references.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/llenergymeasure/study/runner.py (_consume_progress_events — lines 118-129):
```python
def _consume_progress_events(q: Any) -> None:
    """Consume and discard progress events from the queue until None sentinel.

    Runs as a daemon thread. Display wiring is Phase 12 — this stub just
    drains the queue so the child never blocks on a full Queue.
    """
    while True:
        event = q.get()
        if event is None:
            break
        # Phase 12: forward to Rich display layer here
```

From src/llenergymeasure/study/runner.py (_run_one — line 335):
```python
# Phantom getattr reference — field does not exist on ExecutionConfig
user_timeout = getattr(self.study.execution, "experiment_timeout_seconds", None)
timeout = int(user_timeout) if user_timeout is not None else _calculate_timeout(config)
```

From src/llenergymeasure/study/runner.py (_calculate_timeout — lines 39-45):
```python
def _calculate_timeout(config: ExperimentConfig) -> int:
    """Generous timeout heuristic: 2 seconds per prompt, minimum 10 minutes.

    No model-size scaling — keep it simple. The escape hatch is
    execution.experiment_timeout_seconds in the study YAML.
    """
    return max(config.n * 2, 600)
```

From src/llenergymeasure/cli/_display.py (print_study_progress — lines 297-330):
```python
def print_study_progress(
    index: int,
    total: int,
    config: ExperimentConfig,
    status: str = "running",
    elapsed: float | None = None,
    energy: float | None = None,
) -> None:
    """Print a per-experiment progress line to stderr.

    Format: [3/12] <icon> model backend precision -- elapsed (energy)
    Icons: completed=OK, failed=FAIL, running=...
    """
    icons = {"running": "...", "completed": "OK", "failed": "FAIL"}
    icon = icons.get(status, "?")
    parts = [f"[{index}/{total}]", icon, config.model, config.backend, config.precision]
    if elapsed is not None:
        parts.append("--")
        parts.append(_format_duration(elapsed))
    if energy is not None:
        parts.append(f"({_sig3(energy)} J)")
    line = " ".join(parts)
    print(line, file=sys.stderr)
```

From src/llenergymeasure/study/runner.py (_run_experiment_worker — progress events):
```python
# Worker sends these events via progress_queue:
progress_queue.put({"event": "started", "config_hash": config_hash})
progress_queue.put({"event": "completed", "config_hash": config_hash})
progress_queue.put({"event": "failed", "error": str(exc)})
```

From src/llenergymeasure/config/models.py (ExecutionConfig — lines 429-477):
```python
class ExecutionConfig(BaseModel):
    model_config = {"extra": "forbid"}

    n_cycles: int = Field(...)
    cycle_order: Literal["sequential", "interleaved", "shuffled"] = Field(...)
    experiment_gap_seconds: float | None = Field(...)
    cycle_gap_seconds: float | None = Field(...)
    shuffle_seed: int | None = Field(...)
    # NOTE: experiment_timeout_seconds does NOT exist here
    # extra="forbid" means it CANNOT be added via YAML either
```

From src/llenergymeasure/study/runner.py (StudyRunner.run — experiment loop):
```python
for i, config in enumerate(ordered):
    # ...
    result = self._run_one(config, mp_ctx)
    results.append(result)
```

From src/llenergymeasure/study/runner.py (_run_one — lines 320-395):
The _run_one method spawns a subprocess, starts the consumer thread, does p.join, then
collects the result. The consumer thread gets events while the subprocess runs.
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire progress display in _consume_progress_events()</name>
  <files>
    src/llenergymeasure/study/runner.py
    tests/unit/test_study_runner.py
  </files>
  <action>
    **Understanding the gap:** `_consume_progress_events()` is a daemon thread that drains the progress queue but discards every event. `print_study_progress()` exists in `cli/_display.py` and accepts `(index, total, config, status, elapsed, energy)`. The worker sends events with `config_hash` and `event` type. The consumer needs to call `print_study_progress()` with the correct arguments.

    **The challenge:** The consumer thread runs in the parent process and receives events from the child. It needs:
    - `index` and `total` — the experiment's position in the study
    - `config` — the `ExperimentConfig` object
    - `status` — derived from the event type

    These are available in `_run_one()` which starts the consumer thread. The cleanest approach is to pass context into the consumer.

    **Fix — Rewrite _consume_progress_events() to accept context and forward to display:**

    Change the signature to accept study-level context:

    ```python
    def _consume_progress_events(
        q: Any,
        index: int,
        total: int,
        config: Any,  # ExperimentConfig
    ) -> None:
        """Consume progress events from the queue and forward to display.

        Runs as a daemon thread in the parent process. Receives events from the
        child subprocess via multiprocessing.Queue and calls print_study_progress()
        for each meaningful event.
        """
        while True:
            event = q.get()
            if event is None:
                break

            if not isinstance(event, dict):
                continue

            event_type = event.get("event")
            if event_type == "started":
                from llenergymeasure.cli._display import print_study_progress

                print_study_progress(index, total, config, status="running")
            elif event_type == "completed":
                from llenergymeasure.cli._display import print_study_progress

                print_study_progress(index, total, config, status="completed")
            elif event_type == "failed":
                from llenergymeasure.cli._display import print_study_progress

                print_study_progress(index, total, config, status="failed")
    ```

    Use lazy imports for `print_study_progress` inside the function body (not at module top level) to keep `study/runner.py` decoupled from the CLI display layer. The imports are inside the event handlers so they only run when events arrive — no import cost if progress display is unused.

    **Update _run_one() to pass context to the consumer thread:**

    In `_run_one()`, the consumer thread is currently started as:
    ```python
    consumer = threading.Thread(
        target=_consume_progress_events,
        args=(progress_queue,),
        daemon=True,
    )
    ```

    The `index` and `total` need to come from the caller. Add `index` and `total` parameters to `_run_one()`:

    ```python
    def _run_one(self, config: ExperimentConfig, mp_ctx: Any, index: int, total: int) -> Any:
    ```

    And update the consumer thread creation:
    ```python
    consumer = threading.Thread(
        target=_consume_progress_events,
        args=(progress_queue, index, total, config),
        daemon=True,
    )
    ```

    **Update the call site in run():**

    In `StudyRunner.run()`, the loop currently calls:
    ```python
    result = self._run_one(config, mp_ctx)
    ```

    Change to pass 1-based index and total:
    ```python
    result = self._run_one(config, mp_ctx, index=i + 1, total=len(ordered))
    ```

    **Update _calculate_timeout() docstring:**

    The docstring says "The escape hatch is execution.experiment_timeout_seconds in the study YAML." Remove this misleading sentence since the field does not exist. Replace with:
    ```python
    def _calculate_timeout(config: ExperimentConfig) -> int:
        """Generous timeout heuristic: 2 seconds per prompt, minimum 10 minutes.

        No model-size scaling — keep it simple for M2.
        """
        return max(config.n * 2, 600)
    ```

    **Add test:**

    In `tests/unit/test_study_runner.py`, add:

    ```python
    def test_progress_events_forwarded(monkeypatch):
        """_consume_progress_events forwards events to print_study_progress."""
        from unittest.mock import MagicMock, patch
        from queue import Queue

        from llenergymeasure.config.models import ExperimentConfig

        config = ExperimentConfig(model="test-model", backend="pytorch", n=10)
        q = Queue()
        q.put({"event": "started", "config_hash": "abc123"})
        q.put({"event": "completed", "config_hash": "abc123"})
        q.put(None)  # sentinel

        with patch(
            "llenergymeasure.cli._display.print_study_progress"
        ) as mock_progress:
            from llenergymeasure.study.runner import _consume_progress_events

            _consume_progress_events(q, index=3, total=12, config=config)

        assert mock_progress.call_count == 2
        # First call: running
        assert mock_progress.call_args_list[0].kwargs.get("status") == "running" or mock_progress.call_args_list[0][1][3] == "running" or mock_progress.call_args_list[0] == call(3, 12, config, status="running")
        # Second call: completed
        assert mock_progress.call_args_list[1].kwargs.get("status") == "completed" or mock_progress.call_args_list[1][1][3] == "completed" or mock_progress.call_args_list[1] == call(3, 12, config, status="completed")
    ```

    Simplify the assertion approach — just check call count and that the mock was called with expected status values:
    ```python
    def test_progress_events_forwarded():
        """_consume_progress_events forwards events to print_study_progress."""
        from queue import Queue
        from unittest.mock import patch

        from llenergymeasure.config.models import ExperimentConfig
        from llenergymeasure.study.runner import _consume_progress_events

        config = ExperimentConfig(model="test-model", backend="pytorch", n=10)
        q = Queue()
        q.put({"event": "started", "config_hash": "abc123"})
        q.put({"event": "completed", "config_hash": "abc123"})
        q.put(None)  # sentinel

        with patch(
            "llenergymeasure.cli._display.print_study_progress"
        ) as mock_progress:
            _consume_progress_events(q, index=3, total=12, config=config)

        assert mock_progress.call_count == 2
        statuses = [c.kwargs["status"] for c in mock_progress.call_args_list]
        assert statuses == ["running", "completed"]
    ```
  </action>
  <verify>
    <automated>pytest tests/unit/test_study_runner.py -x -q</automated>
  </verify>
  <done>
    - `_consume_progress_events()` imports and calls `print_study_progress()` for started/completed/failed events
    - `_run_one()` passes `index` and `total` and `config` to the consumer thread
    - `run()` passes 1-based index and total to `_run_one()`
    - `test_progress_events_forwarded` passes — events forwarded to display
    - All existing runner tests pass without regressions
  </done>
</task>

<task type="auto">
  <name>Task 2: Remove phantom experiment_timeout_seconds reference</name>
  <files>
    src/llenergymeasure/study/runner.py
  </files>
  <action>
    **Understanding the phantom:** `_run_one()` line 335 uses `getattr(self.study.execution, "experiment_timeout_seconds", None)` to check for a user-supplied timeout. But `ExecutionConfig` has `extra="forbid"` and no `experiment_timeout_seconds` field. This `getattr` always returns `None`. The field cannot be added to `ExecutionConfig` without breaking YAML validation for existing study configs that don't include it (since `extra="forbid"` rejects unknown keys).

    The correct fix is to remove the phantom reference entirely and always use the `_calculate_timeout()` heuristic. If a user-configurable timeout is needed in a future milestone, it should be added as a proper field on `ExecutionConfig` at that time.

    **Fix — Simplify _run_one() timeout logic:**

    In `_run_one()`, replace the phantom getattr block:
    ```python
    # REMOVE:
    user_timeout = getattr(self.study.execution, "experiment_timeout_seconds", None)
    timeout = int(user_timeout) if user_timeout is not None else _calculate_timeout(config)
    ```

    With:
    ```python
    timeout = _calculate_timeout(config)
    ```

    **Update _calculate_timeout() docstring:**

    Already done in Task 1 (remove the misleading "escape hatch" sentence). If Task 1 already made this change, skip it here.

    **Also remove the comment in the module docstring or _calculate_timeout docstring that references experiment_timeout_seconds** if any remain.

    **No new tests needed** — existing timeout tests cover `_calculate_timeout()`. The `getattr` was always returning `None` anyway, so removing it changes no behaviour.
  </action>
  <verify>
    <automated>pytest tests/unit/test_study_runner.py -x -q</automated>
    <automated>python -c "import ast; tree = ast.parse(open('src/llenergymeasure/study/runner.py').read()); assert 'experiment_timeout_seconds' not in ast.dump(tree), 'phantom field still referenced'"</automated>
  </verify>
  <done>
    - `getattr(self.study.execution, "experiment_timeout_seconds", None)` removed from _run_one()
    - `_calculate_timeout()` is the sole timeout source
    - `_calculate_timeout()` docstring no longer references experiment_timeout_seconds
    - Zero references to `experiment_timeout_seconds` in runner.py
    - All existing tests pass
  </done>
</task>

</tasks>

<verification>
Run full unit test suite to confirm no regressions:

```bash
pytest tests/unit/ -x -q
```

Spot-check the 3 must-have truths:

1. **Progress wiring:** Grep `runner.py` for `print_study_progress` — imported and called in `_consume_progress_events()`
2. **Phantom removed:** Grep `runner.py` for `experiment_timeout_seconds` — zero results
3. **Tests pass:** Full test suite green

```bash
# Verification commands
grep -n "print_study_progress" src/llenergymeasure/study/runner.py  # should find import + call
grep -n "experiment_timeout_seconds" src/llenergymeasure/study/runner.py  # should return 0 lines
pytest tests/unit/ -x -q  # all tests pass
```
</verification>

<success_criteria>
- _consume_progress_events() forwards events to print_study_progress() — "[3/12] ... model backend precision" visible during study execution
- experiment_timeout_seconds phantom getattr reference removed — _calculate_timeout() is the sole timeout source
- Zero references to experiment_timeout_seconds remain in runner.py
- All existing unit tests pass with the changes applied
</success_criteria>

<output>
After completion, create `.planning/phases/15-m2-tech-debt-and-progress-wiring/15-01-SUMMARY.md`
</output>
