---
phase: 11-subprocess-isolation-and-studyrunner
plan: "01"
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/study/runner.py
  - tests/unit/test_study_runner.py
autonomous: true
requirements:
  - STU-01
  - STU-02
  - STU-03
  - STU-04
  - STU-07

must_haves:
  truths:
    - "StudyRunner spawns each experiment in a separate process using multiprocessing.get_context('spawn') — never fork"
    - "Results travel parent←child exclusively via multiprocessing.Pipe (no temp files for normal-sized results)"
    - "Child processes are created with daemon=False — clean CUDA teardown guaranteed"
    - "A subprocess that exceeds timeout is killed with p.kill() (SIGKILL), not p.terminate(), and _run_one returns a failure dict rather than raising"
    - "A subprocess that raises an unhandled exception sends a structured error dict through the Pipe; StudyRunner marks it failed and continues to the next experiment"
    - "cycle_order=interleaved produces round-robin scheduling; sequential produces all-cycles-of-A before B"
  artifacts:
    - path: "src/llenergymeasure/study/runner.py"
      provides: "StudyRunner class, _run_experiment_worker, _collect_result, _consume_progress_events"
      exports:
        - "StudyRunner"
        - "_run_experiment_worker"
        - "_calculate_timeout"
    - path: "tests/unit/test_study_runner.py"
      provides: "TDD test suite for StudyRunner — all paths covered without GPU"
      min_lines: 120
  key_links:
    - from: "src/llenergymeasure/study/runner.py"
      to: "multiprocessing.get_context('spawn')"
      via: "mp_ctx = multiprocessing.get_context('spawn') in StudyRunner.__init__ or run()"
      pattern: "get_context.*spawn"
    - from: "src/llenergymeasure/study/runner.py"
      to: "src/llenergymeasure/study/manifest.py"
      via: "ManifestWriter.mark_running / mark_completed / mark_failed after each subprocess"
      pattern: "mark_running|mark_completed|mark_failed"
    - from: "tests/unit/test_study_runner.py"
      to: "src/llenergymeasure/study/runner.py"
      via: "FakeWorker functions injected via mp_ctx mock — no real GPU needed"
      pattern: "test_.*timeout|test_.*failure|test_.*success"
---

<objective>
Implement `StudyRunner` — the subprocess dispatch core for Phase 11.

Purpose: Every experiment in a study must run in a freshly spawned subprocess with a clean CUDA
context. Results return via Pipe, failures are structured and non-fatal, and timeouts are
enforced via SIGKILL. This is the central correctness requirement for measurement rigour.

Output:
- `src/llenergymeasure/study/runner.py` — StudyRunner, worker function, result collection
- `tests/unit/test_study_runner.py` — TDD tests covering all execution paths (no GPU required)
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/11-subprocess-isolation-and-studyrunner/11-CONTEXT.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/config/models.py:
```python
class ExecutionConfig(BaseModel):
    n_cycles: int = Field(default=1, ge=1)
    cycle_order: Literal["sequential", "interleaved", "shuffled"] = Field(default="sequential")
    config_gap_seconds: float | None = Field(default=None, ge=0.0)
    cycle_gap_seconds: float | None = Field(default=None, ge=0.0)
    shuffle_seed: int | None = Field(default=None)

class StudyConfig(BaseModel):
    experiments: list[ExperimentConfig] = Field(..., min_length=1)
    name: str | None = Field(default=None)
    execution: ExecutionConfig = Field(default_factory=ExecutionConfig)
    study_design_hash: str | None = Field(default=None)
    skipped_configs: list[dict[str, Any]] = Field(default_factory=list)

class ExperimentConfig(BaseModel):
    model: str
    backend: str = "pytorch"
    precision: str = "bf16"
    n: int = 100          # number of prompts
    # ... many other fields ...
```

From src/llenergymeasure/study/manifest.py:
```python
class ManifestWriter:
    def __init__(self, study: StudyConfig, study_dir: Path) -> None: ...
    def mark_running(self, config_hash: str, cycle: int) -> None: ...
    def mark_completed(self, config_hash: str, cycle: int, result_file: str) -> None: ...
    def mark_failed(self, config_hash: str, cycle: int, error_type: str, error_message: str) -> None: ...
```

From src/llenergymeasure/domain/experiment.py:
```python
def compute_measurement_config_hash(config: ExperimentConfig) -> str:
    """SHA-256[:16] of ExperimentConfig."""

class ExperimentResult(BaseModel):
    model_config = {"frozen": True, "extra": "forbid"}
    # ... many fields ...

class StudyResult(BaseModel):
    experiments: list[ExperimentResult] = Field(default_factory=list)
    name: str | None = Field(default=None)
```

From src/llenergymeasure/study/grid.py:
```python
def apply_cycles(
    experiments: list[ExperimentConfig],
    n_cycles: int,
    cycle_order: CycleOrder,
    study_design_hash: str,
    shuffle_seed: int | None = None,
) -> list[ExperimentConfig]:
    """Returns ordered execution sequence for n_cycles repetitions."""
```
</interfaces>
</context>

<tasks>

<task type="tdd">
  <name>Task 1 (RED): Write failing tests for StudyRunner subprocess lifecycle</name>
  <files>tests/unit/test_study_runner.py</files>
  <action>
Write a TDD test suite that fails because `src/llenergymeasure/study/runner.py` does not exist yet.
Tests must cover every subprocess execution path without requiring a GPU. Use
`unittest.mock.patch` and fake worker functions to simulate child process behaviour.

Test cases to write (all should fail at import or assertion):

1. `test_calculate_timeout_minimum` — `_calculate_timeout(config_with_n_100)` returns >= 600
2. `test_calculate_timeout_scales_with_n` — `_calculate_timeout(config_with_n_1000)` returns >= 2000
3. `test_study_runner_success_path` — inject a fake worker that sends a fake ExperimentResult
   via Pipe; assert StudyRunner collects it and ManifestWriter.mark_completed is called
4. `test_study_runner_subprocess_exception` — inject a fake worker that sends a structured
   error dict via Pipe and raises; assert StudyRunner marks the experiment failed and continues
   (does not raise)
5. `test_study_runner_timeout` — patch `p.join` to simulate timeout (p.is_alive() stays True);
   assert p.kill() is called and experiment is marked failed
6. `test_study_runner_exitcode_nonzero_no_pipe_data` — simulate non-zero exitcode with empty Pipe;
   assert experiment marked failed with "ProcessCrash" error type
7. `test_spawn_context_used` — assert that `multiprocessing.get_context` is called with `"spawn"`,
   not `"fork"` or default
8. `test_daemon_false` — assert the Process is created with `daemon=False`
9. `test_interleaved_ordering` — `StudyConfig` with 2 experiments, n_cycles=2,
   cycle_order=interleaved; assert the ordering is A,B,A,B (round-robin)
10. `test_sequential_ordering` — same but cycle_order=sequential; assert A,A,B,B

For tests 3-8, avoid spawning real subprocesses. Instead, patch
`multiprocessing.get_context` to return a mock context whose `Process` class:
- Records constructor args (daemon= kwarg, target= function)
- Can have `is_alive()`, `exitcode`, `pid` controlled per test

For manifest interaction, pass a MagicMock ManifestWriter and assert calls.

Worker functions should be simple in-process callables (not importable in a real
child process — that is fine for unit tests; only integration tests need real spawning).
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_runner.py -x 2>&1 | grep -E "ImportError|ModuleNotFoundError|ERROR|collected" | head -5</automated>
  </verify>
  <done>tests/unit/test_study_runner.py exists; running pytest on it produces errors
  (ImportError for runner module or assertion failures) — the RED phase is confirmed.</done>
</task>

<task type="tdd">
  <name>Task 2 (GREEN): Implement StudyRunner to pass all tests</name>
  <files>src/llenergymeasure/study/runner.py</files>
  <action>
Implement `src/llenergymeasure/study/runner.py`. Write only what is needed to pass the
tests from Task 1. Follow the design spec in `.product/designs/experiment-isolation.md`
exactly — the pseudocode there is the authoritative implementation pattern.

Key implementation requirements (all locked in 11-CONTEXT.md and product decisions):

**Module-level:**
```python
# _calculate_timeout: generous heuristic, no model-size scaling
def _calculate_timeout(config: ExperimentConfig) -> int:
    return max(config.n * 2, 600)  # 2s/prompt estimate, minimum 10 minutes
```

**_run_experiment_worker (runs inside child process):**
- Install `signal.signal(signal.SIGINT, signal.SIG_IGN)` at the top — parent owns SIGINT
- Send `{"event": "started", "config_hash": ...}` to progress_queue before running
- Call the actual experiment runner (use `ExperimentOrchestrator` or the v2.0 equivalent)
- On success: put `{"event": "completed"}` to queue, send ExperimentResult via conn
- On exception: send `{"type": ..., "message": ..., "traceback": traceback.format_exc()}`
  to conn first (best-effort — pipe may be broken on timeout), put failed event to queue, re-raise
- Always: `conn.close()` in finally block

**StudyRunner:**
```python
class StudyRunner:
    def __init__(self, study: StudyConfig, manifest_writer: ManifestWriter) -> None:
        self.study = study
        self.manifest = manifest_writer
        self._interrupted = False

    def run(self) -> list[ExperimentResult | dict]:
        """Run all experiments; return list of results (success or failure dicts)."""
        from llenergymeasure.study.grid import apply_cycles, CycleOrder
        ordered = apply_cycles(
            self.study.experiments,
            self.study.execution.n_cycles,
            CycleOrder(self.study.execution.cycle_order),
            self.study.study_design_hash or "",
            self.study.execution.shuffle_seed,
        )
        mp_ctx = multiprocessing.get_context("spawn")
        results = []
        for config in ordered:
            result = self._run_one(config, mp_ctx)
            results.append(result)
        return results

    def _run_one(self, config, mp_ctx) -> ExperimentResult | dict:
        """Spawn subprocess, collect result or failure dict."""
        from llenergymeasure.domain.experiment import compute_measurement_config_hash
        config_hash = compute_measurement_config_hash(config)
        cycle = 1  # cycle tracking deferred to Phase 12 wiring
        timeout = _calculate_timeout(config)

        child_conn, parent_conn = mp_ctx.Pipe(duplex=False)
        progress_queue = mp_ctx.Queue()

        p = mp_ctx.Process(
            target=_run_experiment_worker,
            args=(config, child_conn, progress_queue),
            daemon=False,  # daemon=False: clean CUDA teardown
        )

        consumer = threading.Thread(
            target=_consume_progress_events,
            args=(progress_queue,),
            daemon=True,
        )
        consumer.start()

        p.start()
        child_conn.close()
        p.join(timeout=timeout)

        progress_queue.put(None)  # sentinel — covers SIGKILL path too
        consumer.join()

        return _collect_result(p, parent_conn, config, timeout)
```

**_collect_result:**
- If `p.is_alive()`: call `p.kill(); p.join()` — SIGKILL (not SIGTERM). Return failure dict.
- If `p.exitcode != 0`: read from pipe if data available, return failure dict.
- If success: `parent_conn.recv()`. Handle `{"__file__": str}` file-based IPC path.

**_consume_progress_events:** daemon thread; loops on `queue.get()` until `None` sentinel.
No display logic yet — just consume and discard events. Display wiring is Phase 12.

**ManifestWriter integration in _run_one:**
- Call `self.manifest.mark_running(config_hash, cycle)` before p.start()
- On success: `self.manifest.mark_completed(config_hash, cycle, result_file="")`
- On failure: `self.manifest.mark_failed(config_hash, cycle, error_type, error_message)`

Note: `experiment_timeout_seconds` field in ExecutionConfig is the user-override escape hatch
(CONTEXT.md). Check `getattr(self.study.execution, 'experiment_timeout_seconds', None)` and
use it as the timeout if set; fall back to `_calculate_timeout(config)`.

Do NOT add `experiment_timeout_seconds` to ExecutionConfig in this plan — that field addition
belongs in Phase 12 config wiring. Use `getattr` with default None for forward-compatibility.

Import guard: use `from __future__ import annotations` and TYPE_CHECKING for ExperimentResult
to avoid circular imports at module level.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_runner.py -x -q 2>&1 | tail -5</automated>
  </verify>
  <done>All tests in tests/unit/test_study_runner.py pass. No regressions in existing test suite:
  `pytest tests/unit/ -q` passes.</done>
</task>

<task type="tdd">
  <name>Task 3 (REFACTOR): Clean up and export StudyRunner</name>
  <files>
    src/llenergymeasure/study/__init__.py
    src/llenergymeasure/study/runner.py
    tests/unit/test_study_runner.py
  </files>
  <action>
Refactor and wire up exports. Do not change behaviour — tests must still pass.

1. **`src/llenergymeasure/study/__init__.py`**: Add `StudyRunner` to exports if not already there.
   Check current contents first; add only what is missing.

2. **`src/llenergymeasure/study/runner.py`**: Final clean-up pass:
   - Ensure module docstring explains the subprocess isolation rationale briefly
   - Ensure `__all__` lists `["StudyRunner", "_run_experiment_worker", "_calculate_timeout"]`
   - Verify all imports are at module level (not inside function bodies) except where
     circular import prevention requires deferred import (use TYPE_CHECKING pattern)
   - Add inline comments at key decision points:
     - `daemon=False` → "# daemon=False: clean CUDA teardown if parent exits unexpectedly"
     - `get_context("spawn")` → "# spawn: CUDA-safe; fork causes silent CUDA corruption (CP-1)"
     - `p.kill()` → "# SIGKILL: SIGTERM may be ignored by hung CUDA operations"
     - `signal.SIG_IGN` → "# parent owns SIGINT; child ignores it"

3. **`tests/unit/test_study_runner.py`**: Verify the test file has a docstring explaining
   the test strategy (mock-based, no GPU). If tests relied on implementation details of
   the internal _run_one that changed during refactor, update assertions to match final API.

Run full unit suite as final check.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/ -q 2>&1 | tail -5</automated>
  </verify>
  <done>
  - `from llenergymeasure.study import StudyRunner` succeeds
  - All tests pass (no regressions)
  - runner.py has `__all__` and inline decision comments
  </done>
</task>

</tasks>

<verification>
After all three tasks complete, verify the phase-level success criteria:

```bash
# 1. spawn context confirmed
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool
grep -n "get_context.*spawn" src/llenergymeasure/study/runner.py

# 2. daemon=False confirmed
grep -n "daemon=False" src/llenergymeasure/study/runner.py

# 3. SIGKILL confirmed (p.kill not p.terminate for timeout)
grep -n "p\.kill()" src/llenergymeasure/study/runner.py

# 4. SIGINT ignored in worker
grep -n "SIG_IGN" src/llenergymeasure/study/runner.py

# 5. All unit tests pass
python -m pytest tests/unit/ -q 2>&1 | tail -3
```
</verification>

<success_criteria>
1. `src/llenergymeasure/study/runner.py` exists with `StudyRunner`, `_run_experiment_worker`,
   `_calculate_timeout`, `_collect_result`, `_consume_progress_events`
2. `multiprocessing.get_context("spawn")` is the only context creation call — never fork
3. `daemon=False` on every Process creation
4. Timeout path uses `p.kill()` (SIGKILL); `p.terminate()` is NOT called on timeout
5. Failed subprocess sends structured error dict `{type, message, traceback}` through Pipe
6. `_run_experiment_worker` installs `signal.signal(signal.SIGINT, signal.SIG_IGN)` at entry
7. All 10 test cases pass; full unit suite passes
</success_criteria>

<output>
After completion, create `.planning/phases/11-subprocess-isolation-and-studyrunner/11-01-SUMMARY.md`

Frontmatter:
```
---
plan: 11-01
status: complete
files_modified:
  - src/llenergymeasure/study/runner.py
  - src/llenergymeasure/study/__init__.py
  - tests/unit/test_study_runner.py
tests_added: <count>
tests_total: <count>
---
```
</output>
