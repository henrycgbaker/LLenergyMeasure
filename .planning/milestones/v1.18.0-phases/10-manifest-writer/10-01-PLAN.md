---
phase: 10-manifest-writer
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/study/manifest.py
  - tests/unit/test_study_manifest.py
autonomous: true
requirements: [STU-08, STU-09, RES-14, RES-NEW-01]

must_haves:
  truths:
    - "ManifestWriter.mark_running(), mark_completed(), and mark_failed() each produce a valid manifest.json via atomic os.replace()"
    - "StudyManifest and StudyResult are distinct Pydantic models with no inheritance relationship"
    - "Study output directory follows {study_name}_{timestamp}/ layout"
    - "Experiment result files use flat {model}_{backend}_{precision}_{hash[:8]}.json naming"
    - "manifest.json sits at study directory root with indent=2 pretty-printed JSON"
    - "Manifest write failure logs warning and continues (does not abort the study)"
    - "Directory creation failure raises StudyError immediately"
  artifacts:
    - path: "src/llenergymeasure/study/manifest.py"
      provides: "ExperimentManifestEntry, StudyManifest, ManifestWriter"
      exports: ["ExperimentManifestEntry", "StudyManifest", "ManifestWriter", "create_study_dir", "experiment_result_filename"]
    - path: "tests/unit/test_study_manifest.py"
      provides: "TDD test suite for manifest models and writer"
      min_lines: 150
  key_links:
    - from: "src/llenergymeasure/study/manifest.py"
      to: "src/llenergymeasure/results/persistence.py"
      via: "_atomic_write() reuse"
      pattern: "from llenergymeasure\\.results\\.persistence import _atomic_write"
    - from: "src/llenergymeasure/study/manifest.py"
      to: "src/llenergymeasure/config/models.py"
      via: "StudyConfig and ExperimentConfig imports"
      pattern: "from llenergymeasure\\.config\\.models import"
    - from: "src/llenergymeasure/study/manifest.py"
      to: "src/llenergymeasure/exceptions.py"
      via: "StudyError for directory creation failure"
      pattern: "from llenergymeasure\\.exceptions import StudyError"
---

<objective>
Implement `StudyManifest` Pydantic model, `ManifestWriter` with atomic writes, and study output directory layout — the checkpoint infrastructure that records every experiment's state during a study run.

Purpose: Every study run produces a corruption-proof checkpoint file (manifest.json) so interrupted studies leave a readable record of what completed. This is the foundation for future `--resume` support (M4).

Output: `study/manifest.py` with models and writer, comprehensive TDD test suite.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.product/designs/study-resume.md

<interfaces>
<!-- Key types and contracts the executor needs. Extracted from codebase. -->

From src/llenergymeasure/results/persistence.py:
```python
def _atomic_write(content: str, path: Path) -> None:
    """Write content to path atomically via temp file + os.replace().
    Uses POSIX rename semantics — atomic on same filesystem.
    Cleans up temp file on failure.
    """
```

From src/llenergymeasure/config/models.py:
```python
class ExecutionConfig(BaseModel):
    n_cycles: int = Field(default=1, ge=1)
    cycle_order: Literal["sequential", "interleaved", "shuffled"] = ...
    config_gap_seconds: float | None = ...
    cycle_gap_seconds: float | None = ...
    shuffle_seed: int | None = ...

class StudyConfig(BaseModel):
    experiments: list[ExperimentConfig] = Field(..., min_length=1)
    name: str | None = None
    execution: ExecutionConfig = Field(default_factory=ExecutionConfig)
    study_design_hash: str | None = None
    skipped_configs: list[dict[str, Any]] = Field(default_factory=list)
```

From src/llenergymeasure/domain/experiment.py:
```python
class ExperimentResult(BaseModel):
    schema_version: str = "2.0"
    experiment_id: str
    measurement_config_hash: str
    backend: str
    ...

class StudyResult(BaseModel):
    """M1 stub — experiments list + name only."""
    experiments: list[ExperimentResult] = Field(default_factory=list)
    name: str | None = None
```

From src/llenergymeasure/config/models.py (ExperimentConfig relevant fields):
```python
class ExperimentConfig(BaseModel):
    model: str = Field(...)  # e.g. "meta-llama/Llama-3.1-8B"
    backend: str = Field(default="pytorch")
    precision: str = Field(default="fp32")
    ...
```

From src/llenergymeasure/exceptions.py:
```python
class StudyError(LLEMError):
    """Error during study orchestration."""
```

From src/llenergymeasure/__init__.py:
```python
__version__: str = "1.17.0"
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: StudyManifest model, ManifestWriter, and study output helpers (TDD — RED then GREEN)</name>
  <files>
    src/llenergymeasure/study/manifest.py
    tests/unit/test_study_manifest.py
  </files>
  <action>
**RED phase — write tests first**, then **GREEN phase — implement to pass them**.

### Test file: `tests/unit/test_study_manifest.py`

Write the following test cases (RED — all must fail initially because manifest.py does not exist yet):

**Model tests (ExperimentManifestEntry + StudyManifest):**
1. `test_experiment_manifest_entry_defaults` — create with only required fields (config_hash, config_summary, cycle, status), verify optional fields are None
2. `test_experiment_manifest_entry_completed` — create with status="completed", result_file set, started_at/completed_at set, verify all fields populated
3. `test_experiment_manifest_entry_failed` — create with status="failed", error_type and error_message set, verify fields
4. `test_study_manifest_schema_version` — create StudyManifest, verify schema_version="2.0"
5. `test_study_manifest_aggregate_counters` — create with 3 entries (1 completed, 1 pending, 1 failed), verify `total_experiments`, `completed`, `failed`, `pending` counters
6. `test_study_manifest_roundtrip_json` — model_dump_json then model_validate_json, verify equality
7. `test_study_manifest_distinct_from_study_result` — import both StudyManifest and StudyResult, assert they are different classes (not aliases, no inheritance)

**ManifestWriter tests:**
8. `test_manifest_writer_creates_initial_manifest` — construct ManifestWriter with a StudyConfig (2 experiments, 2 cycles), verify manifest.json written to disk with 4 entries all status="pending"
9. `test_manifest_writer_mark_running` — call mark_running(config_hash, cycle), verify entry status="running" and started_at is set in re-read JSON
10. `test_manifest_writer_mark_completed` — call mark_completed(config_hash, cycle, result_file), verify entry status="completed", result_file set, completed_at set
11. `test_manifest_writer_mark_failed` — call mark_failed(config_hash, cycle, error_type="RuntimeError", error_message="boom"), verify entry status="failed", error fields set
12. `test_manifest_writer_uses_atomic_write` — mock `_atomic_write` from persistence, verify ManifestWriter calls it (not raw write_text)
13. `test_manifest_writer_write_failure_logs_warning` — mock `_atomic_write` to raise OSError, verify ManifestWriter catches it, logs warning, does not raise
14. `test_manifest_writer_find_raises_for_unknown` — call mark_running with a non-existent config_hash, verify KeyError raised

**Study output directory helpers:**
15. `test_create_study_dir_layout` — call `create_study_dir(name="batch-sweep", output_dir=tmp_path)`, verify directory created matching `{name}_{timestamp}` pattern, returns Path
16. `test_create_study_dir_raises_study_error_on_failure` — mock Path.mkdir to raise PermissionError, verify StudyError raised (not PermissionError)
17. `test_experiment_result_filename` — call `experiment_result_filename(model="meta-llama/Llama-3.1-8B", backend="pytorch", precision="bf16", config_hash="abcdef1234567890")`, verify returns `meta-llama-llama-3.1-8b_pytorch_bf16_abcdef12.json` (lowered, slashes to hyphens, hash truncated to 8)
18. `test_experiment_result_filename_parquet` — same as above but with `extension=".parquet"`, verify `.parquet` suffix

**Config summary helper:**
19. `test_config_summary_from_experiment` — call `build_config_summary(ExperimentConfig(...))`, verify returns `"pytorch / model-name / bf16"` style string

### Implementation: `src/llenergymeasure/study/manifest.py`

After tests are written and confirmed RED, implement:

```python
"""Study manifest — checkpoint model and atomic writer."""

from __future__ import annotations

import logging
import os
from datetime import datetime, timezone
from pathlib import Path
from typing import TYPE_CHECKING, Literal

from pydantic import BaseModel, Field

from llenergymeasure.exceptions import StudyError
from llenergymeasure.results.persistence import _atomic_write

if TYPE_CHECKING:
    from llenergymeasure.config.models import ExperimentConfig, StudyConfig

logger = logging.getLogger(__name__)
```

**ExperimentManifestEntry** — Pydantic model:
- `config_hash: str` — identifies the experiment (from ExperimentConfig's measurement_config_hash)
- `config_summary: str` — human-readable: "pytorch / llama-3.1-8b / bf16"
- `cycle: int` — which study cycle (1-indexed)
- `status: Literal["pending", "running", "completed", "failed"]`
- `result_file: str | None = None` — relative path to result JSON
- `error_type: str | None = None` — exception class name if failed
- `error_message: str | None = None` — exception str if failed
- `started_at: datetime | None = None`
- `completed_at: datetime | None = None`

**StudyManifest** — Pydantic model:
- `schema_version: str = "2.0"`
- `study_name: str`
- `study_design_hash: str` — from StudyConfig (NOT study_yaml_hash — per CONTEXT.md decision)
- `llenergymeasure_version: str`
- `started_at: datetime`
- `completed_at: datetime | None = None`
- `total_experiments: int` — aggregate counter
- `completed: int = 0` — aggregate counter
- `failed: int = 0` — aggregate counter
- `pending: int` — aggregate counter (= total - completed - failed)
- `experiments: list[ExperimentManifestEntry]`
- Add a `model_config = {"extra": "forbid"}` to both models

NOTE: The design doc shows `study_yaml_hash` but CONTEXT.md explicitly says "Record `study_design_hash` only (not `study_yaml_hash`)". Honor the CONTEXT.md decision.

**ManifestWriter** class:
- `__init__(self, study: StudyConfig, study_dir: Path)` — builds initial manifest from study, writes to disk immediately
- `mark_running(self, config_hash: str, cycle: int)` — sets entry status="running", started_at=now, recounts, writes
- `mark_completed(self, config_hash: str, cycle: int, result_file: str)` — sets entry status="completed", result_file, completed_at=now, recounts, writes
- `mark_failed(self, config_hash: str, cycle: int, error_type: str, error_message: str)` — sets entry status="failed", error fields, completed_at=now, recounts, writes
- `_find(self, config_hash: str, cycle: int) -> ExperimentManifestEntry` — linear scan, raises KeyError
- `_recount(self)` — updates completed/failed/pending counters from entries
- `_write(self)` — calls `_atomic_write(self.manifest.model_dump_json(indent=2), self.path)`, wrapped in try/except that logs warning on failure (per CONTEXT.md decision: "log warning and continue the study")
- `_build_entries(study)` — static method: iterate study.experiments, pair with cycle numbers (derive cycle from position in the list using n_configs = len(unique configs) pattern from apply_cycles), create ExperimentManifestEntry for each with status="pending"

**build_config_summary(experiment: ExperimentConfig) -> str** — public helper:
- Returns `"{backend} / {model_slug} / {precision}"` where model_slug is the model name with `/` replaced by `-`, lowered, truncated to 30 chars if needed
- This is the `config_summary` field value (per CONTEXT.md: "auto-generated from sweep dimensions")

**create_study_dir(name: str | None, output_dir: Path) -> Path** — public function:
- Creates `{name}_{timestamp}/` directory (or `study_{timestamp}/` if name is None)
- Timestamp format: `%Y-%m-%dT%H-%M-%S` (colons replaced with hyphens for filesystem safety)
- `output_dir.mkdir(parents=True, exist_ok=True)` first, then study_dir.mkdir()
- On mkdir failure: catch OSError and raise `StudyError("Failed to create study directory: ...")`
- Returns the created directory Path

**experiment_result_filename(model: str, backend: str, precision: str, config_hash: str, extension: str = ".json") -> str** — public function:
- Returns `{model_slug}_{backend}_{precision}_{hash[:8]}{extension}`
- model_slug: model name with `/` replaced by `-`, lowered
- Per CONTEXT.md: "Experiment result file naming: `{model}_{backend}_{precision}_{hash[:8]}.json`"

Use `datetime.now(timezone.utc)` (not deprecated `datetime.utcnow()`).

Run tests after GREEN: `python -m pytest tests/unit/test_study_manifest.py -x -v`

All 19 tests must pass. Also run: `python -m pytest tests/ -x --timeout=60` for full suite regression.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_study_manifest.py -x -v</automated>
  </verify>
  <done>
    - ExperimentManifestEntry and StudyManifest are distinct Pydantic models (not aliases of StudyResult)
    - ManifestWriter writes manifest.json via _atomic_write after every state transition
    - mark_running, mark_completed, mark_failed all update correct entry and recount aggregates
    - Manifest write failure logs warning (does not raise)
    - create_study_dir produces {name}_{timestamp}/ layout, raises StudyError on failure
    - experiment_result_filename produces correct flat-file naming pattern
    - 19 tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire exports and run full regression</name>
  <files>
    src/llenergymeasure/study/__init__.py
  </files>
  <action>
Update `src/llenergymeasure/study/__init__.py` to export the new manifest types:

```python
"""Study module — sweep expansion, cycle ordering, manifest, runner."""

from llenergymeasure.study.manifest import (
    ExperimentManifestEntry,
    ManifestWriter,
    StudyManifest,
    create_study_dir,
    experiment_result_filename,
)

__all__ = [
    "ExperimentManifestEntry",
    "ManifestWriter",
    "StudyManifest",
    "create_study_dir",
    "experiment_result_filename",
]
```

Then run the full test suite to ensure no regressions:
`python -m pytest tests/ -x --timeout=60`

And run mypy on the new module:
`python -m mypy src/llenergymeasure/study/manifest.py --ignore-missing-imports`

Both must pass cleanly.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/ -x --timeout=60 && python -m mypy src/llenergymeasure/study/manifest.py --ignore-missing-imports</automated>
  </verify>
  <done>
    - study/__init__.py exports ExperimentManifestEntry, ManifestWriter, StudyManifest, create_study_dir, experiment_result_filename
    - Full test suite passes with zero regressions
    - mypy reports no errors on manifest.py
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/unit/test_study_manifest.py -x -v` — all 19 manifest tests pass
2. `python -m pytest tests/ -x --timeout=60` — full suite passes (no regressions)
3. `python -m mypy src/llenergymeasure/study/manifest.py --ignore-missing-imports` — clean
4. `python -c "from llenergymeasure.study import StudyManifest, ManifestWriter, create_study_dir"` — imports work
5. `python -c "from llenergymeasure.study.manifest import StudyManifest; from llenergymeasure import StudyResult; assert StudyManifest is not StudyResult"` — distinct types confirmed
</verification>

<success_criteria>
- STU-08: StudyManifest written after each experiment state transition (mark_running/mark_completed/mark_failed all call _write)
- STU-09: ManifestWriter uses atomic os.replace() via _atomic_write from persistence.py
- RES-14: StudyManifest is a distinct Pydantic model from StudyResult (checkpoint vs final return)
- RES-NEW-01: create_study_dir produces {study_name}_{timestamp}/ layout; experiment_result_filename produces flat file naming
- Manifest write failure logs warning and continues (does not abort study)
- Directory creation failure raises StudyError (fast-fail)
- All aggregate counters (total, completed, failed, pending) maintained automatically
</success_criteria>

<output>
After completion, create `.planning/phases/10-manifest-writer/10-01-SUMMARY.md`
</output>
