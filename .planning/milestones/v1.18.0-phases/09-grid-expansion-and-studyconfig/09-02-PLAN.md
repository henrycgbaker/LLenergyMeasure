---
phase: 09-grid-expansion-and-studyconfig
plan: 02
type: execute
wave: 2
depends_on: [09-01]
files_modified:
  - src/llenergymeasure/config/loader.py
  - src/llenergymeasure/study/grid.py
  - tests/unit/test_config_loader.py
  - tests/unit/test_study_grid.py
autonomous: true
requirements: [CFG-12]

must_haves:
  truths:
    - "load_study_config() takes a file path and returns a resolved StudyConfig"
    - "Sweep resolution happens at YAML parse time, before Pydantic validation of individual ExperimentConfigs"
    - "load_study_config() calls expand_grid() then apply_cycles() then constructs StudyConfig with hash"
    - "Pre-flight summary string shows hash, config count, cycle count, total runs, order mode"
    - "Skipped configs shown with per-skip reason; >50% skip rate shows warning"
    - "Empty study (no sweep, no experiments) raises ConfigError"
    - "All-invalid study raises ConfigError with skip reasons"
    - "CLI overrides on execution block merge correctly"
  artifacts:
    - path: "src/llenergymeasure/config/loader.py"
      provides: "load_study_config() public function"
      exports: ["load_study_config"]
    - path: "src/llenergymeasure/study/grid.py"
      provides: "format_preflight_summary() display function"
      exports: ["format_preflight_summary"]
    - path: "tests/unit/test_config_loader.py"
      provides: "Tests for load_study_config()"
    - path: "tests/unit/test_study_grid.py"
      provides: "Tests for format_preflight_summary()"
  key_links:
    - from: "src/llenergymeasure/config/loader.py"
      to: "src/llenergymeasure/study/grid.py"
      via: "imports expand_grid, apply_cycles, compute_study_design_hash, CycleOrder"
      pattern: "from llenergymeasure\\.study\\.grid import"
    - from: "src/llenergymeasure/config/loader.py"
      to: "src/llenergymeasure/config/models.py"
      via: "imports ExecutionConfig, StudyConfig"
      pattern: "from llenergymeasure\\.config\\.models import.*ExecutionConfig.*StudyConfig"
---

<objective>
Implement load_study_config() as the public entry point for study YAML loading, and the pre-flight summary display function.

Purpose: This is the glue that connects YAML file loading to grid expansion — the CFG-12 requirement that sweep resolution happens at YAML parse time, before Pydantic validation. It also delivers the pre-flight display function that Phase 12 (CLI) will call.

Output: load_study_config() in config/loader.py; format_preflight_summary() in study/grid.py; integration-level tests verifying the full YAML → StudyConfig pipeline.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md
@.planning/phases/09-grid-expansion-and-studyconfig/09-CONTEXT.md
@.planning/phases/09-grid-expansion-and-studyconfig/09-RESEARCH.md
@.planning/phases/09-grid-expansion-and-studyconfig/09-01-SUMMARY.md

<interfaces>
<!-- Contracts from Plan 01 that this plan depends on. -->

From src/llenergymeasure/study/grid.py (created in 09-01):
```python
class CycleOrder(StrEnum):
    SEQUENTIAL = "sequential"
    INTERLEAVED = "interleaved"
    SHUFFLED = "shuffled"

@dataclass
class SkippedConfig:
    raw_config: dict[str, Any]
    reason: str
    errors: list[dict[str, Any]]
    @property
    def short_label(self) -> str: ...
    def to_dict(self) -> dict[str, Any]: ...

def expand_grid(
    raw_study: dict[str, Any],
    study_yaml_path: Path | None = None,
) -> tuple[list[ExperimentConfig], list[SkippedConfig]]: ...

def compute_study_design_hash(experiments: list[ExperimentConfig]) -> str: ...

def apply_cycles(
    experiments: list[ExperimentConfig],
    n_cycles: int,
    cycle_order: CycleOrder,
    study_design_hash: str,
    shuffle_seed: int | None = None,
) -> list[ExperimentConfig]: ...
```

From src/llenergymeasure/config/models.py (expanded in 09-01):
```python
class ExecutionConfig(BaseModel):
    model_config = {"extra": "forbid"}
    n_cycles: int = Field(default=1, ge=1)
    cycle_order: Literal["sequential", "interleaved", "shuffled"] = Field(default="sequential")
    config_gap_seconds: float | None = Field(default=None, ge=0.0)
    cycle_gap_seconds: float | None = Field(default=None, ge=0.0)
    shuffle_seed: int | None = Field(default=None)

class StudyConfig(BaseModel):
    model_config = {"extra": "forbid"}
    experiments: list[ExperimentConfig] = Field(..., min_length=1)
    name: str | None = Field(default=None)
    execution: ExecutionConfig = Field(default_factory=ExecutionConfig)
    study_design_hash: str | None = Field(default=None)
    skipped_configs: list[dict] = Field(default_factory=list)
```

From src/llenergymeasure/config/loader.py (existing):
```python
def _load_file(path: Path | str) -> dict[str, Any]: ...
def deep_merge(base: dict[str, Any], overlay: dict[str, Any]) -> dict[str, Any]: ...
# __all__ = ["load_experiment_config", "deep_merge"]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement load_study_config() and format_preflight_summary()</name>
  <files>
    src/llenergymeasure/config/loader.py
    src/llenergymeasure/study/grid.py
  </files>
  <action>
**1. Add `load_study_config()` to config/loader.py:**

Add imports at the top of loader.py:
```python
from llenergymeasure.config.models import ExecutionConfig, StudyConfig
from llenergymeasure.study.grid import (
    CycleOrder,
    apply_cycles,
    compute_study_design_hash,
    expand_grid,
)
```

Add `load_study_config` to the `__all__` list.

Implement the function:

```python
def load_study_config(
    path: Path | str,
    cli_overrides: dict[str, Any] | None = None,
) -> StudyConfig:
    """Load, expand, and validate a study YAML file.

    Resolution order:
      1. Load YAML file
      2. Apply CLI overrides on execution block
      3. Parse execution block (Pydantic validates it)
      4. expand_grid() — Cartesian product + Pydantic validation of each ExperimentConfig
      5. Guard: empty or all-invalid → ConfigError
      6. compute_study_design_hash() over valid experiments
      7. apply_cycles() for cycle ordering
      8. Construct and return StudyConfig

    This is the CFG-12 contract: sweep resolution at YAML parse time, before
    Pydantic sees the individual ExperimentConfig objects.

    Args:
        path: Path to study YAML file.
        cli_overrides: Optional dict of CLI flag overrides for execution block
            (e.g. {"execution": {"n_cycles": 5}}). Phase 12 translates
            --cycles/--order/--no-gaps flags into this dict.

    Returns:
        Resolved StudyConfig with ordered experiments and study_design_hash.

    Raises:
        ConfigError: File not found, parse error, base file missing, ALL configs invalid,
            empty study (no sweep and no experiments).
        ValidationError: Pydantic structural errors on ExecutionConfig pass through.
    """
    path = Path(path)
    raw = _load_file(path)  # reuse existing _load_file — raises ConfigError

    # Apply CLI overrides (Phase 12 translates --cycles etc. into this dict)
    if cli_overrides:
        raw = deep_merge(raw, cli_overrides)

    # Strip version key (same as experiment loader)
    raw.pop("version", None)

    # Extract study-level metadata
    name = raw.get("name")

    # Parse execution block — Pydantic validates it
    execution = ExecutionConfig(**(raw.get("execution") or {}))

    # Expand sweep → list[ExperimentConfig], collect skipped
    # This is CFG-12: sweep resolution at YAML parse time, before Pydantic
    valid_experiments, skipped = expand_grid(raw, study_yaml_path=path)

    # Guard: empty or all-invalid
    total = len(valid_experiments) + len(skipped)
    if total == 0:
        raise ConfigError(
            "Study produced no experiments (empty sweep and no experiments: list)."
        )
    if not valid_experiments:
        skip_details = "\n".join(
            f"  {s.short_label}: {s.reason}" for s in skipped[:5]
        )
        raise ConfigError(
            f"All {total} generated configs are invalid. "
            "Nothing to run. Check sweep dimensions against backend constraints.\n"
            + skip_details
        )

    # Compute study_design_hash BEFORE applying cycles (hash is over unique configs)
    study_hash = compute_study_design_hash(valid_experiments)

    # Apply cycle ordering to produce execution sequence
    ordered = apply_cycles(
        valid_experiments,
        n_cycles=execution.n_cycles,
        cycle_order=CycleOrder(execution.cycle_order),
        study_design_hash=study_hash,
        shuffle_seed=execution.shuffle_seed,
    )

    return StudyConfig(
        experiments=ordered,
        name=name,
        execution=execution,
        study_design_hash=study_hash,
        skipped_configs=[s.to_dict() for s in skipped],
    )
```

**2. Add `format_preflight_summary()` to study/grid.py:**

Add this function after the existing functions in grid.py:

```python
def format_preflight_summary(
    study_config: StudyConfig,
    skipped: list[SkippedConfig] | None = None,
) -> str:
    """Return pre-flight display string for terminal output.

    Format (CONTEXT.md locked):
        Study [abc123de]: 4 configs x 3 cycles = 12 runs
        Order: interleaved
        Skipping 2/6: (per-skip log line with reason)
          - pytorch, fp32: [Pydantic message]
        WARNING: 67% of sweep configs are invalid — check your sweep dimensions.

    Args:
        study_config: Resolved StudyConfig (after load_study_config).
        skipped: Optional list of SkippedConfig from expand_grid.
            If None, derives skip info from study_config.skipped_configs.

    Returns:
        Multi-line string for terminal display.
    """
    n_cycles = study_config.execution.n_cycles
    n_runs = len(study_config.experiments)
    # n_configs is the unique config count (before cycle multiplication)
    n_configs = n_runs // n_cycles if n_cycles > 0 else n_runs
    hash_display = study_config.study_design_hash or "unknown"

    lines = [
        f"Study [{hash_display}]: {n_configs} configs x {n_cycles} cycles = {n_runs} runs",
        f"Order: {study_config.execution.cycle_order}",
    ]

    # Handle skipped configs display
    skipped_dicts = study_config.skipped_configs if skipped is None else [s.to_dict() for s in skipped]
    if skipped_dicts:
        total_generated = n_configs + len(skipped_dicts)
        lines.append(f"Skipping {len(skipped_dicts)}/{total_generated}:")
        for s in skipped_dicts:
            label = s.get("short_label", "unknown")
            reason = s.get("reason", "unknown error")
            lines.append(f"  - {label}: {reason}")

        skip_rate = len(skipped_dicts) / total_generated if total_generated > 0 else 0
        if skip_rate > 0.5:
            lines.append(
                f"  WARNING: {skip_rate:.0%} of sweep configs are invalid "
                "-- check your sweep dimensions."
            )

    return "\n".join(lines)
```

Add the import for `StudyConfig` at the top of grid.py (use TYPE_CHECKING to avoid circular):
```python
from __future__ import annotations
from typing import TYPE_CHECKING, Any
if TYPE_CHECKING:
    from llenergymeasure.config.models import StudyConfig
```

Wait — `StudyConfig` is needed at runtime for the type annotation of `format_preflight_summary`. Since the function only reads attributes, use the `from __future__ import annotations` approach (already present) so the annotation is a string. The import under `TYPE_CHECKING` block is sufficient.

Add `format_preflight_summary` to the grid.py module's logical exports.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.config.loader import load_study_config; from llenergymeasure.study.grid import format_preflight_summary; print('imports OK')"</automated>
  </verify>
  <done>
    load_study_config() is importable from config.loader and has the full resolution pipeline. format_preflight_summary() is importable from study.grid. Both functions are implemented, not stubs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Write integration tests for load_study_config() and format_preflight_summary()</name>
  <files>
    tests/unit/test_config_loader.py
    tests/unit/test_study_grid.py
  </files>
  <action>
**Add tests to test_config_loader.py** (append to existing file, in a new test class or section):

1. **test_load_study_config_grid_sweep:**
   Write a temp YAML file with model, backend, sweep block → call `load_study_config(path)` → assert returns StudyConfig with correct experiment count, study_design_hash is 16-char hex, execution defaults applied.

2. **test_load_study_config_explicit_experiments:**
   Write a temp YAML with `experiments:` list → assert correct count.

3. **test_load_study_config_combined_mode:**
   Write a temp YAML with both `sweep:` and `experiments:` → assert total = sweep count + explicit count.

4. **test_load_study_config_with_execution_block:**
   Write a temp YAML with `execution: {n_cycles: 3, cycle_order: interleaved}` → assert `study_config.execution.n_cycles == 3` and `len(study_config.experiments) == base_count * 3`.

5. **test_load_study_config_cli_overrides:**
   Load a study YAML, pass `cli_overrides={"execution": {"n_cycles": 5}}` → assert execution.n_cycles == 5.

6. **test_load_study_config_with_base:**
   Write a temp experiment.yaml (model, backend, dataset), write a study.yaml with `base: experiment.yaml` and a `sweep:` block → assert experiments inherit from base.

7. **test_load_study_config_empty_study_raises:**
   Write a temp YAML with only `execution:` block (no model, no sweep, no experiments) → assert raises ConfigError.

8. **test_load_study_config_all_invalid_raises:**
   Write a temp YAML that produces only invalid configs (e.g., sweep over values that fail ExperimentConfig validation) → assert raises ConfigError with "All .* generated configs are invalid".

9. **test_load_study_config_file_not_found:**
   Call with a non-existent path → assert raises ConfigError.

10. **test_load_study_config_hash_excludes_execution:**
    Load same study YAML twice with different execution blocks → assert study_design_hash is identical (execution excluded from hash).

**Add tests to test_study_grid.py** (append to existing file):

11. **test_format_preflight_summary_basic:**
    Create a StudyConfig with 4 experiments, 3 cycles (so 12 runs), interleaved → assert output contains "4 configs x 3 cycles = 12 runs" and "Order: interleaved".

12. **test_format_preflight_summary_with_skipped:**
    Create a StudyConfig with skipped_configs populated → assert output contains "Skipping" line with per-skip reasons.

13. **test_format_preflight_summary_high_skip_rate_warning:**
    Create a StudyConfig where >50% configs were skipped → assert output contains "WARNING" about invalid configs.

14. **test_format_preflight_summary_hash_displayed:**
    Create a StudyConfig with study_design_hash → assert hash appears in output.

Use `tmp_path` for all file-based tests. All temp YAML files should use `yaml.dump()` or raw strings written via `Path.write_text()`.

**Test naming convention:** Follow existing pattern in test_config_loader.py (check existing test function naming style and match it).
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_config_loader.py tests/unit/test_study_grid.py -x -v --tb=short -k "study" 2>&1 | tail -40</automated>
  </verify>
  <done>
    All new study-related tests pass. load_study_config() correctly resolves YAML → StudyConfig through the full pipeline (CFG-12). format_preflight_summary() produces the correct display format. Hash stability across execution changes verified. Error cases (empty, all-invalid, missing file) all raise ConfigError. No regressions in existing loader tests.
  </done>
</task>

</tasks>

<verification>
```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool

# All study-related tests pass
python -m pytest tests/unit/test_study_grid.py tests/unit/test_config_loader.py -v --tb=short -k "study"

# No regressions in full unit test suite
python -m pytest tests/unit/ -x --tb=short

# Type check
python -m mypy src/llenergymeasure/config/loader.py src/llenergymeasure/study/grid.py --ignore-missing-imports

# Full pipeline smoke test — create temp study YAML and load it
python -c "
import tempfile, yaml
from pathlib import Path
from llenergymeasure.config.loader import load_study_config
from llenergymeasure.study.grid import format_preflight_summary

with tempfile.NamedTemporaryFile(suffix='.yaml', mode='w', delete=False) as f:
    yaml.dump({
        'model': 'gpt2',
        'backend': 'pytorch',
        'sweep': {'precision': ['fp16', 'bf16'], 'n': [50, 100]},
        'execution': {'n_cycles': 2, 'cycle_order': 'interleaved'},
    }, f)
    path = f.name

sc = load_study_config(path)
print(f'Experiments: {len(sc.experiments)}')
print(f'Hash: {sc.study_design_hash}')
print(format_preflight_summary(sc))
print('Pipeline OK')
"
```
</verification>

<success_criteria>
- load_study_config() exists in config/loader.py and is in __all__
- Sweep resolution happens at YAML parse time via expand_grid() call BEFORE individual ExperimentConfig Pydantic validation (CFG-12)
- load_study_config() constructs StudyConfig with hash, ordered experiments, and skipped_configs
- format_preflight_summary() returns display string matching CONTEXT.md format
- Pre-flight shows hash, config count, cycle count, total runs, ordering mode
- Skipped configs shown per-skip with reason; >50% skip rate triggers WARNING line
- CLI overrides merge into execution block correctly
- Empty study and all-invalid study both raise ConfigError
- All new and existing tests pass with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/09-grid-expansion-and-studyconfig/09-02-SUMMARY.md`
</output>
