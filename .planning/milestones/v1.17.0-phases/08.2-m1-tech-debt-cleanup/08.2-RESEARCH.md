# Phase 8.2: M1 Tech Debt Cleanup — Research

**Researched:** 2026-02-27
**Domain:** Documentation verification, dead-code removal, broken import cleanup
**Confidence:** HIGH — all findings are based on direct codebase inspection, not external dependencies

---

## Summary

Phase 8.2 is a housekeeping phase with five discrete tasks derived directly from the M1 audit. None involve new external libraries or architectural decisions. Every item has a clear source in the audit report and a clear fix.

The phase falls into two categories:

**Documentation tasks (no code change):** Write Phase 2 VERIFICATION.md, fix REQUIREMENTS.md status drift for CM-15–CM-20/CM-25, and correct `cli/CLAUDE.md`.

**Code cleanup tasks (small, contained changes):** Fix broken imports in `cli/experiment.py` and `cli/utils.py`, and assess/remove five orphaned exports.

The most important design question is whether the five "orphaned exports" are truly orphaned. Research shows they are not all equivalent: `aggregate_results()` and `calculate_efficiency_metrics` have active callers in `cli/results.py` (the latter is missing from the module it's imported from — this is a real broken import, not an orphan). `FlopsEstimator` has active callers in `core/compute_metrics.py`. `StateManager` is the v2.0 canonical state type — it is not orphaned, just imported from a defunct submodule path. Only `SubprocessRunner` / `export_aggregated_to_csv` are genuinely unused in the v2.0 `_run()` pipeline.

**Primary recommendation:** Treat these as five separate assessments before acting. Do not delete anything that has an active caller; fix the broken import chain first to understand what is dead.

---

<phase_requirements>
## Phase Requirements

| ID | Description | Research Support |
|----|-------------|-----------------|
| CFG-01 | Single `ExperimentConfig` with composition + `extra="forbid"` | SUMMARY 02-01 lists as `requirements-completed`. UAT test 2 confirms extra=forbid. Code in `config/models.py`. |
| CFG-02 | Field renames: `model`, `precision`, `n` | SUMMARY 02-01 lists. UAT test 1 confirms v2.0 field names accessible. |
| CFG-03 | `passthrough_kwargs` declared field | SUMMARY 02-01 lists. Confirmed in `config/models.py`. |
| CFG-04 | Shared fields with correct types and defaults | SUMMARY 02-01 lists. UAT test 1 confirms. |
| CFG-05 | Sub-configs: decoder, warmup, baseline | SUMMARY 02-01 lists. |
| CFG-06 | Backend section fields None-optional | SUMMARY 02-01 lists. UAT test 3 confirms. |
| CFG-07 | `lora: LoRAConfig \| None = None` | SUMMARY 02-01 lists. |
| CFG-08 | `PRECISION_SUPPORT`/`DECODING_SUPPORT` dicts in `config/ssot.py` | SUMMARY 02-01 lists. |
| CFG-09 | Cross-validators (3): precision vs backend, decoding vs backend, section mismatch | SUMMARY 02-01 lists. UAT test 4 confirms section mismatch. |
| CFG-10 | None-as-sentinel on backend section fields | SUMMARY 02-01 lists. UAT test 3 confirms. |
| CFG-18 | `load_experiment_config()` in `config/loader.py` | SUMMARY 02-02 lists. UAT test 5 confirms. |
| CFG-19 | `yaml.safe_load` only | SUMMARY 02-02 lists. Code inspection confirms. |
| CFG-20 | YAML errors → ConfigError; ValidationError passthrough | SUMMARY 02-02 lists. UAT test 6 confirms. |
| CFG-21 | XDG path via platformdirs | SUMMARY 02-03 lists. UAT test 7 confirms. |
| CFG-22 | Missing file → all defaults, no error | SUMMARY 02-03 lists. UAT test 7 confirms. |
| CFG-23 | UserConfig sections: output, runners, measurement, execution | SUMMARY 02-03 lists. |
| CFG-24 | Runner precedence: env var → user config → local default | SUMMARY 02-03 lists. UAT test 8 confirms. |
| CFG-25 | Env vars: LLEM_CARBON_INTENSITY, LLEM_DATACENTER_PUE, LLEM_NO_PROMPT | SUMMARY 02-03 lists. UAT test 8 confirms. |
| CFG-26 | `config/introspection.py` with `model_json_schema()` | SUMMARY 02-04 lists as CFG-02/CFG-03 (schema export). UAT tests 9-10 confirm. |
</phase_requirements>

---

## Standard Stack

### Core
No external libraries required. This phase operates entirely on:

| Tool | Purpose |
|------|---------|
| Python built-ins | File reads, string formatting |
| Pydantic v2 | Verifying models exist — read-only inspection |
| pytest (existing) | Running existing test suite to confirm nothing is broken after removal |

### Supporting
None required. All changes are documentation writes, import fixes, and code deletions.

---

## Architecture Patterns

### Pattern 1: VERIFICATION.md Structure

Phase VERIFICATION.md files follow a strict frontmatter + body pattern. The Phase 8.1 VERIFICATION.md at `.planning/phases/08.1-pytorch-result-wiring-fixes/08.1-VERIFICATION.md` is the most recent reference and should be the structural template.

Key sections (from inspection of all existing VERIFICATION.md files):

```
---
phase: {phase-name}
verified: {ISO timestamp}
status: passed | gaps_found
score: N/N must-haves verified
---

# Phase X: Name — Verification Report

## Goal Achievement
### Observable Truths
| # | Truth | Status | Evidence |

### Required Artifacts
| Artifact | Expected | Status | Details |

### Key Link Verification (if relevant)
| From | To | Via | Status |

### Requirements Coverage
| Requirement | Source Plan | Description | Status | Evidence |

### Test Results
...

### Human Verification Required
...

### Verified Commits
...

## Summary
```

For Phase 2's VERIFICATION.md: the 4 SUMMARYs from plans 02-01 through 02-04 are the primary source for the "Verified Commits" section. The UAT results (11/11 passed in `02-UAT.md`) provide the "Test Results" evidence.

**Confidence: HIGH** — directly inspected all VERIFICATION.md files in the project.

### Pattern 2: REQUIREMENTS.md Status Correction

The `.planning/REQUIREMENTS.md` file (not `.product/REQUIREMENTS.md`) is the traceability tracker with Phase + Status columns. The M1 Milestone audit and Phase 5 VERIFICATION.md both cite `.planning/REQUIREMENTS.md` lines 143-148 and 153 as containing Pending status that should be Complete.

The `.planning/REQUIREMENTS.md` uses the format:
```
| RES-06 | Phase 8.1 | Complete | Partial — baseline fields never populated |
```

**Confirmed from inspection:** The `.planning/REQUIREMENTS.md` file at this repo path is the M2 REQUIREMENTS file (requirements for M2 phases). It does NOT contain CM-15–CM-25 rows. The status drift is in the Phase 5 VERIFICATION.md's gap record, which references an older state of the REQUIREMENTS.md tracker. The `05-VERIFICATION.md` notes "lines 143-148 and 153" which no longer applies to the current planning REQUIREMENTS.md structure.

**Action:** Verify whether the drift actually persists by checking what file the audit references. The `.planning/REQUIREMENTS.md` currently only covers M2 requirements — not M1 CM-series requirements. The product REQUIREMENTS.md (`.product/REQUIREMENTS.md`) does not have a Status column.

**Conclusion:** The "status drift" tech debt item may refer to a now-superseded REQUIREMENTS.md (the old planning file). The current `.planning/REQUIREMENTS.md` is the M2 tracker, not the M1 tracker. The M1 audit flagged this, but the REQUIREMENTS.md has since been restructured. This needs careful verification before any changes are made. The traceability table in the current `.planning/REQUIREMENTS.md` already shows Phase 8.2 as the resolver for CFG-01–26 — so the "status drift" fix may simply be updating those entries to "Complete" once Phase 8.2 delivers the VERIFICATION.md.

### Pattern 3: Broken Import Assessment

The M1 audit identifies two broken import chains:

**`cli/experiment.py` (line 40):**
```python
from llenergymeasure.state.experiment_state import (
    ExperimentState,
    ExperimentStatus,
    StateManager,
    compute_config_hash,
)
```
`llenergymeasure.state.experiment_state` does not exist. `llenergymeasure.state` exists and re-exports from `core/state.py`. However `ExperimentStatus` is the v1.x name — the v2.0 enum is `ExperimentPhase`. The entire `cli/experiment.py` is dead code in v2.0 (the v2.0 CLI entry point only imports `run.py` and `config_cmd.py`, not `experiment.py`).

**`cli/utils.py` (lines 16, 119):**
Uses `TYPE_CHECKING` guard and lazy function-body import:
- Line 16: `from llenergymeasure.state.experiment_state import ExperimentState, StateManager` — TYPE_CHECKING only (safe at runtime)
- Line 119: `from llenergymeasure.state.experiment_state import ProcessProgress, ProcessStatus` — inside `update_process_state_from_markers()` function body

The function `update_process_state_from_markers()` in `cli/utils.py` references `ProcessProgress` and `ProcessStatus` which are v1.x types. It is also dead code in v2.0. The `cli/utils.py` module is not imported by the active v2.0 CLI entry point.

**Fix approach:** The audit says "removed or fixed". Given these files are not reachable via the v2.0 CLI entry point (`cli/__init__.py`), the simplest fix is to **remove the broken imports** (or remove the dead functions entirely). Do not "fix" by adding shims to `state.__init__.py` — the v2.0 state module has no `ProcessProgress`/`ProcessStatus`.

### Pattern 4: Orphaned Export Assessment

Each export needs individual assessment:

| Export | Location | Audit Claim | Research Finding |
|--------|----------|-------------|-----------------|
| `aggregate_results()` | `results/aggregation.py` | "no caller in v2.0 _run() pipeline" | **Active caller found:** `cli/results.py:154` calls it; `results/__init__.py` re-exports it. NOT truly orphaned — used in the v2.0 results CLI command. |
| `calculate_efficiency_metrics` | `results/aggregation.py` | (implied by broken import) | **Function does not exist** in `aggregation.py`. `cli/results.py:22` imports it but it's never defined there. This is a **broken import** causing `ImportError` when `cli/experiment.py` (which imports `cli/results.py`) is loaded. The function's _logic_ is in `ExperimentResult` methods directly (`avg_tokens_per_second`, `avg_energy_per_token_j` fields). |
| `export_aggregated_to_csv()` | `results/exporters.py` | "no CLI command or internal caller" | **Confirmed orphaned** — exported via `results/__init__.py` but no caller in v2.0 code path. `cli/results.py` does not call it. |
| `FlopsEstimator` | `core/flops.py` | "superseded by estimate_flops_palm()" | **Active caller found:** `core/compute_metrics.py:265` uses it with fallback chain. `core/__init__.py:20` exports it. The docstring says "Legacy fallback chain" but it IS wired into the active measurement pipeline. NOT safe to remove without understanding `compute_metrics.py`. |
| `StateManager` | `core/state.py` | "not wired into v2.0 _run() pipeline — only used by broken v1.x cli/experiment.py" | `core/state.py` IS the v2.0 state module (re-exported via `state/__init__.py`). The `StateManager` IS used by active v2.0 code (infra/subprocess.py, orchestration). The audit's claim is partially wrong — it's orphaned only in the *CLI _run() pipeline* (the v2.0 `run.py` path doesn't use StateManager), but it IS exported and IS used by `infra/subprocess.py`. |
| `SubprocessRunner` | `infra/subprocess.py` | "created in Phase 1, unused in v2.0" | `infra/__init__.py` exports it. No importer in v2.0 `_run()` path confirmed — v2.0 `cli/run.py` uses `subprocess.Popen` directly. **Confirmed: not called by v2.0 pipeline.** |

**Summary of assessment:**
- `aggregate_results()` — keep, has callers
- `calculate_efficiency_metrics` — **broken import** (function doesn't exist), fix by removing the import and inlining the calculation in `cli/results.py` using existing `ExperimentResult` fields
- `export_aggregated_to_csv()` — remove from `results/__init__.py` re-export (keep the function in `exporters.py` for now or remove it — no external callers)
- `FlopsEstimator` — keep, active caller
- `StateManager` — keep, active in `infra/subprocess.py`
- `SubprocessRunner` — confirmed unused in v2.0 pipeline; safe to remove export, file may be kept for future use

### Anti-Patterns to Avoid

- **Don't delete `FlopsEstimator` or `StateManager`**: Both have active callers. The audit's claim needs cross-referencing against the codebase — research confirms they are in use.
- **Don't shim the broken imports**: `ProcessProgress`/`ProcessStatus` don't exist in v2.0. Adding shims adds dead code. The correct fix is removal.
- **Don't update `.product/REQUIREMENTS.md`**: That file has no Status column. The drift is in `.planning/REQUIREMENTS.md` (the traceability tracker).
- **Don't rewrite `cli/experiment.py`**: It is dead code. The v2.0 CLI never registers it. Options are: remove it entirely, or just remove the broken imports at the top. Given it's 1000+ lines of v1.x code, removing the broken imports (lines 40-45) is the minimal change. Full removal is also acceptable since no v2.0 path reaches it.

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead |
|---------|-------------|-------------|
| VERIFICATION.md structure | Custom format | Copy Phase 8.1's structure exactly |
| Levenshtein distance | Custom impl | Already in config/loader.py |
| UAT evidence | Re-run tests | UAT results are in 02-UAT.md (11/11 passed) |

---

## Common Pitfalls

### Pitfall 1: Confusing `.planning/REQUIREMENTS.md` with `.product/REQUIREMENTS.md`
**What goes wrong:** Editing the product requirements file which has no Status column and is the upstream source of truth.
**How to avoid:** The drift is documented as affecting `.planning/REQUIREMENTS.md`. The current `.planning/REQUIREMENTS.md` is the M2 traceability tracker — it contains the CFG-01–26 rows in its "M1 Gap Closure" traceability section. Updating those rows from Pending to Complete once the VERIFICATION.md exists is the correct action.

### Pitfall 2: Marking `FlopsEstimator` or `aggregate_results()` as orphaned
**What goes wrong:** Deleting them based on the audit's claim without verifying active callers first.
**How to avoid:** Research above confirms both have active callers. Do not remove either.

### Pitfall 3: Removing `cli/experiment.py` entirely
**What goes wrong:** Although dead in v2.0, removing a 1000-line file is higher risk than removing 6 import lines.
**How to avoid:** Minimal fix: remove lines 40-45 (the broken `state.experiment_state` import). The file can be flagged for full removal later once confirmed all tests still pass.

### Pitfall 4: Treating `calculate_efficiency_metrics` as an orphaned export
**What goes wrong:** The audit lists it under "orphaned exports" but the real issue is it's a **missing function** — the import in `cli/results.py:22` fails at runtime. This cascades into a complete `ImportError` for anything that imports `cli/results.py`.
**How to avoid:** Fix the broken import in `cli/results.py`. The calculation is already available via `result.avg_tokens_per_second` and `result.avg_energy_per_token_j` on ExperimentResult. Replace the two call sites in `cli/results.py` (lines 167-171) with direct field access.

### Pitfall 5: Phase 2 VERIFICATION.md score
**What goes wrong:** Setting score to 19/19 (one per CFG requirement) when the phase only had 4 plans covering 11 UAT tests.
**How to avoid:** Structure the VERIFICATION.md around the UAT tests (11 truths matching the 11 UAT results), not the raw requirement count. Each truth maps to one or more requirements. Evidence comes from the 4 SUMMARY frontmatter `requirements-completed` lists and the UAT 11/11 result.

---

## Code Examples

### Replacing `calculate_efficiency_metrics` call in cli/results.py

```python
# BEFORE (broken import):
from llenergymeasure.results.aggregation import aggregate_results, calculate_efficiency_metrics
# ...
metrics = calculate_efficiency_metrics(aggregated)
console.print(f"  Throughput: {metrics['tokens_per_second']:.2f} tok/s")
console.print(f"  Efficiency: {metrics['tokens_per_joule']:.2f} tok/J")

# AFTER (use ExperimentResult fields directly):
from llenergymeasure.results.aggregation import aggregate_results
# ...
console.print(f"  Throughput: {aggregated.avg_tokens_per_second:.2f} tok/s")
tokens_per_joule = aggregated.total_tokens / aggregated.total_energy_j if aggregated.total_energy_j > 0 else 0.0
console.print(f"  Efficiency: {tokens_per_joule:.2f} tok/J")
```

Note: `ExperimentResult` has `tokens_per_joule` as a property (`total_tokens / total_energy_j`). Verify: `grep -n "tokens_per_joule" src/llenergymeasure/domain/experiment.py`.

### Fixing `cli/experiment.py` broken import (minimal fix)

```python
# REMOVE lines 40-45 (the entire state.experiment_state block):
from llenergymeasure.state.experiment_state import (
    ExperimentState,
    ExperimentStatus,
    StateManager,
    compute_config_hash,
)
# The file is dead code — this import runs at module load time and fails.
# After removal the file still won't be imported by v2.0 CLI, but it won't
# pollute sys.modules if ever accidentally imported.
```

### Fixing `cli/utils.py` broken imports

```python
# Line 16 TYPE_CHECKING block — fix the module path:
if TYPE_CHECKING:
    from llenergymeasure.core.state import ExperimentState, StateManager  # was: state.experiment_state

# Line 119 function-body import — ProcessProgress/ProcessStatus don't exist in v2.0.
# The whole update_process_state_from_markers() function is dead code.
# Remove the function or remove the interior import and stub the body.
```

---

## Open Questions

1. **`cli/experiment.py` — remove the file or just the broken imports?**
   - What we know: it is dead code (not imported by v2.0 CLI). It imports the broken `state.experiment_state` module at the top level, causing `ImportError` if the file is imported.
   - What's unclear: whether any external user (test, doc, script) references `cli.experiment`.
   - Recommendation: Run `grep -rn "cli.experiment\|from.*cli.*import.*experiment_cmd"` across tests and docs before deciding. If no callers, remove the file.

2. **CM-15–CM-25 status drift — which file?**
   - What we know: Phase 5 VERIFICATION.md says `.planning/REQUIREMENTS.md` has the drift, referencing "lines 143-148 and 153". The current `.planning/REQUIREMENTS.md` is the M2 traceability file and doesn't have those lines.
   - What's unclear: whether the old M1 REQUIREMENTS.md still exists, or whether the correct action is to update the M1 Gap Closure traceability table in the current file.
   - Recommendation: The traceability table in current `.planning/REQUIREMENTS.md` already has `CFG-01–26 | Phase 8.2 | Pending`. After delivering the VERIFICATION.md, update those rows to `Complete`. There is no separate M1 status tracker to update.

3. **`export_aggregated_to_csv()` — remove function or just remove from `__init__.py`?**
   - What we know: no v2.0 caller. Exported in `results/__init__.py`.
   - What's unclear: whether any user might be calling it via the public API.
   - Recommendation: Remove from `results/__init__.py` `__all__` and the import. Leave the function in `exporters.py` with a deprecation note. This is the safest removal with no breaking change to importers who import directly from `exporters.py`.

---

## Sources

### Primary (HIGH confidence)
- Direct codebase inspection: `src/llenergymeasure/cli/experiment.py`, `cli/utils.py`, `cli/results.py`, `results/aggregation.py`, `results/exporters.py`, `core/flops.py`, `core/state.py`, `infra/subprocess.py` — all read at research time
- `.planning/phases/02-config-system/` — all 4 SUMMARYs and UAT file read directly
- `.planning/M1-MILESTONE-AUDIT.md` — full audit read
- `.planning/phases/08.1-pytorch-result-wiring-fixes/08.1-VERIFICATION.md` — template structure read
- `.planning/phases/05-energy-measurement/05-VERIFICATION.md` — status drift gap verified
- `.planning/REQUIREMENTS.md` — current traceability structure confirmed
- Python import tests (`python3 -c "..."`) — confirmed broken import chains directly

### Secondary (MEDIUM confidence)
- `.planning/phases/01-measurement-foundations/01-VERIFICATION.md` — cross-referenced for VERIFICATION.md structure patterns

---

## Metadata

**Confidence breakdown:**
- Task scope: HIGH — all 5 tasks have clear sources in the audit
- Broken imports: HIGH — confirmed via Python runtime import test
- Orphaned exports: HIGH — confirmed presence/absence of callers via grep
- REQUIREMENTS.md drift: MEDIUM — the exact file/lines need verification; the current file structure differs from what the audit references
- VERIFICATION.md content: HIGH — UAT evidence (11/11) and SUMMARY frontmatter provide the needed evidence

**Research date:** 2026-02-27
**Valid until:** This phase is about fixing M1 tech debt — findings are stable, no external dependencies.
