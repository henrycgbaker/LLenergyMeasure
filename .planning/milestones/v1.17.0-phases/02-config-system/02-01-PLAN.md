---
phase: 02-config-system
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/config/backend_configs.py
  - src/llenergymeasure/config/ssot.py
autonomous: true
requirements: [CFG-01, CFG-02, CFG-03, CFG-04, CFG-05, CFG-06, CFG-08, CFG-09, CFG-10]

must_haves:
  truths:
    - "ExperimentConfig accepts v2.0 field names: model, precision, n, passthrough_kwargs"
    - "ExperimentConfig uses extra='forbid' — unknown YAML keys raise ValidationError"
    - "Three cross-validators enforce structural rules: backend-section/backend mismatch, fp16/bf16+cpu, passthrough_kwargs key collision"
    - "PyTorchConfig, VLLMConfig, TensorRTConfig use extra='forbid' with None-as-backend-default pattern"
    - "Pydantic ValidationError passes through unchanged — not wrapped in ConfigError"
    - "PRECISION_SUPPORT and DECODING_SUPPORT dicts exist in config/ssot.py as the SSOT for cross-validators and introspection"
  artifacts:
    - path: "src/llenergymeasure/config/models.py"
      provides: "v2.0 ExperimentConfig with field renames, extra=forbid, cross-validators"
      contains: "class ExperimentConfig"
    - path: "src/llenergymeasure/config/backend_configs.py"
      provides: "PyTorchConfig, VLLMConfig, TensorRTConfig with extra=forbid, None-as-default pattern"
      contains: "class PyTorchConfig"
    - path: "src/llenergymeasure/config/ssot.py"
      provides: "PRECISION_SUPPORT and DECODING_SUPPORT dicts as single source of truth"
      contains: "PRECISION_SUPPORT"
  key_links:
    - from: "src/llenergymeasure/config/models.py"
      to: "src/llenergymeasure/config/backend_configs.py"
      via: "TYPE_CHECKING forward references + model_rebuild()"
      pattern: "ExperimentConfig.model_rebuild"
    - from: "src/llenergymeasure/config/models.py"
      to: "src/llenergymeasure/config/ssot.py"
      via: "import PRECISION_SUPPORT for cross-validator"
      pattern: "from.*ssot import"
---

<objective>
Rewrite ExperimentConfig and backend configs to v2.0 schema: field renames, extra="forbid", three structural cross-validators, and the None-as-backend-default pattern. Also create config/ssot.py with PRECISION_SUPPORT and DECODING_SUPPORT dicts as the single source of truth for precision and decoding capabilities per backend.

Purpose: This is the foundational schema that all downstream plans (loader, user config, introspection) depend on. Must be correct before Wave 2 begins.
Output: models.py, backend_configs.py, and ssot.py in v2.0 shape, importable and testable.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/02-config-system/02-CONTEXT.md
@.product/designs/experiment-config.md

<interfaces>
<!-- v1.x ExperimentConfig fields being RENAMED — executor must rename not add -->

FIELD RENAMES (old → new):
  model_name         → model
  fp_precision       → precision
  num_input_prompts  → n
  extra_metadata     → REMOVED (not in v2.0)
  extra: "allow"     → extra: "forbid"

FIELD REMOVALS (v1.x-only, not in v2.0):
  config_name        → REMOVED (model is the identity now)
  schema_version     → REMOVED (from ExperimentConfig; version is in result schema)
  adapter            → keep as optional str (LoRA adapter HF ID or local path)
  TrafficSimulation  → REMOVED (M2 scope)
  ScheduleConfig     → REMOVED (M2 scope)
  IOConfig           → inline output_dir: str | None = None field instead
  query_rate         → REMOVED
  streaming          → REMOVED (Phase 5 measurement concern, not config)
  streaming_warmup_requests → REMOVED
  save_outputs       → REMOVED (Phase 6 results concern)
  decode_token_to_text → REMOVED

FIELDS KEPT (v2.0):
  model: str                      # required, no default (HF model ID)
  backend: Literal["pytorch", "vllm", "tensorrt"] = "pytorch"
  precision: Literal["fp32", "fp16", "bf16"] = "bf16"
  n: int = 100                    # number of prompts
  random_seed: int = 42
  max_input_tokens: int = 512
  max_output_tokens: int = 128
  decoder: DecoderConfig          # sub-config (keep as-is)
  warmup: WarmupConfig            # sub-config (keep as-is, simplified)
  baseline: BaselineConfig        # sub-config (keep as-is, simplified)
  pytorch: PyTorchConfig | None = None
  vllm: VLLMConfig | None = None
  tensorrt: TensorRTConfig | None = None
  lora: LoRAConfig | None = None  # new: optional LoRA sub-config
  passthrough_kwargs: dict[str, Any] | None = None  # renamed from extra_metadata

NEW v2.0-only fields:
  output_dir: str | None = None   # per-experiment output dir override

</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite ExperimentConfig with v2.0 schema</name>
  <files>src/llenergymeasure/config/models.py</files>
  <action>
Rewrite models.py from scratch preserving only the classes that remain in v2.0:

**Keep and simplify:**
- `DecoderConfig` — keep existing fields (temperature, do_sample, top_k, top_p, repetition_penalty, preset). Remove SAMPLING_PRESETS preset validation (keep it, it's useful). Keep `is_deterministic` property.
- `WarmupConfig` — simplify to: `n_warmup: int = 3` (fixed count, full-length prompts per .product/designs/experiment-config.md) and `thermal_floor_seconds: float = 60.0`. Remove CV-based convergence detection fields — that's measurement concern, not config.
- `BaselineConfig` — keep: `enabled: bool = True`, `duration_seconds: float = 30.0`. Remove: `required`, `cache_ttl_sec`, `sample_interval_ms` (measurement concerns).

**Add new:**
- `SyntheticDatasetConfig` (per .product/designs/experiment-config.md): `n: int`, `input_len: int = 512`, `output_len: int = 128`, `seed: int = 42`. All fields with `extra="forbid"`.
- `LoRAConfig`: `adapter_id: str | None = None`, `adapter_path: str | None = None`, `merge_weights: bool = False`. Validator: exactly one of adapter_id or adapter_path must be set. `extra="forbid"`.

**Remove entirely:**
- `TrafficSimulation`, `LatencySimulation`, `ScheduleConfig`, `IOConfig`, `TimeSeriesConfig`
- `FilePromptSource`, `HuggingFacePromptSource`, `PromptSourceConfig`, `DatasetConfig` — these are Phase 5/6 concerns, not Phase 2 config schema
- `BUILTIN_DATASETS`, `AUTO_DETECT_COLUMNS`, `SAMPLING_PRESETS` dict (keep SAMPLING_PRESETS if referenced by DecoderConfig)
- `CURRENT_SCHEMA_VERSION`, `DEFAULT_DATASET` constants

**ExperimentConfig (new schema):**
```python
class ExperimentConfig(BaseModel):
    model_config = {"extra": "forbid"}

    # Required
    model: str = Field(..., min_length=1, description="HuggingFace model ID or local path")
    backend: Literal["pytorch", "vllm", "tensorrt"] = Field(default="pytorch", description="Inference backend")

    # Data
    n: int = Field(default=100, ge=1, description="Number of prompts from dataset")
    dataset: str | SyntheticDatasetConfig = Field(default="aienergyscore", description="Dataset name (built-in alias) or synthetic config")

    # Hardware
    precision: Literal["fp32", "fp16", "bf16"] = Field(default="bf16", description="Floating point precision")
    random_seed: int = Field(default=42, description="Random seed for reproducibility")

    # Token limits
    max_input_tokens: int = Field(default=512, ge=1, description="Max input tokens")
    max_output_tokens: int = Field(default=128, ge=1, description="Max output tokens")

    # Sub-configs
    decoder: DecoderConfig = Field(default_factory=DecoderConfig)
    warmup: WarmupConfig = Field(default_factory=WarmupConfig)
    baseline: BaselineConfig = Field(default_factory=BaselineConfig)

    # Backend sections (None = use backend's own defaults)
    pytorch: PyTorchConfig | None = None
    vllm: VLLMConfig | None = None
    tensorrt: TensorRTConfig | None = None

    # LoRA adapter (optional)
    lora: LoRAConfig | None = None

    # Escape hatch — explicitly declared for extra="forbid" compatibility
    passthrough_kwargs: dict[str, Any] | None = None

    # Output override
    output_dir: str | None = None
```

**Three cross-validators (model_validator(mode="after")):**

1. Backend-section/backend mismatch:
```python
if self.pytorch is not None and self.backend != "pytorch":
    raise ValueError(
        f"pytorch: config section provided but backend={self.backend!r}. "
        "Remove the pytorch: section or set backend: pytorch."
    )
# Same for vllm, tensorrt
```

2. fp16/bf16 + cpu backend (static, no GPU detection in Phase 2):
```python
# Note: cpu backend is not in the Literal above, so this validator is future-proofing.
# Phase 4 pre-flight does GPU detection; this is the static structural rule only.
# For now, this validator is a no-op placeholder — backend is constrained to
# pytorch/vllm/tensorrt (all GPU backends). Leave this as a comment for Phase 4.
```

Actually, from CONTEXT.md: "precision: fp16/bf16 disallowed when backend: cpu (static rule — GPU detection is Phase 4)". Since `cpu` is not a valid backend in the Literal, this validator cannot fire in Phase 2. Add a comment documenting that GPU detection is Phase 4.

3. passthrough_kwargs key collision:
```python
if self.passthrough_kwargs:
    top_level_fields = set(ExperimentConfig.model_fields.keys())
    collisions = set(self.passthrough_kwargs.keys()) & top_level_fields
    if collisions:
        raise ValueError(
            f"passthrough_kwargs keys collide with ExperimentConfig fields: "
            f"{sorted(collisions)}. Use the named fields instead."
        )
```

**NOTE on error collection:** The CONTEXT.md says "Collect ALL validation errors before raising". This applies to the YAML loader (Plan 02), not to Pydantic validators themselves. Individual `@model_validator` methods raise immediately — the loader wraps and collects Pydantic ValidationErrors. Keep validators simple and specific.

**Keep forward-reference pattern** for backend configs:
```python
if TYPE_CHECKING:
    from llenergymeasure.config.backend_configs import PyTorchConfig, VLLMConfig, TensorRTConfig

# At bottom of file:
def _rebuild_experiment_config() -> None:
    from llenergymeasure.config.backend_configs import PyTorchConfig, TensorRTConfig, VLLMConfig
    ExperimentConfig.model_rebuild(_types_namespace={...})

_rebuild_experiment_config()
```

**Remove `_rebuild_experiment_config` complexity** — use `model_rebuild` only if needed for forward refs. If using `from __future__ import annotations` at top, forward refs resolve automatically via `model_rebuild`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.config.models import ExperimentConfig, DecoderConfig, WarmupConfig, BaselineConfig, LoRAConfig; c = ExperimentConfig(model='gpt2'); assert c.precision == 'bf16'; assert c.n == 100; assert c.backend == 'pytorch'; print('OK')"</automated>
  </verify>
  <done>ExperimentConfig(model="gpt2") instantiates with defaults; model, precision, n fields present; extra="forbid" means ExperimentConfig(model="gpt2", unknown_field=1) raises ValidationError</done>
</task>

<task type="auto">
  <name>Task 2: Rewrite backend configs with v2.0 schema</name>
  <files>src/llenergymeasure/config/backend_configs.py</files>
  <action>
Rewrite backend_configs.py to the minimal v2.0 design from .product/designs/experiment-config.md.

**Key principles:**
- All fields `None` by default — `None` means "use backend's own default at execution time"
- `extra="forbid"` on every class
- Only include fields that M1 PyTorch backend will actually use (full completeness audit is Phase 4.1)
- No complex nested sub-configs for now (remove PyTorchBeamSearchConfig, PyTorchAssistedGenerationConfig, etc.)

**PyTorchConfig (v2.0 M1 minimal):**
```python
class PyTorchConfig(BaseModel):
    model_config = {"extra": "forbid"}

    batch_size: int | None = Field(default=None, ge=1, description="Batch size (None → 1)")
    attn_implementation: Literal["sdpa", "flash_attention_2", "eager"] | None = Field(default=None, description="Attention implementation")
    torch_compile: bool | None = Field(default=None, description="Enable torch.compile (None → False)")
    load_in_4bit: bool | None = Field(default=None, description="BitsAndBytes 4-bit quantization")
    load_in_8bit: bool | None = Field(default=None, description="BitsAndBytes 8-bit quantization")
    num_processes: int | None = Field(default=None, ge=1, description="Data parallel processes (None → 1)")

    @model_validator(mode="after")
    def validate_quantization(self) -> "PyTorchConfig":
        if self.load_in_4bit and self.load_in_8bit:
            raise ValueError("Cannot use both load_in_4bit=True and load_in_8bit=True simultaneously")
        return self
```

**VLLMConfig (v2.0 M1 minimal):**
```python
class VLLMConfig(BaseModel):
    model_config = {"extra": "forbid"}

    max_num_seqs: int | None = Field(default=None, ge=1, description="Max concurrent sequences (None → 256)")
    tensor_parallel_size: int | None = Field(default=None, ge=1, description="Tensor parallel degree (None → 1)")
    gpu_memory_utilization: float | None = Field(default=None, ge=0.1, le=1.0, description="GPU memory fraction (None → 0.9)")
    enable_prefix_caching: bool | None = Field(default=None, description="APC prefix caching (None → False)")
    quantization: Literal["awq", "gptq", "fp8"] | None = Field(default=None, description="Quantization method")
```

**TensorRTConfig (v2.0 M1 minimal):**
```python
class TensorRTConfig(BaseModel):
    model_config = {"extra": "forbid"}

    max_batch_size: int | None = Field(default=None, ge=1, description="Max batch size (compile-time constant)")
    tp_size: int | None = Field(default=None, ge=1, description="Tensor parallel size (None → 1)")
    quantization: Literal["int8_sq", "int4_awq", "fp8"] | None = Field(default=None, description="Quantization method")
    engine_path: str | None = Field(default=None, description="Pre-compiled engine path (skip compilation)")
```

Remove ALL other v1.x classes: `PyTorchBeamSearchConfig`, `PyTorchAssistedGenerationConfig`, and any other sub-configs. The Phase 4.1 parameter audit will expand these.

NOTE: The v1.x file has many more fields (batching_strategy, cache_implementation, beam_search sub-configs, etc.). These are being intentionally replaced — do NOT preserve them. The audit phase will restore researcher-relevant parameters after verification.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.config.backend_configs import PyTorchConfig, VLLMConfig, TensorRTConfig; p = PyTorchConfig(); v = VLLMConfig(); t = TensorRTConfig(); assert p.batch_size is None; print('OK')"</automated>
  </verify>
  <done>All three backend configs instantiate with all-None defaults; load_in_4bit + load_in_8bit simultaneously raises ValueError; extra="forbid" means unknown fields raise ValidationError</done>
</task>

<task type="auto">
  <name>Task 3: Create config/ssot.py with PRECISION_SUPPORT and DECODING_SUPPORT dicts</name>
  <files>src/llenergymeasure/config/ssot.py</files>
  <action>
Create config/ssot.py as the single source of truth for which backends support which precision modes and decoding strategies. These dicts are consumed by cross-validators in models.py and by the introspection module.

**Why a separate file:** These dicts would otherwise be duplicated between cross-validators, introspection metadata, and future CLI help text. One definition, multiple consumers.

```python
"""Single source of truth for backend capability constants.

These dicts define which precision modes and decoding strategies each backend
supports. They are consumed by:
- ExperimentConfig cross-validators (structural validation)
- config/introspection.py (backend capability metadata)
- Future CLI help generation (Phase 7)

Do not inline these values in validators — always import from here.
"""

# Precision modes supported by each backend.
# "fp32" = full precision, "fp16" = half, "bf16" = bfloat16.
# Note: fp16/bf16 require GPU. The cpu backend (future) would be fp32-only.
# GPU detection and cpu-precision cross-validation is Phase 4 (pre-flight).
PRECISION_SUPPORT: dict[str, list[str]] = {
    "pytorch": ["fp32", "fp16", "bf16"],
    "vllm": ["fp16", "bf16"],        # vLLM does not support fp32 inference
    "tensorrt": ["fp16", "bf16"],    # TRT-LLM does not support fp32 inference
}

# Decoding strategies supported by each backend.
# Keys match DecoderConfig fields that affect backend behaviour.
# Values indicate whether the backend respects the parameter.
# "sampling" = do_sample=True path; "greedy" = do_sample=False path.
DECODING_SUPPORT: dict[str, list[str]] = {
    "pytorch": ["greedy", "sampling"],           # full HuggingFace generate() support
    "vllm": ["greedy", "sampling"],              # vLLM supports both via SamplingParams
    "tensorrt": ["greedy", "sampling"],          # TRT-LLM supports both
}

# Backends that support the full DecoderConfig temperature/top_k/top_p fields.
# All current backends support these — this dict exists to make future
# backend additions explicit rather than implicit.
DECODER_PARAM_SUPPORT: dict[str, list[str]] = {
    "pytorch": ["temperature", "top_k", "top_p", "repetition_penalty"],
    "vllm": ["temperature", "top_k", "top_p", "repetition_penalty"],
    "tensorrt": ["temperature", "top_k", "top_p"],  # TRT-LLM: repetition_penalty support varies
}
```

**No cross-validator logic in this file** — it is data-only. Import from here in models.py cross-validators. Example usage in a future cross-validator (not required in Phase 2 since precision Literal already constrains values):

```python
# In models.py, a future validator could use:
# from llenergymeasure.config.ssot import PRECISION_SUPPORT
# if self.precision not in PRECISION_SUPPORT.get(self.backend, []):
#     raise ValueError(f"precision={self.precision!r} not supported by backend={self.backend!r}")
```

For Phase 2, the precision Literal ["fp32", "fp16", "bf16"] already limits values to valid options. The PRECISION_SUPPORT dict is primarily consumed by introspection for generating backend capability tables.

**__all__:**
```python
__all__ = ["PRECISION_SUPPORT", "DECODING_SUPPORT", "DECODER_PARAM_SUPPORT"]
```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.config.ssot import PRECISION_SUPPORT, DECODING_SUPPORT, DECODER_PARAM_SUPPORT; assert 'pytorch' in PRECISION_SUPPORT; assert 'vllm' in PRECISION_SUPPORT; assert 'tensorrt' in PRECISION_SUPPORT; assert 'bf16' in PRECISION_SUPPORT['pytorch']; assert 'pytorch' in DECODING_SUPPORT; print('OK')"</automated>
  </verify>
  <done>PRECISION_SUPPORT and DECODING_SUPPORT importable from config.ssot; all three backends present in both dicts; PRECISION_SUPPORT["pytorch"] contains "bf16"</done>
</task>

</tasks>

<verification>
After all three tasks:
```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool
# Schema validates correctly
python -c "
from llenergymeasure.config.models import ExperimentConfig
from llenergymeasure.config.backend_configs import PyTorchConfig
from llenergymeasure.config.ssot import PRECISION_SUPPORT, DECODING_SUPPORT
from pydantic import ValidationError

# Basic construction
c = ExperimentConfig(model='gpt2')
assert c.model == 'gpt2'
assert c.precision == 'bf16'
assert c.n == 100

# extra=forbid rejects unknown keys
try:
    ExperimentConfig(model='gpt2', unknown_field=1)
    assert False, 'Should have raised'
except ValidationError:
    pass

# Backend section mismatch
try:
    ExperimentConfig(model='gpt2', backend='vllm', pytorch=PyTorchConfig())
    assert False, 'Should have raised'
except ValidationError:
    pass

# passthrough_kwargs collision
try:
    ExperimentConfig(model='gpt2', passthrough_kwargs={'model': 'other'})
    assert False, 'Should have raised'
except ValidationError:
    pass

# SSOT is consistent with ExperimentConfig precision Literal
precision_literal_values = {'fp32', 'fp16', 'bf16'}
for backend, precisions in PRECISION_SUPPORT.items():
    assert set(precisions) <= precision_literal_values, f'{backend} has unknown precision: {precisions}'

print('All checks passed')
"
```
</verification>

<success_criteria>
- `ExperimentConfig(model="gpt2")` succeeds with all defaults
- `ExperimentConfig(model="gpt2", unknown_field=1)` raises `ValidationError` (extra="forbid")
- Backend section/backend field mismatch raises `ValidationError`
- `passthrough_kwargs` key collision with top-level field raises `ValidationError`
- All three backend configs import cleanly from `backend_configs.py`
- `PyTorchConfig(load_in_4bit=True, load_in_8bit=True)` raises `ValueError`
- `from llenergymeasure.config.ssot import PRECISION_SUPPORT, DECODING_SUPPORT` succeeds
- `PRECISION_SUPPORT` covers all three backends with valid precision values
</success_criteria>

<output>
After completion, create `.planning/phases/02-config-system/02-01-SUMMARY.md`
</output>
