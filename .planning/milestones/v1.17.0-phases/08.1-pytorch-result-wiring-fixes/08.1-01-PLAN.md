---
phase: 08.1-pytorch-result-wiring-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/domain/experiment.py
  - src/llenergymeasure/core/backends/pytorch.py
  - src/llenergymeasure/cli/run.py
  - tests/unit/test_measurement_integration.py
  - tests/unit/test_experiment_result_v2.py
  - tests/conftest.py
autonomous: true
requirements:
  - RES-06
  - RES-16
  - CM-16

must_haves:
  truths:
    - "ExperimentResult.timeseries is a str (not None) after PyTorchBackend.run() when timeseries data exists"
    - "ExperimentResult.effective_config contains the experiment's resolved config dict (model key is not 'unknown')"
    - "ExperimentResult.baseline_power_w and energy_adjusted_j are populated from EnergyBreakdown data"
    - "ExperimentResult rejects unrecognised kwargs with ValidationError (extra='forbid')"
    - "Timeseries parquet sidecar is co-located with result.json in the same output subdirectory"
  artifacts:
    - path: "src/llenergymeasure/core/backends/pytorch.py"
      provides: "Fixed _build_result() with correct field wiring"
      contains: "timeseries=timeseries_path"
    - path: "src/llenergymeasure/domain/experiment.py"
      provides: "ExperimentResult with extra=forbid"
      contains: "extra.*forbid"
    - path: "src/llenergymeasure/cli/run.py"
      provides: "Timeseries co-location via timeseries_source"
      contains: "timeseries_source"
    - path: "tests/unit/test_measurement_integration.py"
      provides: "Tests for _build_result field wiring"
    - path: "tests/unit/test_experiment_result_v2.py"
      provides: "Test for extra=forbid rejection"
  key_links:
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/domain/experiment.py"
      via: "ExperimentResult constructor kwargs matching field names"
      pattern: "timeseries=|effective_config=|baseline_power_w=|energy_adjusted_j="
    - from: "src/llenergymeasure/cli/run.py"
      to: "src/llenergymeasure/domain/experiment.py"
      via: "result.save(output_dir, timeseries_source=...)"
      pattern: "timeseries_source"
---

<objective>
Fix all 4 broken E2E flows caused by `PyTorchBackend._build_result()` field wiring bugs, add `extra="forbid"` to prevent recurrence, and wire timeseries co-location in CLI.

Purpose: M1 audit found 3 fields silently dropped or unpopulated in `_build_result()`, producing broken timeseries round-trip, wrong output directory names, missing baseline display, and timeseries/result.json separation. These are all 1-2 line fixes with targeted tests.

Output: Working `_build_result()` wiring, defensive `ExperimentResult` model, co-located timeseries, and tests proving each fix.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08.1-pytorch-result-wiring-fixes/08.1-RESEARCH.md

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/llenergymeasure/domain/experiment.py (ExperimentResult fields — lines 139-259):
```python
class ExperimentResult(BaseModel):
    # ...
    timeseries: str | None = Field(default=None, ...)       # ← correct field name
    baseline_power_w: float | None = Field(default=None, ...)
    energy_adjusted_j: float | None = Field(default=None, ...)
    effective_config: dict[str, Any] = Field(default_factory=dict, ...)
    energy_breakdown: EnergyBreakdown | None = Field(default=None, ...)
    # ...
    model_config = {"frozen": True}  # ← needs extra="forbid" added
```

From src/llenergymeasure/core/backends/pytorch.py (_build_result — lines 696-784):
```python
def _build_result(self, config, data, snapshot, start_time, end_time,
                  thermal_info, energy_measurement, baseline, flops_result,
                  warmup_result, timeseries_path, measurement_warnings) -> ExperimentResult:
    # ...
    energy_breakdown = create_energy_breakdown(total_energy_j, baseline, duration_sec)
    return ExperimentResult(
        # BUG 1: timeseries_path=timeseries_path  ← wrong kwarg name (field is `timeseries`)
        # BUG 2: effective_config not passed       ← defaults to {}
        # BUG 3: baseline_power_w not passed       ← defaults to None
        # BUG 3: energy_adjusted_j not passed      ← defaults to None
        energy_breakdown=energy_breakdown,
        timeseries_path=timeseries_path,  # SILENT DROP — Pydantic ignores unknown kwargs
        ...
    )
```

From src/llenergymeasure/cli/run.py (save call — line 194):
```python
if experiment_config.output_dir:
    result.save(Path(experiment_config.output_dir))
    # ← timeseries_source not passed → timeseries stays in flat output_dir/
```

From src/llenergymeasure/results/persistence.py (save_result — line 70):
```python
def save_result(result, output_dir, timeseries_source=None) -> Path:
    # timeseries_source already supported — copies parquet into subdir if provided
```

From src/llenergymeasure/domain/metrics.py (EnergyBreakdown fields):
```python
class EnergyBreakdown(BaseModel):
    baseline_power_w: float | None = None
    adjusted_j: float | None = None  # ← this maps to ExperimentResult.energy_adjusted_j
```

From tests/conftest.py (make_result factory — line 29):
```python
def make_result(**overrides) -> ExperimentResult:
    defaults = {
        "experiment_id": "test-001",
        "measurement_config_hash": "abc123def4567890",
        "measurement_methodology": "total",
        "aggregation": AggregationMetadata(num_processes=1),
        "total_tokens": 1000,
        "total_energy_j": 10.0,
        "total_inference_time_sec": 5.0,
        "avg_tokens_per_second": 200.0,
        "avg_energy_per_token_j": 0.01,
        "total_flops": 1e9,
        "start_time": _EPOCH,
        "end_time": _EPOCH_END,
    }
    defaults.update(overrides)
    return ExperimentResult(**defaults)
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix _build_result() wiring and add extra="forbid" to ExperimentResult</name>
  <files>
    src/llenergymeasure/core/backends/pytorch.py
    src/llenergymeasure/domain/experiment.py
    tests/conftest.py
    tests/unit/test_experiment_result_v2.py
  </files>
  <action>
    **Fix 1 — Field name mismatch (CM-16):** In `pytorch.py` `_build_result()` (line 782), change `timeseries_path=timeseries_path` to `timeseries=timeseries_path`. The ExperimentResult field is named `timeseries`, not `timeseries_path`. The old kwarg was silently ignored by Pydantic.

    **Fix 2 — effective_config (RES-16):** In `_build_result()`, add `effective_config=config.model_dump()` to the `ExperimentResult(...)` constructor call. This populates the config dict so `_experiment_dir_name()` in persistence.py reads the correct model name instead of "unknown".

    **Fix 3 — baseline fields (RES-06):** In `_build_result()`, after `energy_breakdown = create_energy_breakdown(...)` (line 756), add two lines to extract from the breakdown, then pass to the constructor:
    ```python
    baseline_power_w=energy_breakdown.baseline_power_w if energy_breakdown else None,
    energy_adjusted_j=energy_breakdown.adjusted_j if energy_breakdown else None,
    ```
    Note: `EnergyBreakdown.adjusted_j` maps to `ExperimentResult.energy_adjusted_j` — the field names differ.

    **Fix 4 — extra="forbid" (structural):** In `domain/experiment.py` (line 259), change:
    ```python
    model_config = {"frozen": True}
    ```
    to:
    ```python
    model_config = {"frozen": True, "extra": "forbid"}
    ```
    This prevents future silent field name mismatches. IMPORTANT: both `frozen` and `extra` must be in the same dict.

    **Fix test breakage:** After adding `extra="forbid"`, run the full test suite. Any test that constructs `ExperimentResult(...)` with an unrecognised kwarg will fail with `ValidationError: Extra inputs are not permitted`. Fix these — they are evidence of the same bug class. Check:
    - `tests/conftest.py` `make_result()` factory — ensure it only passes recognised field names
    - `tests/unit/test_experiment_result_v2.py` — scan for any `timeseries_path=` usage
    - Any other test file that constructs `ExperimentResult` directly

    **Add test — extra="forbid" enforcement:** In `tests/unit/test_experiment_result_v2.py`, add:
    ```python
    def test_experiment_result_rejects_unknown_kwargs(make_result):
        """ExperimentResult raises ValidationError for unrecognised kwargs (extra='forbid')."""
        with pytest.raises(ValidationError, match="Extra inputs are not permitted"):
            make_result(timeseries_path="ts.parquet")
    ```
  </action>
  <verify>
    <automated>pytest tests/unit/test_experiment_result_v2.py -x -q</automated>
  </verify>
  <done>
    - `ExperimentResult` has `model_config = {"frozen": True, "extra": "forbid"}`
    - `_build_result()` passes `timeseries=`, `effective_config=`, `baseline_power_w=`, `energy_adjusted_j=`
    - `ExperimentResult(timeseries_path="x")` raises `ValidationError`
    - All existing tests pass
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire timeseries co-location in CLI and add wiring tests</name>
  <files>
    src/llenergymeasure/cli/run.py
    tests/unit/test_measurement_integration.py
  </files>
  <action>
    **Fix 5 — Timeseries co-location (RES-16):** In `cli/run.py` around line 192-195, change:
    ```python
    if experiment_config.output_dir:
        result.save(Path(experiment_config.output_dir))
    ```
    to:
    ```python
    if experiment_config.output_dir:
        output_dir = Path(experiment_config.output_dir)
        ts_source = output_dir / result.timeseries if result.timeseries else None
        result.save(output_dir, timeseries_source=ts_source)
        # Clean up stale flat timeseries file after copy into subdirectory
        if ts_source is not None:
            ts_source.unlink(missing_ok=True)
    ```
    This passes the timeseries file written by the backend at `output_dir/timeseries.parquet` to `save_result()`, which copies it into the `{model}_{backend}_{ts}/` subdirectory alongside `result.json`. The stale flat copy is then removed.

    **Add wiring tests in test_measurement_integration.py:** Add the following tests at the end of the file:

    1. `test_build_result_populates_timeseries_field()` — Construct a mock `PyTorchBackend`, call `_build_result()` with `timeseries_path="timeseries.parquet"`, assert `result.timeseries == "timeseries.parquet"` (not None).

    2. `test_build_result_populates_effective_config()` — Call `_build_result()` with a real `ExperimentConfig(model="gpt2")`, assert `result.effective_config.get("model") == "gpt2"` and `result.effective_config != {}`.

    3. `test_build_result_propagates_baseline_fields()` — Call `_build_result()` with a real `BaselineCache(power_w=30.0, ...)` baseline, assert `result.baseline_power_w == pytest.approx(30.0)` and `result.energy_adjusted_j is not None`.

    For all three tests, mock the expensive parts (model loading, inference) and construct the `_MeasurementData` namedtuple and other parameters directly. Use the existing test patterns from the file (MagicMock, patch). The `_build_result()` method is a pure assembler — it needs: `config`, `data` (a `_MeasurementData` with `total_tokens`, `total_time_sec`, `output_tokens`), `snapshot` (can be `None`), `start_time`/`end_time` (datetimes), `thermal_info` (a `ThermalThrottleInfo`), `energy_measurement` (mock with `.total_j`), `baseline` (a `BaselineCache` or `None`), `flops_result` (mock with `.value` or `None`), `warmup_result` (a `WarmupResult`), `timeseries_path` (str or None), `measurement_warnings` (list).

    Import `_MeasurementData` from `llenergymeasure.core.backends.pytorch` (it is a `NamedTuple`). Import `ThermalThrottleInfo` from `llenergymeasure.core.power_thermal`. Import `BaselineCache` from `llenergymeasure.core.baseline`. Import `WarmupResult` from `llenergymeasure.core.warmup`. Import `ExperimentConfig` from `llenergymeasure.config.models`.
  </action>
  <verify>
    <automated>pytest tests/unit/test_measurement_integration.py -x -q && pytest tests/unit/ -x -q</automated>
  </verify>
  <done>
    - CLI passes `timeseries_source` to `result.save()` and cleans up the stale flat file
    - `test_build_result_populates_timeseries_field` passes — proves CM-16 fix
    - `test_build_result_populates_effective_config` passes — proves RES-16 fix (dir naming)
    - `test_build_result_propagates_baseline_fields` passes — proves RES-06 fix
    - Full unit test suite passes with no regressions
  </done>
</task>

</tasks>

<verification>
Run full unit test suite to confirm no regressions:

```bash
pytest tests/unit/ -x -q
```

Spot-check the 5 success criteria from ROADMAP.md:
1. Grep `pytorch.py` for `timeseries=timeseries_path` (not `timeseries_path=`)
2. Grep `pytorch.py` for `effective_config=config.model_dump()`
3. Grep `pytorch.py` for `baseline_power_w=` and `energy_adjusted_j=`
4. Grep `experiment.py` for `extra.*forbid`
5. Grep `cli/run.py` for `timeseries_source`
</verification>

<success_criteria>
- All 5 phase success criteria from ROADMAP.md are met
- All unit tests pass (0 failures)
- No extra kwargs silently dropped by ExperimentResult
- Output directory naming uses real model name from effective_config
- Timeseries parquet lives alongside result.json in subdirectory
</success_criteria>

<output>
After completion, create `.planning/phases/08.1-pytorch-result-wiring-fixes/08.1-01-SUMMARY.md`
</output>
