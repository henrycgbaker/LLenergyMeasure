# Phase 4.1: PyTorch Parameter Audit - Research

**Researched:** 2026-02-26
**Domain:** HuggingFace Transformers — `AutoModelForCausalLM.from_pretrained()` + `model.generate()` parameter completeness
**Confidence:** HIGH (verified against official HuggingFace docs and source code)

---

## Summary

The current `PyTorchConfig` (v2.0 M1 minimal) exposes 6 fields: `batch_size`, `attn_implementation`, `torch_compile`, `load_in_4bit`, `load_in_8bit`, and `num_processes`. The `DecoderConfig` exposes 5 fields: `temperature`, `do_sample`, `top_k`, `top_p`, `repetition_penalty`. Both are explicitly described as "M1 minimal" with a note that Phase 4.1 will expand them.

The v1.x codebase (still present in `src/llenergymeasure/core/inference_backends/pytorch.py`) implemented a substantially richer parameter set across both `from_pretrained()` and `model.generate()`. The audit task is to decide which of these v1.x parameters — and which new Transformers 5.x parameters — should be promoted to first-class `PyTorchConfig`/`DecoderConfig` fields, versus intentionally left to `passthrough_kwargs`.

The guiding principle established in Phase 02 is **None-as-default**: fields represent researcher intent, not backend defaults. The v2.0 design doc (`experiment-config.md`) already provides the target `PyTorchConfig` schema — the audit validates that implementation matches the design and identifies any gaps.

**Primary recommendation:** Promote ~15 additional parameters across `PyTorchConfig` and `DecoderConfig`. The v2.0 design doc has a richer target schema than what was implemented in M1. Phase 4.1 closes that gap, plus adds `BitsAndBytesConfig`-style fields and wires them through the backend.

---

## Current State: What Is Implemented

### `PyTorchConfig` (v2.0 M1 as-built)

Source: `/src/llenergymeasure/config/backend_configs.py`

| Field | Type | Default | Wired in Backend? |
|-------|------|---------|-------------------|
| `batch_size` | `int \| None` | None | YES — `_run_measurement()` |
| `attn_implementation` | `Literal["sdpa","flash_attention_2","eager"] \| None` | None | YES — `_model_load_kwargs()` |
| `torch_compile` | `bool \| None` | None | NOT YET — accepted but not applied |
| `load_in_4bit` | `bool \| None` | None | YES — `_model_load_kwargs()` |
| `load_in_8bit` | `bool \| None` | None | YES — `_model_load_kwargs()` |
| `num_processes` | `int \| None` | None | NOT YET — M1 single-process only |

Note: `torch_compile` is accepted in `PyTorchConfig` but `_model_load_kwargs()` in `core/backends/pytorch.py` does not call `torch.compile()`. The v1.x backend did apply it via `_apply_torch_compile()`. This is a gap to address.

### `DecoderConfig` (v2.0 M1 as-built)

Source: `/src/llenergymeasure/config/models.py`

| Field | Type | Default | Wired in Backend? |
|-------|------|---------|-------------------|
| `temperature` | `float` | 1.0 | YES |
| `do_sample` | `bool` | True | YES |
| `top_k` | `int` | 50 | YES |
| `top_p` | `float` | 1.0 | YES |
| `repetition_penalty` | `float` | 1.0 | YES |
| `preset` | `Literal[...]` | None | YES (expands at validation time) |

---

## What the v2.0 Design Doc Targets

Source: `/.product/designs/experiment-config.md` — this is the SSOT for what PyTorchConfig SHOULD be.

The design doc specifies a minimal target schema:

```python
class PyTorchConfig(BaseModel):
    batch_size: int | None = None
    batching_strategy: Literal["static", "dynamic"] | None = None
    attn_implementation: Literal["sdpa", "flash_attention_2", "eager"] | None = None
    torch_compile: bool | None = None
    load_in_4bit: bool | None = None
    load_in_8bit: bool | None = None
    num_processes: int | None = None
```

This is identical to the M1 as-built plus `batching_strategy`. The design doc also has a TODO comment:
> "Complete PyTorch parameter list — see PARAM-04 backend parameter completeness audit"

This confirms the design doc intentionally left the full list for this phase.

---

## Complete HuggingFace Parameter Inventory

### A. `AutoModelForCausalLM.from_pretrained()` — Inference-Affecting Parameters

Verified against HuggingFace Transformers docs (v5.2.0) and official source.

#### A1. Precision / Dtype
| Parameter | Type | Notes | Recommendation |
|-----------|------|-------|----------------|
| `torch_dtype` | `torch.dtype` | Already handled via `ExperimentConfig.precision` field | COVERED (via precision→dtype mapping) |

#### A2. Attention Implementation
| Parameter | Type | Notes | Recommendation |
|-----------|------|-------|----------------|
| `attn_implementation` | `Literal["eager","sdpa","flash_attention_2","flash_attention_3"]` | flash_attention_3 is new in Transformers 5.x | ADD `"flash_attention_3"` to Literal |

#### A3. Device Management
| Parameter | Type | Notes | Recommendation |
|-----------|------|-------|----------------|
| `device_map` | `str \| dict` | Currently hardcoded to `"auto"` in backend | ADD as optional field (default None→"auto") |
| `max_memory` | `dict` | Per-device memory limits (e.g., `{0: "16GiB", "cpu": "30GiB"}`) | ADD — multi-GPU memory control |
| `low_cpu_mem_usage` | `bool` | Auto-set when device_map is used; rarely needed explicitly | SKIP — redundant when device_map="auto" |
| `offload_folder` | `str` | For disk offloading — advanced use only | SKIP — passthrough_kwargs |

#### A4. BitsAndBytes Quantization (from_pretrained kwargs)

The modern recommended approach is to pass a `BitsAndBytesConfig` object via `quantization_config`. The legacy approach of passing `load_in_4bit`/`load_in_8bit` directly is still supported but `BitsAndBytesConfig` exposes more control.

| Parameter | Config Field | Notes | Recommendation |
|-----------|-------------|-------|----------------|
| `load_in_8bit` | `BitsAndBytesConfig(load_in_8bit=True)` | LLM.int8() — 8-bit quantization | KEEP existing field |
| `load_in_4bit` | `BitsAndBytesConfig(load_in_4bit=True)` | QLoRA — 4-bit quantization | KEEP existing field |
| `bnb_4bit_compute_dtype` | `BitsAndBytesConfig(bnb_4bit_compute_dtype=torch.bfloat16)` | Compute dtype for 4-bit (default float32, usually want bfloat16) | ADD — affects performance significantly |
| `bnb_4bit_quant_type` | `BitsAndBytesConfig(bnb_4bit_quant_type="nf4")` | `"nf4"` or `"fp4"` — NF4 better for inference | ADD |
| `bnb_4bit_use_double_quant` | `BitsAndBytesConfig(bnb_4bit_use_double_quant=True)` | Saves ~0.4 bits/param extra | ADD |
| `llm_int8_threshold` | `BitsAndBytesConfig(llm_int8_threshold=6.0)` | Outlier threshold for int8 | SKIP (advanced, passthrough_kwargs) |
| `llm_int8_skip_modules` | `BitsAndBytesConfig(llm_int8_skip_modules=["lm_head"])` | Skip specific modules | SKIP (advanced, passthrough_kwargs) |
| `llm_int8_enable_fp32_cpu_offload` | `BitsAndBytesConfig(...)` | CPU offload for int8 | SKIP (advanced, passthrough_kwargs) |

**Implementation note:** The backend must build a `BitsAndBytesConfig` object and pass it via `quantization_config=` to `from_pretrained()`. The current backend passes `load_in_4bit` / `load_in_8bit` as raw kwargs, which is the legacy API.

#### A5. Model Versioning / Identity
| Parameter | Type | Notes | Recommendation |
|-----------|------|-------|----------------|
| `revision` | `str` | Model version / commit hash — critical for reproducibility | ADD — affects measurement identity |
| `trust_remote_code` | `bool` | Currently hardcoded to `True` in backend | ADD as optional field (default None→True) |

#### A6. Memory Management
| Parameter | Type | Notes | Recommendation |
|-----------|------|-------|----------------|
| `max_memory` | `dict[int\|str, str]` | Per-device memory limit, e.g. `{0: "10GiB", "cpu": "50GiB"}` | ADD |

---

### B. `model.generate()` — Complete GenerationConfig Parameter Inventory

Verified against HuggingFace Transformers v5.2.0 source code (`configuration_utils.py`).

#### B1. Currently in `DecoderConfig` (COVERED)
| Parameter | Status |
|-----------|--------|
| `temperature` | COVERED |
| `do_sample` | COVERED |
| `top_k` | COVERED |
| `top_p` | COVERED |
| `repetition_penalty` | COVERED |

#### B2. Sampling Parameters — MISSING from DecoderConfig
| Parameter | HF Default | Notes | Recommendation |
|-----------|-----------|-------|----------------|
| `min_p` | None | Min probability filter — peer tools (vLLM, Ollama) all support this | ADD to DecoderConfig |
| `typical_p` | None | Locally typical sampling — niche | SKIP (passthrough_kwargs) |
| `epsilon_cutoff` | None | Token probability floor — niche | SKIP |
| `eta_cutoff` | None | Adaptive cutoff — niche | SKIP |
| `top_h` | None | New in Transformers 5.x — niche | SKIP |

#### B3. Beam Search Parameters
The v1.x backend had `BeamSearchConfig` with `num_beams`, `early_stopping`, `length_penalty`, `no_repeat_ngram_size`. These were in PyTorchConfig as a nested object. The design doc dropped the separate `BeamSearchConfig` class. Decision needed: flat fields in `PyTorchConfig` or nested?

| Parameter | Notes | Recommendation |
|-----------|-------|----------------|
| `num_beams` | Beam search width (1=greedy/sampling) | ADD to `PyTorchConfig` as flat field |
| `early_stopping` | Stop when all beams hit EOS | ADD to `PyTorchConfig` |
| `length_penalty` | Exponential penalty for sequence length (>1=shorter, <1=longer) | ADD to `PyTorchConfig` |
| `num_beam_groups` | For diverse beam search | SKIP (advanced, passthrough_kwargs) |
| `diversity_penalty` | Used with `num_beam_groups` | SKIP |

#### B4. Repetition/N-gram Control
| Parameter | HF Default | Notes | Recommendation |
|-----------|-----------|-------|----------------|
| `no_repeat_ngram_size` | 0 | Prevents n-gram repetition — used in summarisation | ADD to `PyTorchConfig` |
| `encoder_repetition_penalty` | 1.0 | Encoder-decoder only — irrelevant for CausalLM | SKIP |

#### B5. Length Control Parameters
| Parameter | HF Default | Notes | Recommendation |
|-----------|-----------|-------|----------------|
| `max_new_tokens` | None | Already covered by `ExperimentConfig.max_output_tokens` | COVERED |
| `min_new_tokens` | None | Minimum output length | ADD to `DecoderConfig` |
| `max_length` | 20 | Absolute max (input+output) — confusing with max_new_tokens | SKIP (covered by max_output_tokens) |
| `max_time` | None | Wallclock timeout — experimental/niche | SKIP |

#### B6. KV Cache Control
| Parameter | HF Default | Notes | Recommendation |
|-----------|-----------|-------|----------------|
| `use_cache` | True | Disabling helps profile memory bandwidth vs compute trade-off | ADD to `PyTorchConfig` |
| `cache_implementation` | None | `"static"` enables CUDA graph capture (big speedup) | ADD to `PyTorchConfig` |

**`cache_implementation` values:** `"static"`, `"offloaded_static"`, `"sliding_window"`, `"hybrid"`, `"mamba"`. The most important for benchmarking is `"static"` (enables CUDA graphs → ~30% throughput gain for fixed-length batches).

#### B7. Speculative Decoding
The v1.x backend supported `assisted_generation` with a `model` (draft model path) and other kwargs. This maps to `generate(assistant_model=...)`.

| Parameter | Notes | Recommendation |
|-----------|-------|----------------|
| `assistant_model` (passed to `generate()`) | Draft model for speculative decoding | Handled via `passthrough_kwargs` — too complex for a simple field |
| `prompt_lookup_num_tokens` | Prompt-lookup speculative — fast, no draft model needed | ADD to `PyTorchConfig` (simple integer) |

#### B8. `torch.compile` Application
The current `PyTorchConfig.torch_compile` field is accepted but NOT applied by `core/backends/pytorch.py::_model_load_kwargs()`. The v1.x backend had `_apply_torch_compile()` that called `torch.compile(model, mode=..., backend=...)`.

Additional related parameters:
| Parameter | Notes | Recommendation |
|-----------|-------|----------------|
| `torch_compile_mode` | `"default"`, `"reduce-overhead"`, `"max-autotune"` | ADD to `PyTorchConfig` |
| `torch_compile_backend` | Compilation backend, default `"inductor"` | ADD to `PyTorchConfig` |

---

## v1.x Fields: Disposition Matrix

The v1.x `PyTorchConfig` (in `core/inference_backends/pytorch.py` — stale, not the v2.0 backend) had many additional fields. This matrix documents intentional decisions on each.

| v1.x Field | v2.0 Status | Rationale |
|------------|-------------|-----------|
| `batch_size` | KEPT | Core parameter |
| `batching_strategy` | DROPPED | v2.0 uses static batching only; dynamic batching is a streaming/serving concern |
| `max_tokens_per_batch` | DROPPED | Goes with `batching_strategy` |
| `num_processes` | KEPT | Data parallelism via Accelerate |
| `attn_implementation` | KEPT | Core parameter |
| `torch_compile` | KEPT (not wired) | Phase 4.1 wires it |
| `torch_compile_backend` | TO ADD | Pairs with torch_compile |
| `load_in_4bit` | KEPT | Core quantization |
| `load_in_8bit` | KEPT | Core quantization |
| `bnb_4bit_compute_dtype` | TO ADD | Significant for performance |
| `bnb_4bit_quant_type` | TO ADD | NF4 vs FP4 matters for quality |
| `bnb_4bit_use_double_quant` | TO ADD | Memory efficiency |
| `min_p` | TO ADD (DecoderConfig) | Peer tools support it; complements top_p/top_k |
| `no_repeat_ngram_size` | TO ADD (PyTorchConfig) | Used in research contexts |
| `BeamSearchConfig.num_beams` | TO ADD as flat field | Flatten, no nested class |
| `BeamSearchConfig.early_stopping` | TO ADD as flat field | |
| `BeamSearchConfig.length_penalty` | TO ADD as flat field | |
| `use_cache` | TO ADD | Benchmarking use case: compare cached vs uncached |
| `cache_implementation` | TO ADD | CUDA graph capture via "static" — significant |
| `low_cpu_mem_usage` | DROPPED | Auto-set by device_map="auto" |
| `max_memory` | TO ADD | Multi-GPU memory control |
| `use_bettertransformer` | DROPPED | BetterTransformer is deprecated in Transformers 4.36+ (replaced by SDPA) |
| `output_scores` | DROPPED | Not relevant to efficiency measurement |
| `return_dict_in_generate` | DROPPED | Internal backend concern, not user-facing |
| `assisted_generation` (nested config) | DROPPED | Flattened to `prompt_lookup_num_tokens` |
| `extra` (dict) | RENAMED to `passthrough_kwargs` on ExperimentConfig | |

---

## Recommended Target Schema

### `PyTorchConfig` (v2.0 complete)

```python
class PyTorchConfig(BaseModel):
    model_config = {"extra": "forbid"}

    # Batching
    batch_size: int | None = Field(default=None, ge=1)

    # Attention
    attn_implementation: Literal["sdpa", "flash_attention_2", "flash_attention_3", "eager"] | None = None

    # Compilation
    torch_compile: bool | None = None
    torch_compile_mode: Literal["default", "reduce-overhead", "max-autotune"] | None = None
    torch_compile_backend: str | None = None  # "inductor" default

    # BitsAndBytes quantization
    load_in_4bit: bool | None = None
    load_in_8bit: bool | None = None
    bnb_4bit_compute_dtype: Literal["float16", "bfloat16", "float32"] | None = None
    bnb_4bit_quant_type: Literal["nf4", "fp4"] | None = None
    bnb_4bit_use_double_quant: bool | None = None

    # KV caching
    use_cache: bool | None = None
    cache_implementation: Literal["static", "offloaded_static", "sliding_window"] | None = None

    # Beam search
    num_beams: int | None = Field(default=None, ge=1)
    early_stopping: bool | None = None
    length_penalty: float | None = None

    # N-gram repetition
    no_repeat_ngram_size: int | None = Field(default=None, ge=0)

    # Speculative decoding (prompt lookup — no draft model)
    prompt_lookup_num_tokens: int | None = Field(default=None, ge=1)

    # Model loading
    device_map: str | None = None  # None → "auto"
    max_memory: dict | None = None  # e.g. {0: "10GiB", "cpu": "50GiB"}
    revision: str | None = None  # model version for reproducibility
    trust_remote_code: bool | None = None  # None → True

    # Data parallelism
    num_processes: int | None = Field(default=None, ge=1)
```

### `DecoderConfig` additions

```python
# Additional fields to add to existing DecoderConfig:
min_p: float | None = Field(default=None, ge=0.0, le=1.0)   # min probability filter
min_new_tokens: int | None = Field(default=None, ge=1)        # minimum output length
```

---

## Architecture Patterns

### Pattern 1: BitsAndBytesConfig Construction
The backend must build a `BitsAndBytesConfig` object when quantization fields are set.

```python
# In _model_load_kwargs()
from transformers import BitsAndBytesConfig

if config.pytorch and (config.pytorch.load_in_4bit or config.pytorch.load_in_8bit):
    bnb_kwargs = {}
    if config.pytorch.load_in_4bit:
        bnb_kwargs["load_in_4bit"] = True
        if config.pytorch.bnb_4bit_compute_dtype:
            import torch
            dtype_map = {"float16": torch.float16, "bfloat16": torch.bfloat16, "float32": torch.float32}
            bnb_kwargs["bnb_4bit_compute_dtype"] = dtype_map[config.pytorch.bnb_4bit_compute_dtype]
        if config.pytorch.bnb_4bit_quant_type:
            bnb_kwargs["bnb_4bit_quant_type"] = config.pytorch.bnb_4bit_quant_type
        if config.pytorch.bnb_4bit_use_double_quant:
            bnb_kwargs["bnb_4bit_use_double_quant"] = True
    if config.pytorch.load_in_8bit:
        bnb_kwargs["load_in_8bit"] = True
    kwargs["quantization_config"] = BitsAndBytesConfig(**bnb_kwargs)
```

**Important:** Do NOT pass `load_in_4bit`/`load_in_8bit` directly to `from_pretrained()` — use `quantization_config=BitsAndBytesConfig(...)`. The raw kwargs are legacy API and may be removed.

### Pattern 2: torch.compile Application
Must be applied AFTER `from_pretrained()`, not during loading.

```python
# In PyTorchBackend after model load
if config.pytorch and config.pytorch.torch_compile:
    mode = config.pytorch.torch_compile_mode or "default"
    backend = config.pytorch.torch_compile_backend or "inductor"
    try:
        model = torch.compile(model, mode=mode, backend=backend)
    except Exception as e:
        logger.warning("torch.compile failed (non-fatal): %s", e)
```

### Pattern 3: Beam Search in generate() kwargs
```python
# In _build_generate_kwargs()
if config.pytorch:
    if config.pytorch.num_beams and config.pytorch.num_beams > 1:
        kwargs["num_beams"] = config.pytorch.num_beams
        if config.pytorch.early_stopping is not None:
            kwargs["early_stopping"] = config.pytorch.early_stopping
        if config.pytorch.length_penalty is not None:
            kwargs["length_penalty"] = config.pytorch.length_penalty
```

### Pattern 4: cache_implementation with CUDA Graphs
The `"static"` cache implementation enables CUDA graph capture, but requires fixed-length batches (padding). With variable-length batches, use the default dynamic KV cache.

```python
# Warning to emit when cache_implementation="static" with batch_size > 1
# Requires all sequences in batch to be padded to the same length
```

### Pattern 5: Cross-Validator Additions

New validators needed:
1. `load_in_4bit` + `load_in_8bit` mutual exclusion — already exists
2. `num_beams > 1` requires `do_sample=False` for pure beam search (warning, not error)
3. `torch_compile_mode` requires `torch_compile=True` (validation error)
4. `bnb_4bit_*` fields require `load_in_4bit=True` (validation error or warning)
5. `cache_implementation` with `use_cache=False` — contradiction (error)

---

## Don't Hand-Roll

| Problem | Don't Build | Use Instead |
|---------|-------------|-------------|
| BitsAndBytes quantization config | Custom quantization wrappers | `transformers.BitsAndBytesConfig` |
| torch.compile wrapping | Manual compilation logic | `torch.compile(model, mode=..., backend=...)` |
| Flash Attention detection | Manual capability checks | attn_implementation field + Transformers handles the rest |

---

## Common Pitfalls

### Pitfall 1: Passing load_in_4bit/load_in_8bit Directly
**What goes wrong:** The current backend passes `load_in_4bit=True` directly to `from_pretrained()`. This is the legacy API.
**Why it matters:** `bnb_4bit_compute_dtype`, `bnb_4bit_quant_type`, and `bnb_4bit_use_double_quant` only take effect when passed via `BitsAndBytesConfig`.
**How to avoid:** Always build `BitsAndBytesConfig` when any bnb field is set.

### Pitfall 2: torch.compile Not Applied
**What goes wrong:** `PyTorchConfig.torch_compile=True` is accepted by the config but `core/backends/pytorch.py::_model_load_kwargs()` does NOT call `torch.compile()`. The field is silently ignored.
**Why it matters:** Researcher sets `torch_compile: true` and sees no speedup.
**How to avoid:** Phase 4.1 must wire `torch_compile` → `torch.compile(model, ...)` post-load.

### Pitfall 3: flash_attention_3 Literal Missing
**What goes wrong:** Transformers 5.x added `flash_attention_3` as a valid `attn_implementation` value. Current Literal only has `"sdpa"`, `"flash_attention_2"`, `"eager"`.
**Why it matters:** Researchers on H100 GPUs may want FA3; the config rejects it.
**How to avoid:** Add `"flash_attention_3"` to the Literal.

### Pitfall 4: SSOT Introspection Not Updated
**What goes wrong:** New fields added to `PyTorchConfig` are not reflected in `get_backend_specific_params()` in `introspection.py`, and new cross-validators not added to `get_validation_rules()`.
**How to avoid:** Phase 4.1 MUST update `introspection.py` and `ssot.py` after every field addition.

### Pitfall 5: Beam Search Requires device_map Consideration
**What goes wrong:** Beam search with `num_beams > 1` can OOM on GPU with large models if memory is not accounted for (beam search holds `num_beams` copies of KV cache).
**How to avoid:** Document the memory cost in field description; no need to block it.

### Pitfall 6: `cache_implementation="static"` Requires Fixed-Length Batches
**What goes wrong:** Static KV cache with CUDA graphs requires all sequences in a batch to be the same length. Variable-length prompts will cause shape errors.
**How to avoid:** Document the constraint. Consider a warning when `batch_size > 1` and `cache_implementation="static"`.

---

## SSOT Updates Required

The SSOT files (`config/ssot.py`, `config/introspection.py`) must be updated alongside schema changes.

### `config/ssot.py` additions:
- `DECODER_PARAM_SUPPORT`: Add `"min_p"`, `"min_new_tokens"` for pytorch
- No changes needed to `PRECISION_SUPPORT` or `DECODING_SUPPORT`

### `config/introspection.py` additions:
- `get_backend_specific_params()["pytorch"]`: Add all new `PyTorchConfig` field paths
- `get_mutual_exclusions()`: Add `torch_compile_mode` requires `torch_compile`; `bnb_4bit_*` requires `load_in_4bit`
- `get_param_skip_conditions()`: Add skip conditions for `flash_attention_3` (Hopper+ GPU), beam search fields, `prompt_lookup_num_tokens`
- `get_validation_rules()`: Add new cross-validator documentation

---

## Peer Tool Comparison

What parameters do peer tools expose for their PyTorch/Transformers backend:

| Parameter | lm-eval-harness | This tool (M1) | Gap? |
|-----------|----------------|----------------|------|
| `dtype` | YES (`dtype` arg) | YES (via `precision`) | COVERED |
| `device_map` | YES (via `parallelize`) | Hardcoded "auto" | ADD |
| `trust_remote_code` | YES | Hardcoded True | ADD |
| `revision` | YES | Not exposed | ADD |
| `attn_implementation` | Not directly | YES | |
| `load_in_4bit` | YES (via autogptq/bitsandbytes) | YES | COVERED |
| `load_in_8bit` | YES | YES | COVERED |
| `batch_size` | YES | YES | COVERED |
| `max_memory` | YES (max_memory_per_gpu) | Not exposed | ADD |
| `temperature` | YES (gen_kwargs) | YES | COVERED |
| `top_p` | YES (gen_kwargs) | YES | COVERED |
| `top_k` | YES (gen_kwargs) | YES | COVERED |
| `min_p` | Not found | NO | ADD |
| `num_beams` | Not exposed | NO | ADD |
| `repetition_penalty` | YES (gen_kwargs) | YES | COVERED |
| `no_repeat_ngram_size` | Not exposed | NO | ADD |

---

## Open Questions

1. **`batching_strategy` — add back or leave to passthrough_kwargs?**
   - What we know: v1.x had `"static"` and `"dynamic"` strategies. v2.0 design doc dropped it.
   - What's unclear: Is dynamic batching (max_tokens_per_batch) useful for M1 measurement scenarios?
   - Recommendation: Omit from Phase 4.1. Single-experiment PyTorch measurement = static batching. Dynamic batching is a serving concern.

2. **`num_beams` placement: `PyTorchConfig` vs `DecoderConfig`?**
   - What we know: vLLM and TensorRT-LLM both support beam search. If in `DecoderConfig`, it becomes universal. If in `PyTorchConfig`, it's pytorch-only.
   - What's unclear: Do vLLM/TensorRT implement beam search identically?
   - Recommendation: Put in `PyTorchConfig` for now (pytorch-only). Phase 4.1 scope is PyTorch only. Move to DecoderConfig in M3 after verifying cross-backend semantics.

3. **`revision` field — included in config_hash?**
   - What we know: `revision` is critical for reproducibility (same model ID, different revision = different weights).
   - What's unclear: Current `compute_config_hash()` uses `config.model_dump()` — if `revision` is in `PyTorchConfig`, it is included.
   - Recommendation: Yes, include. A researcher comparing `revision=None` vs `revision="abc123"` should get different hashes.

4. **`torch_compile_mode` as `str` or `Literal`?**
   - Modes: `"default"`, `"reduce-overhead"`, `"max-autotune"`, `"max-autotune-no-cudagraphs"` + possibly more in future PyTorch versions.
   - Recommendation: Use `str | None` rather than Literal to avoid restricting future torch versions.

---

## Validation Architecture

No nyquist_validation configured — validation section skipped per config.json.

---

## Sources

### Primary (HIGH confidence)
- HuggingFace Transformers docs v5.2.0 — `GenerationConfig` all parameters (verified from source: `generation/configuration_utils.py`): https://huggingface.co/docs/transformers/en/main_classes/text_generation
- HuggingFace BitsAndBytes quantization docs: https://huggingface.co/docs/transformers/en/quantization/bitsandbytes
- Codebase source: `src/llenergymeasure/config/backend_configs.py` — current M1 PyTorchConfig
- Codebase source: `src/llenergymeasure/config/models.py` — current DecoderConfig
- Codebase source: `src/llenergymeasure/core/backends/pytorch.py` — v2.0 backend wiring
- Product design: `.product/designs/experiment-config.md` — target schema (SSOT for decisions)

### Secondary (MEDIUM confidence)
- v1.x backend source: `src/llenergymeasure/core/inference_backends/pytorch.py` — full parameter set implemented in v1.x, used to derive the disposition matrix
- lm-evaluation-harness `HFLM.__init__()` parameter list — peer tool comparison: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py

---

## Metadata

**Confidence breakdown:**
- Parameter inventory (from_pretrained): HIGH — verified from HF docs + source
- Parameter inventory (generate/GenerationConfig): HIGH — verified from HF source code
- v1.x disposition matrix: HIGH — directly from codebase
- Wiring gaps (torch_compile not applied): HIGH — directly from code inspection
- BitsAndBytes legacy API issue: HIGH — verified from HF docs
- Peer tool comparison: MEDIUM — lm-eval-harness source inspected via WebFetch

**Research date:** 2026-02-26
**Valid until:** 2026-08-26 (stable Transformers API — 6 month window)
