---
phase: 04.1-pytorch-parameter-audit
plan: 03
type: execute
wave: 2
depends_on: ["04.1-01"]
files_modified:
  - src/llenergymeasure/config/ssot.py
  - src/llenergymeasure/config/introspection.py
  - tests/unit/test_config_backend_configs.py
autonomous: true
requirements: []

must_haves:
  truths:
    - "DECODER_PARAM_SUPPORT includes min_p and min_new_tokens for pytorch"
    - "get_backend_specific_params() lists all new PyTorchConfig field paths"
    - "get_mutual_exclusions() includes torch_compile_mode/torch_compile and bnb_4bit_*/load_in_4bit dependencies"
    - "get_param_skip_conditions() includes flash_attention_3 (Hopper GPU) and BitsAndBytes fields"
    - "get_validation_rules() documents the 3 new cross-validators"
    - "get_backend_capabilities() reflects torch_compile, beam_search, speculative_decoding, static_kv_cache capabilities"
    - "Unit tests cover all new PyTorchConfig fields, DecoderConfig additions, and cross-validators (v2.0 schema)"
  artifacts:
    - path: "src/llenergymeasure/config/ssot.py"
      provides: "Updated DECODER_PARAM_SUPPORT with min_p, min_new_tokens"
      contains: "min_p"
    - path: "src/llenergymeasure/config/introspection.py"
      provides: "Updated SSOT introspection for expanded PyTorchConfig"
      contains: "torch_compile_mode"
    - path: "tests/unit/test_config_backend_configs.py"
      provides: "v2.0 backend config tests (replaces stale v1.x tests)"
      contains: "class TestPyTorchConfig"
  key_links:
    - from: "src/llenergymeasure/config/introspection.py::get_backend_specific_params"
      to: "src/llenergymeasure/config/backend_configs.py"
      via: "listing all PyTorchConfig field paths"
      pattern: "pytorch\\.torch_compile_mode"
    - from: "tests/unit/test_config_backend_configs.py"
      to: "src/llenergymeasure/config/backend_configs.py"
      via: "import and validate PyTorchConfig v2.0"
      pattern: "from llenergymeasure.config.backend_configs import.*PyTorchConfig"
---

<objective>
Update SSOT metadata files and rewrite unit tests to reflect the expanded PyTorchConfig and DecoderConfig schemas.

Purpose: The SSOT files (ssot.py, introspection.py) are consumed by tests, CLI, and docs. They must reflect the new fields or downstream consumers will have stale metadata. The existing test file (test_config_backend_configs.py) imports v1.x classes that no longer exist — it must be rewritten for v2.0.

Output: Updated ssot.py and introspection.py with complete field metadata, and a rewritten test_config_backend_configs.py covering v2.0 PyTorchConfig, DecoderConfig new fields, and all cross-validators.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/04.1-pytorch-parameter-audit/04.1-RESEARCH.md
@.planning/phases/04.1-pytorch-parameter-audit/04.1-01-SUMMARY.md

<interfaces>
<!-- Current SSOT files to update -->
From src/llenergymeasure/config/ssot.py:

```python
DECODER_PARAM_SUPPORT: dict[str, list[str]] = {
    "pytorch": ["temperature", "top_k", "top_p", "repetition_penalty"],
    "vllm": ["temperature", "top_k", "top_p", "repetition_penalty"],
    "tensorrt": ["temperature", "top_k", "top_p"],
}
```

From src/llenergymeasure/config/introspection.py (key functions to update):

```python
def get_backend_specific_params() -> dict[str, list[str]]:
    return {
        "pytorch": [
            "pytorch.batch_size",
            "pytorch.attn_implementation",
            "pytorch.torch_compile",
            "pytorch.load_in_4bit",
            "pytorch.load_in_8bit",
            "pytorch.num_processes",
        ],
        ...
    }

def get_mutual_exclusions() -> dict[str, list[str]]:
    return {
        "pytorch.load_in_4bit": ["pytorch.load_in_8bit"],
        "pytorch.load_in_8bit": ["pytorch.load_in_4bit"],
        ...
    }

def get_param_skip_conditions() -> dict[str, str]: ...
def get_validation_rules() -> list[dict[str, str]]: ...
def get_backend_capabilities() -> dict[str, dict[str, bool | str]]: ...
```

Existing test file (STALE — imports v1.x classes):
tests/unit/test_config_backend_configs.py imports PyTorchAssistedGenerationConfig,
TensorRTCalibrationConfig, VLLMAttentionConfig, VLLMLoRAConfig, VLLMSpeculativeConfig
— none of which exist in v2.0 backend_configs.py.
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update SSOT metadata files (ssot.py and introspection.py)</name>
  <files>
    src/llenergymeasure/config/ssot.py
    src/llenergymeasure/config/introspection.py
  </files>
  <action>
**ssot.py updates:**

Update `DECODER_PARAM_SUPPORT` to include new DecoderConfig fields for pytorch:
```python
DECODER_PARAM_SUPPORT: dict[str, list[str]] = {
    "pytorch": ["temperature", "top_k", "top_p", "repetition_penalty", "min_p", "min_new_tokens"],
    "vllm": ["temperature", "top_k", "top_p", "repetition_penalty"],
    "tensorrt": ["temperature", "top_k", "top_p"],
}
```
(min_p and min_new_tokens added for pytorch only — vLLM/TensorRT support is M3 audit scope.)

No changes to `PRECISION_SUPPORT` or `DECODING_SUPPORT`.

**introspection.py updates:**

1. **`get_backend_specific_params()`** — Replace the pytorch list with all v2.0 fields:
```python
"pytorch": [
    "pytorch.batch_size",
    "pytorch.attn_implementation",
    "pytorch.torch_compile",
    "pytorch.torch_compile_mode",
    "pytorch.torch_compile_backend",
    "pytorch.load_in_4bit",
    "pytorch.load_in_8bit",
    "pytorch.bnb_4bit_compute_dtype",
    "pytorch.bnb_4bit_quant_type",
    "pytorch.bnb_4bit_use_double_quant",
    "pytorch.use_cache",
    "pytorch.cache_implementation",
    "pytorch.num_beams",
    "pytorch.early_stopping",
    "pytorch.length_penalty",
    "pytorch.no_repeat_ngram_size",
    "pytorch.prompt_lookup_num_tokens",
    "pytorch.device_map",
    "pytorch.max_memory",
    "pytorch.revision",
    "pytorch.trust_remote_code",
    "pytorch.num_processes",
],
```
Do NOT change vllm or tensorrt lists.

2. **`get_mutual_exclusions()`** — Add new mutual exclusion entries:
```python
# torch_compile sub-options require torch_compile=True
"pytorch.torch_compile_mode": ["pytorch.torch_compile=None|False"],
"pytorch.torch_compile_backend": ["pytorch.torch_compile=None|False"],
# BitsAndBytes 4-bit sub-options require load_in_4bit=True
"pytorch.bnb_4bit_compute_dtype": ["pytorch.load_in_4bit=None|False"],
"pytorch.bnb_4bit_quant_type": ["pytorch.load_in_4bit=None|False"],
"pytorch.bnb_4bit_use_double_quant": ["pytorch.load_in_4bit=None|False"],
# cache_implementation contradicts use_cache=False
"pytorch.cache_implementation": ["pytorch.use_cache=False"],
```

3. **`get_param_skip_conditions()`** — Add skip conditions for new fields:
```python
# Flash Attention 3 - requires Hopper+ (H100)
"pytorch.attn_implementation=flash_attention_3": "Requires Hopper+ GPU (compute capability 9.0+)",
# torch.compile - may not work on all model architectures
"pytorch.torch_compile=True": "May fail on some model architectures (non-fatal fallback)",
# BitsAndBytes - requires bitsandbytes package
"pytorch.bnb_4bit_compute_dtype": "Requires load_in_4bit=True and bitsandbytes package",
"pytorch.bnb_4bit_quant_type": "Requires load_in_4bit=True and bitsandbytes package",
"pytorch.bnb_4bit_use_double_quant": "Requires load_in_4bit=True and bitsandbytes package",
# Prompt lookup speculative decoding
"pytorch.prompt_lookup_num_tokens": "Requires compatible model and sufficient prompt overlap",
```

4. **`get_params_requiring_gpu_capability()`** — Add flash_attention_3 for Hopper:
```python
hopper_required = [
    "pytorch.attn_implementation=flash_attention_3",
]
ampere_required = [
    "vllm.quantization=fp8",
    "tensorrt.quantization=fp8",
    "pytorch.attn_implementation=flash_attention_2",
]
```
Update the function to return hopper_required when min_compute_capability >= 9.0, and ampere_required + hopper_required when both thresholds are relevant.

5. **`get_validation_rules()`** — Add 3 new rules for the cross-validators:
```python
{
    "backend": "pytorch",
    "combination": "torch_compile_mode without torch_compile=True",
    "reason": "torch_compile_mode/torch_compile_backend only take effect when torch_compile=True",
    "resolution": "Set pytorch.torch_compile=true when using torch_compile_mode or torch_compile_backend",
},
{
    "backend": "pytorch",
    "combination": "bnb_4bit_* without load_in_4bit=True",
    "reason": "BitsAndBytes 4-bit options require 4-bit quantization to be enabled",
    "resolution": "Set pytorch.load_in_4bit=true when using bnb_4bit_compute_dtype, bnb_4bit_quant_type, or bnb_4bit_use_double_quant",
},
{
    "backend": "pytorch",
    "combination": "cache_implementation with use_cache=False",
    "reason": "Cannot specify a cache strategy when caching is explicitly disabled",
    "resolution": "Remove use_cache=false or remove cache_implementation",
},
```

6. **`get_backend_capabilities()`** — Add new capability rows:
```python
"torch_compile": {
    "pytorch": "torch_compile" in pytorch_fields,
    "vllm": False,
    "tensorrt": False,
},
"beam_search": {
    "pytorch": "num_beams" in pytorch_fields,
    "vllm": False,  # vLLM best_of removed in v1
    "tensorrt": False,
},
"speculative_decoding": {
    "pytorch": "prompt_lookup_num_tokens" in pytorch_fields,
    "vllm": False,  # vLLM speculative in VLLMConfig (not minimal v2.0)
    "tensorrt": False,
},
"static_kv_cache": {
    "pytorch": "cache_implementation" in pytorch_fields,
    "vllm": False,
    "tensorrt": False,
},
```

Also update `get_capability_matrix_markdown()` with display names for the new capabilities:
```python
"torch_compile": "torch.compile",
"beam_search": "Beam Search",
"speculative_decoding": "Speculative Decoding",
"static_kv_cache": "Static KV Cache",
```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && /usr/bin/python3 -c "
from llenergymeasure.config.ssot import DECODER_PARAM_SUPPORT
from llenergymeasure.config.introspection import (
    get_backend_specific_params,
    get_mutual_exclusions,
    get_param_skip_conditions,
    get_validation_rules,
    get_backend_capabilities,
    get_params_requiring_gpu_capability,
    get_capability_matrix_markdown,
)

# SSOT checks
assert 'min_p' in DECODER_PARAM_SUPPORT['pytorch']
assert 'min_new_tokens' in DECODER_PARAM_SUPPORT['pytorch']

# Introspection checks
pt_params = get_backend_specific_params()['pytorch']
assert 'pytorch.torch_compile_mode' in pt_params
assert 'pytorch.bnb_4bit_compute_dtype' in pt_params
assert 'pytorch.device_map' in pt_params
assert 'pytorch.revision' in pt_params
assert 'pytorch.cache_implementation' in pt_params
assert 'pytorch.num_beams' in pt_params
assert 'pytorch.prompt_lookup_num_tokens' in pt_params
assert len(pt_params) >= 22, f'Expected >=22 params, got {len(pt_params)}'

# Mutual exclusions
excl = get_mutual_exclusions()
assert 'pytorch.torch_compile_mode' in excl
assert 'pytorch.bnb_4bit_compute_dtype' in excl
assert 'pytorch.cache_implementation' in excl

# Skip conditions
skip = get_param_skip_conditions()
assert 'pytorch.attn_implementation=flash_attention_3' in skip

# Validation rules
rules = get_validation_rules()
rule_combos = [r['combination'] for r in rules]
assert any('torch_compile_mode' in c for c in rule_combos)
assert any('bnb_4bit' in c for c in rule_combos)
assert any('cache_implementation' in c for c in rule_combos)

# Capabilities
caps = get_backend_capabilities()
assert 'torch_compile' in caps
assert 'beam_search' in caps
assert caps['torch_compile']['pytorch'] is True
assert caps['beam_search']['pytorch'] is True

# GPU capability
hopper = get_params_requiring_gpu_capability(9.0)
assert any('flash_attention_3' in p for p in hopper)

# Markdown generation (smoke test)
md = get_capability_matrix_markdown()
assert 'torch.compile' in md

print('All SSOT checks passed')
"
    </automated>
  </verify>
  <done>
    ssot.py DECODER_PARAM_SUPPORT includes min_p and min_new_tokens for pytorch. introspection.py has all new PyTorchConfig fields in get_backend_specific_params(), new mutual exclusions, skip conditions, validation rules, capability matrix entries, and GPU capability thresholds for flash_attention_3.
  </done>
</task>

<task type="auto">
  <name>Task 2: Rewrite unit tests for v2.0 backend configs</name>
  <files>tests/unit/test_config_backend_configs.py</files>
  <action>
Completely rewrite `tests/unit/test_config_backend_configs.py`. The current file imports v1.x classes (`PyTorchAssistedGenerationConfig`, `VLLMAttentionConfig`, `VLLMSpeculativeConfig`, `VLLMLoRAConfig`, `TensorRTCalibrationConfig`) that no longer exist in the v2.0 `backend_configs.py`.

**New test file structure:**

```python
"""Tests for backend-specific configuration models (v2.0 schema)."""

import pytest
from pydantic import ValidationError

from llenergymeasure.config.backend_configs import (
    PyTorchConfig,
    TensorRTConfig,
    VLLMConfig,
)
from llenergymeasure.config.models import DecoderConfig, ExperimentConfig
```

**Test classes to create:**

1. **`TestPyTorchConfig`** — Test all PyTorchConfig fields:
   - `test_defaults`: All fields None by default
   - `test_batch_size_constraint`: ge=1 enforced
   - `test_attn_implementation_values`: All 4 Literal values accepted (sdpa, flash_attention_2, flash_attention_3, eager); invalid rejected
   - `test_torch_compile_bool`: True/False/None accepted
   - `test_torch_compile_mode_str`: Any string accepted (not Literal)
   - `test_bnb_4bit_compute_dtype_values`: float16, bfloat16, float32 accepted; invalid rejected
   - `test_bnb_4bit_quant_type_values`: nf4, fp4 accepted; invalid rejected
   - `test_use_cache_bool`: True/False/None accepted
   - `test_cache_implementation_values`: static, offloaded_static, sliding_window; invalid rejected
   - `test_num_beams_constraint`: ge=1 enforced
   - `test_no_repeat_ngram_size_constraint`: ge=0 enforced
   - `test_prompt_lookup_num_tokens_constraint`: ge=1 enforced
   - `test_device_map_str`: Any string accepted
   - `test_max_memory_dict`: Dict accepted
   - `test_revision_str`: Any string accepted
   - `test_trust_remote_code_bool`: True/False/None accepted
   - `test_num_processes_constraint`: ge=1 enforced
   - `test_extra_forbid`: Unknown fields rejected

2. **`TestPyTorchConfigCrossValidators`** — Test all 4 validators:
   - `test_quantization_mutual_exclusion`: load_in_4bit + load_in_8bit raises
   - `test_quantization_individual_ok`: Each alone is fine
   - `test_torch_compile_mode_requires_compile`: torch_compile_mode without torch_compile raises
   - `test_torch_compile_mode_with_compile_ok`: Both set works
   - `test_torch_compile_backend_requires_compile`: torch_compile_backend without torch_compile raises
   - `test_bnb_4bit_compute_dtype_requires_load_in_4bit`: Raises without load_in_4bit
   - `test_bnb_4bit_quant_type_requires_load_in_4bit`: Raises without load_in_4bit
   - `test_bnb_4bit_use_double_quant_requires_load_in_4bit`: Raises without load_in_4bit
   - `test_bnb_4bit_options_with_load_in_4bit_ok`: All bnb fields with load_in_4bit=True works
   - `test_cache_implementation_with_use_cache_false_raises`: Contradiction detected
   - `test_cache_implementation_with_use_cache_none_ok`: None is fine (means use default)
   - `test_cache_implementation_with_use_cache_true_ok`: Explicit True works

3. **`TestDecoderConfigNewFields`** — Test the 2 new DecoderConfig fields:
   - `test_min_p_default_none`: Default is None
   - `test_min_p_valid_range`: 0.0-1.0 accepted
   - `test_min_p_out_of_range`: -0.1 and 1.1 rejected
   - `test_min_new_tokens_default_none`: Default is None
   - `test_min_new_tokens_constraint`: ge=1 enforced; 0 rejected
   - `test_preset_expansion_with_new_fields`: Preset still works, new fields remain None

4. **`TestVLLMConfig`** — Minimal v2.0 tests:
   - `test_defaults`: All fields None
   - `test_valid_quantization_values`: awq, gptq, fp8 accepted
   - `test_extra_forbid`: Unknown fields rejected

5. **`TestTensorRTConfig`** — Minimal v2.0 tests:
   - `test_defaults`: All fields None
   - `test_valid_quantization_values`: int8_sq, int4_awq, fp8 accepted
   - `test_extra_forbid`: Unknown fields rejected

6. **`TestExperimentConfigBackendIntegration`** — v2.0 integration:
   - `test_pytorch_config_with_new_fields`: ExperimentConfig accepts PyTorchConfig with new fields
   - `test_backend_section_mismatch_rejected`: pytorch section with backend=vllm rejected
   - `test_serialization_roundtrip`: Full PyTorchConfig survives JSON roundtrip

Every test must use the v2.0 imports only (`PyTorchConfig`, `VLLMConfig`, `TensorRTConfig` from backend_configs, `DecoderConfig`/`ExperimentConfig` from models). Do NOT import any v1.x classes.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && /usr/bin/python3 -m pytest tests/unit/test_config_backend_configs.py -v --tb=short 2>&1 | tail -40</automated>
  </verify>
  <done>
    tests/unit/test_config_backend_configs.py has been rewritten for v2.0. All tests pass. Covers: all new PyTorchConfig fields, all 4 cross-validators, DecoderConfig min_p and min_new_tokens, VLLMConfig and TensorRTConfig v2.0 schemas, and ExperimentConfig integration with new fields. No v1.x imports remain.
  </done>
</task>

</tasks>

<verification>
1. `DECODER_PARAM_SUPPORT["pytorch"]` includes "min_p" and "min_new_tokens"
2. `get_backend_specific_params()["pytorch"]` lists all 22+ PyTorchConfig field paths
3. `get_mutual_exclusions()` includes entries for torch_compile_mode, bnb_4bit_*, cache_implementation
4. `get_param_skip_conditions()` includes flash_attention_3, torch_compile, bnb_4bit_* entries
5. `get_validation_rules()` has 3 new rules (8+ total)
6. `get_backend_capabilities()` includes torch_compile, beam_search, speculative_decoding, static_kv_cache
7. `pytest tests/unit/test_config_backend_configs.py` passes with 30+ tests, all v2.0 schema
8. No import errors from stale v1.x class references
</verification>

<success_criteria>
SSOT metadata files accurately reflect the expanded PyTorchConfig. Unit tests cover all new fields and cross-validators with no v1.x artifacts remaining. `pytest tests/unit/test_config_backend_configs.py` passes cleanly.
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-pytorch-parameter-audit/04.1-03-SUMMARY.md`
</output>
