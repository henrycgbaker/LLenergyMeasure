---
phase: 04.1-pytorch-parameter-audit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/backend_configs.py
  - src/llenergymeasure/config/models.py
autonomous: true
requirements: []

must_haves:
  truths:
    - "PyTorchConfig has all researcher-useful from_pretrained() fields: attn_implementation (with flash_attention_3), torch_compile, torch_compile_mode, torch_compile_backend, BitsAndBytes fields (bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant), device_map, max_memory, revision, trust_remote_code, use_cache, cache_implementation, num_beams, early_stopping, length_penalty, no_repeat_ngram_size, prompt_lookup_num_tokens"
    - "DecoderConfig has min_p and min_new_tokens fields"
    - "Cross-validators enforce: torch_compile_mode requires torch_compile=True, bnb_4bit_* requires load_in_4bit=True, cache_implementation requires use_cache != False"
    - "All new fields follow None-as-default pattern (None = use backend own default)"
    - "extra=forbid still enforced — no regressions on invalid field rejection"
  artifacts:
    - path: "src/llenergymeasure/config/backend_configs.py"
      provides: "Complete PyTorchConfig with ~20 additional fields"
      contains: "class PyTorchConfig"
    - path: "src/llenergymeasure/config/models.py"
      provides: "DecoderConfig with min_p and min_new_tokens"
      contains: "min_p"
  key_links:
    - from: "src/llenergymeasure/config/backend_configs.py"
      to: "pydantic model_validator"
      via: "cross-validators on PyTorchConfig"
      pattern: "model_validator"
---

<objective>
Expand PyTorchConfig and DecoderConfig to cover all researcher-useful parameters from HuggingFace Transformers `from_pretrained()` and `model.generate()`.

Purpose: Close the gap between the M1 minimal schema (6 PyTorchConfig fields, 5 DecoderConfig fields) and the complete set identified in the Phase 4.1 research. Researchers should not need `passthrough_kwargs` for common parameters.

Output: Updated `backend_configs.py` with ~20 new PyTorchConfig fields and `models.py` with 2 new DecoderConfig fields, plus cross-validators for new field interactions.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/04.1-pytorch-parameter-audit/04.1-RESEARCH.md

<interfaces>
<!-- Current PyTorchConfig (M1 minimal) — to be expanded -->
From src/llenergymeasure/config/backend_configs.py:

```python
class PyTorchConfig(BaseModel):
    model_config = {"extra": "forbid"}

    batch_size: int | None = Field(default=None, ge=1, description="Batch size (None -> 1)")
    attn_implementation: Literal["sdpa", "flash_attention_2", "eager"] | None = Field(...)
    torch_compile: bool | None = Field(default=None, description="Enable torch.compile (None -> False)")
    load_in_4bit: bool | None = Field(...)
    load_in_8bit: bool | None = Field(...)
    num_processes: int | None = Field(default=None, ge=1, description="Data parallel processes via Accelerate (None -> 1)")

    @model_validator(mode="after")
    def validate_quantization(self) -> "PyTorchConfig":
        """4-bit and 8-bit quantization are mutually exclusive."""
        ...
```

From src/llenergymeasure/config/models.py:

```python
class DecoderConfig(BaseModel):
    model_config = {"extra": "forbid"}

    temperature: float = Field(default=1.0, ge=0.0, le=2.0, ...)
    do_sample: bool = Field(default=True, ...)
    top_k: int = Field(default=50, ge=0, ...)
    top_p: float = Field(default=1.0, ge=0.0, le=1.0, ...)
    repetition_penalty: float = Field(default=1.0, ge=0.1, le=10.0, ...)
    preset: Literal["deterministic", "standard", "creative", "factual"] | None = Field(...)
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Expand PyTorchConfig fields and cross-validators</name>
  <files>src/llenergymeasure/config/backend_configs.py</files>
  <action>
Expand PyTorchConfig with the following new fields, all using None-as-default. Group fields by concern with inline comments. Preserve existing fields and the existing `validate_quantization` validator.

**New fields to add (in order):**

1. **Attention** — Update `attn_implementation` Literal to include `"flash_attention_3"`:
   ```python
   attn_implementation: Literal["sdpa", "flash_attention_2", "flash_attention_3", "eager"] | None
   ```

2. **Compilation** (after existing `torch_compile`):
   ```python
   torch_compile_mode: str | None = Field(default=None, description="torch.compile mode: 'default', 'reduce-overhead', 'max-autotune' (None -> 'default')")
   torch_compile_backend: str | None = Field(default=None, description="torch.compile backend (None -> 'inductor')")
   ```
   Use `str | None` (not Literal) for both — PyTorch may add modes in future versions.

3. **BitsAndBytes quantization** (after existing `load_in_8bit`):
   ```python
   bnb_4bit_compute_dtype: Literal["float16", "bfloat16", "float32"] | None = Field(default=None, description="Compute dtype for 4-bit (None -> float32, usually want bfloat16)")
   bnb_4bit_quant_type: Literal["nf4", "fp4"] | None = Field(default=None, description="4-bit quantization type (None -> 'nf4')")
   bnb_4bit_use_double_quant: bool | None = Field(default=None, description="Double quantization saves ~0.4 bits/param (None -> False)")
   ```

4. **KV caching**:
   ```python
   use_cache: bool | None = Field(default=None, description="Use KV cache during generation (None -> True)")
   cache_implementation: Literal["static", "offloaded_static", "sliding_window"] | None = Field(default=None, description="KV cache strategy; 'static' enables CUDA graphs (None -> dynamic)")
   ```

5. **Beam search** (all flat fields, NOT nested):
   ```python
   num_beams: int | None = Field(default=None, ge=1, description="Beam search width (None -> 1, greedy/sampling)")
   early_stopping: bool | None = Field(default=None, description="Stop beam search when all beams hit EOS (None -> False)")
   length_penalty: float | None = Field(default=None, description="Beam length penalty: >1 shorter, <1 longer (None -> 1.0)")
   ```

6. **N-gram repetition**:
   ```python
   no_repeat_ngram_size: int | None = Field(default=None, ge=0, description="Prevent n-gram repetition (None -> 0, disabled)")
   ```

7. **Speculative decoding** (prompt-lookup only — draft model via passthrough_kwargs):
   ```python
   prompt_lookup_num_tokens: int | None = Field(default=None, ge=1, description="Prompt-lookup speculative decoding tokens (None -> disabled)")
   ```

8. **Model loading**:
   ```python
   device_map: str | None = Field(default=None, description="Device placement strategy (None -> 'auto')")
   max_memory: dict | None = Field(default=None, description="Per-device memory limits, e.g. {0: '10GiB', 'cpu': '50GiB'}")
   revision: str | None = Field(default=None, description="Model revision/commit hash for reproducibility")
   trust_remote_code: bool | None = Field(default=None, description="Trust remote code in model repo (None -> True)")
   ```

**New cross-validators to add** (as separate `@model_validator(mode="after")` methods on PyTorchConfig):

1. `validate_torch_compile_options`: If `torch_compile_mode` or `torch_compile_backend` is set but `torch_compile` is not `True`, raise ValueError: "torch_compile_mode/torch_compile_backend requires torch_compile=True".

2. `validate_bnb_4bit_options`: If any of `bnb_4bit_compute_dtype`, `bnb_4bit_quant_type`, or `bnb_4bit_use_double_quant` is set but `load_in_4bit` is not `True`, raise ValueError: "bnb_4bit_* fields require load_in_4bit=True".

3. `validate_cache_options`: If `cache_implementation` is set but `use_cache is False` (explicitly False, not None), raise ValueError: "cache_implementation requires use_cache to be True or None (not explicitly False)".

**Keep existing** `validate_quantization` unchanged.

Do NOT change `VLLMConfig` or `TensorRTConfig` — those are M3 scope.
Update the module docstring to remove the "M1 minimal" phrasing and replace with "v2.0 schema".
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && /usr/bin/python3 -c "
from llenergymeasure.config.backend_configs import PyTorchConfig
# Test all new fields exist with None defaults
c = PyTorchConfig()
assert c.batch_size is None
assert c.attn_implementation is None
assert c.torch_compile is None
assert c.torch_compile_mode is None
assert c.torch_compile_backend is None
assert c.bnb_4bit_compute_dtype is None
assert c.bnb_4bit_quant_type is None
assert c.bnb_4bit_use_double_quant is None
assert c.use_cache is None
assert c.cache_implementation is None
assert c.num_beams is None
assert c.early_stopping is None
assert c.length_penalty is None
assert c.no_repeat_ngram_size is None
assert c.prompt_lookup_num_tokens is None
assert c.device_map is None
assert c.max_memory is None
assert c.revision is None
assert c.trust_remote_code is None
# Test flash_attention_3 accepted
c2 = PyTorchConfig(attn_implementation='flash_attention_3')
# Test cross-validators
import pydantic
try:
    PyTorchConfig(torch_compile_mode='reduce-overhead')
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
try:
    PyTorchConfig(bnb_4bit_compute_dtype='bfloat16')
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
try:
    PyTorchConfig(use_cache=False, cache_implementation='static')
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
# Test extra=forbid still works
try:
    PyTorchConfig(nonexistent_field=True)
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
print('All PyTorchConfig checks passed')
"
    </automated>
  </verify>
  <done>
    PyTorchConfig has all ~20 new fields from the research recommendations, all with None defaults. Three new cross-validators enforce field dependencies. extra=forbid still rejects unknown fields. flash_attention_3 is accepted in attn_implementation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add min_p and min_new_tokens to DecoderConfig</name>
  <files>src/llenergymeasure/config/models.py</files>
  <action>
Add two new fields to `DecoderConfig` after the existing `repetition_penalty` field (before `preset`):

```python
min_p: float | None = Field(
    default=None, ge=0.0, le=1.0, description="Min probability filter (None -> disabled)"
)
min_new_tokens: int | None = Field(
    default=None, ge=1, description="Minimum output token count (None -> no minimum)"
)
```

Both use None-as-default to distinguish researcher intent from backend defaults. These are universal parameters supported by all major backends (PyTorch, vLLM).

Do NOT change any other DecoderConfig fields or existing validators. The `_validate_sampling_presets()` call at module level will continue to work because new fields have default=None and are not referenced in SAMPLING_PRESETS.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && /usr/bin/python3 -c "
from llenergymeasure.config.models import DecoderConfig
# Test new fields exist with correct defaults
d = DecoderConfig()
assert d.min_p is None
assert d.min_new_tokens is None
# Test valid values
d2 = DecoderConfig(min_p=0.1, min_new_tokens=10)
assert d2.min_p == 0.1
assert d2.min_new_tokens == 10
# Test constraints
import pydantic
try:
    DecoderConfig(min_p=-0.1)
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
try:
    DecoderConfig(min_p=1.1)
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
try:
    DecoderConfig(min_new_tokens=0)
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
# Test preset expansion still works
d3 = DecoderConfig(preset='deterministic')
assert d3.temperature == 0.0
assert d3.do_sample is False
# Test extra=forbid still works
try:
    DecoderConfig(nonexistent_field=True)
    assert False, 'Should have raised'
except pydantic.ValidationError:
    pass
print('All DecoderConfig checks passed')
"
    </automated>
  </verify>
  <done>
    DecoderConfig has min_p (float | None, 0-1 range) and min_new_tokens (int | None, ge=1). Both None-as-default. Preset expansion and extra=forbid unchanged.
  </done>
</task>

</tasks>

<verification>
1. `PyTorchConfig()` creates with all fields None (except existing defaults like model_config)
2. `PyTorchConfig(attn_implementation="flash_attention_3")` is valid
3. `PyTorchConfig(torch_compile_mode="reduce-overhead")` raises ValidationError (torch_compile not True)
4. `PyTorchConfig(bnb_4bit_compute_dtype="bfloat16")` raises ValidationError (load_in_4bit not True)
5. `PyTorchConfig(use_cache=False, cache_implementation="static")` raises ValidationError
6. `PyTorchConfig(load_in_4bit=True, bnb_4bit_compute_dtype="bfloat16")` is valid
7. `PyTorchConfig(torch_compile=True, torch_compile_mode="max-autotune")` is valid
8. `DecoderConfig(min_p=0.1, min_new_tokens=10)` is valid
9. `ExperimentConfig(model="gpt2", pytorch=PyTorchConfig(revision="abc123"))` is valid
10. `ExperimentConfig(model="gpt2", pytorch=PyTorchConfig(nonexistent=True))` raises ValidationError
</verification>

<success_criteria>
PyTorchConfig and DecoderConfig schemas match the Phase 4.1 research target. All new fields follow None-as-default. Cross-validators enforce field dependencies. No regressions on existing config validation.
</success_criteria>

<output>
After completion, create `.planning/phases/04.1-pytorch-parameter-audit/04.1-01-SUMMARY.md`
</output>
