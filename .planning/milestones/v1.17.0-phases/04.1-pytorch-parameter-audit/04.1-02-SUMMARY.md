---
phase: 04.1-pytorch-parameter-audit
plan: 02
subsystem: core/backends
tags: [pytorch, backend, quantization, beam-search, kv-cache, torch-compile]
dependency_graph:
  requires: [complete-pytorch-config, decoder-config-min-p]
  provides: [pytorch-full-parameter-wiring]
  affects: [core/backends/pytorch.py]
tech_stack:
  added: []
  patterns: [BitsAndBytesConfig, conditional-kwargs, post-load-compile, None-as-sentinel]
key_files:
  created: []
  modified:
    - src/llenergymeasure/core/backends/pytorch.py
decisions:
  - "BitsAndBytesConfig used instead of raw load_in_4bit/load_in_8bit kwargs — modern HuggingFace API; raw kwargs are legacy"
  - "torch.compile is non-fatal — compilation failure logs a warning and continues without compilation"
  - "trust_remote_code wired to both AutoTokenizer and AutoModelForCausalLM from config"
metrics:
  duration: ~6 min
  completed_date: "2026-02-26"
  tasks_completed: 2
  files_modified: 1
---

# Phase 4.1 Plan 02: PyTorch Parameter Audit - Backend Wiring Summary

**One-liner:** PyTorch backend fully wired — BitsAndBytesConfig for quantization, config-driven device_map/trust_remote_code, torch.compile post-load, and all beam search/KV cache/sampling params passed to model.generate().

## What Was Built

Closed the gap between the expanded PyTorchConfig schema (Plan 01) and the actual HuggingFace API calls. Every new config field now controls the corresponding model loading or inference behaviour. No field is silently ignored.

### Task 1 — Wire _model_load_kwargs() with BitsAndBytesConfig and loading fields

**Commit:** `e72a813`
**File:** `src/llenergymeasure/core/backends/pytorch.py`

Changes to `_model_load_kwargs()`:

| Change | Before | After |
|--------|--------|-------|
| `device_map` | `"auto"` hardcoded | config-driven, `"auto"` as default |
| `trust_remote_code` | `True` hardcoded | config-driven, `True` as default |
| Quantization | raw `load_in_4bit=True` kwarg | `BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=..., bnb_4bit_quant_type=..., bnb_4bit_use_double_quant=...)` |
| `revision` | absent | wired from `pt.revision` |
| `max_memory` | absent | wired from `pt.max_memory` |
| `attn_implementation` | already wired | unchanged |

Changes to `_load_model()`:
- `trust_remote_code` wired into `AutoTokenizer.from_pretrained()` (was hardcoded `True`)
- `torch.compile` post-load step: applies when `torch_compile=True`, uses `torch_compile_mode` and `torch_compile_backend` from config (non-fatal — warning on failure)

### Task 2 — Wire _build_generate_kwargs() with beam search, KV cache, and new DecoderConfig fields

**Commit:** `7754d17`
**File:** `src/llenergymeasure/core/backends/pytorch.py`

New kwargs added to `_build_generate_kwargs()`:

| Parameter | Source |
|-----------|--------|
| `min_p` | `DecoderConfig.min_p` |
| `min_new_tokens` | `DecoderConfig.min_new_tokens` |
| `use_cache` | `PyTorchConfig.use_cache` |
| `cache_implementation` | `PyTorchConfig.cache_implementation` |
| `num_beams` | `PyTorchConfig.num_beams` |
| `early_stopping` | `PyTorchConfig.early_stopping` |
| `length_penalty` | `PyTorchConfig.length_penalty` |
| `no_repeat_ngram_size` | `PyTorchConfig.no_repeat_ngram_size` |
| `prompt_lookup_num_tokens` | `PyTorchConfig.prompt_lookup_num_tokens` |

Greedy mode guard extended: now also strips `min_p` (sampling parameter) in addition to `temperature`, `top_k`, `top_p`.

## Decisions Made

1. **`BitsAndBytesConfig` instead of raw kwargs** — The raw `load_in_4bit=True` kwarg is legacy HuggingFace API. `BitsAndBytesConfig` is the current approach and allows bnb sub-options to be attached cleanly. No user-visible change — config schema is unchanged.

2. **`torch.compile` is non-fatal** — Some model architectures are not torch.compile-compatible. A warning is logged and inference continues without compilation. This avoids blocking valid experiments on a performance-optional feature.

3. **`trust_remote_code` wired to tokenizer too** — Consistency: both `AutoModelForCausalLM.from_pretrained()` and `AutoTokenizer.from_pretrained()` now respect the same config value. Previously the tokenizer always used `True`.

## Deviations from Plan

None — plan executed exactly as written.

## Verification Results

All 8 plan verification checks passed:

1. `_model_load_kwargs()` returns `quantization_config=BitsAndBytesConfig(...)` when `load_in_4bit=True` (no raw `load_in_4bit` kwarg)
2. `_model_load_kwargs()` returns `device_map` from config (not hardcoded `"auto"`)
3. `_model_load_kwargs()` returns `trust_remote_code` from config (not hardcoded `True`)
4. `_model_load_kwargs()` passes `revision`, `max_memory`, `attn_implementation` when set
5. `_load_model()` contains `torch.compile()` call when `torch_compile=True`
6. `_build_generate_kwargs()` includes `min_p`, `min_new_tokens`, `num_beams`, `use_cache`, `cache_implementation`, `no_repeat_ngram_size`, `prompt_lookup_num_tokens`
7. `None`-valued fields produce no kwargs entries
8. Greedy mode strips `min_p` along with `temperature`/`top_k`/`top_p`

## Self-Check: PASSED

Files modified exist:
- `src/llenergymeasure/core/backends/pytorch.py` — FOUND

Commits exist:
- `e72a813` — FOUND (feat(pytorch): wire _model_load_kwargs)
- `7754d17` — FOUND (feat(pytorch): wire _build_generate_kwargs)
