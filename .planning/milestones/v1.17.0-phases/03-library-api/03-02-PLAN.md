---
phase: 03-library-api
plan: 02
type: execute
wave: 2
depends_on: [03-01]
files_modified:
  - src/llenergymeasure/_api.py
  - src/llenergymeasure/__init__.py
  - tests/unit/test_api.py
autonomous: true
requirements: [LA-01, LA-03, LA-04, LA-06, LA-07, LA-08, LA-10]

must_haves:
  truths:
    - "`from llenergymeasure import run_experiment, run_study, ExperimentConfig, StudyConfig, ExperimentResult, StudyResult, __version__` resolves without error"
    - "`run_experiment(ExperimentConfig(...))` returns exactly `ExperimentResult` (no union, no None) when `_run` is mocked"
    - "`run_experiment()` with no `output_dir` produces no disk writes"
    - "`run_study(...)` raises `NotImplementedError` with M2 message"
    - "Any name NOT in `__init__.py.__all__` raises `AttributeError` on `from llenergymeasure import X`"
    - "`llenergymeasure.__version__ == '2.0.0'`"
  artifacts:
    - path: "src/llenergymeasure/_api.py"
      provides: "run_experiment(), run_study(), _to_study_config(), _run() stub"
      exports: ["run_experiment", "run_study"]
    - path: "src/llenergymeasure/__init__.py"
      provides: "Public API surface with __all__ and stability docstring"
      contains: "__all__"
    - path: "tests/unit/test_api.py"
      provides: "Unit tests for all API success criteria"
      min_lines: 80
  key_links:
    - from: "src/llenergymeasure/__init__.py"
      to: "src/llenergymeasure/_api.py"
      via: "from llenergymeasure._api import run_experiment, run_study"
      pattern: "from llenergymeasure\\._api import"
    - from: "src/llenergymeasure/_api.py"
      to: "src/llenergymeasure/config/loader.py"
      via: "load_experiment_config for YAML path form"
      pattern: "from llenergymeasure\\.config\\.loader import load_experiment_config"
    - from: "src/llenergymeasure/_api.py"
      to: "src/llenergymeasure/config/models.py"
      via: "ExperimentConfig + StudyConfig imports"
      pattern: "from llenergymeasure\\.config\\.models import"
---

<objective>
Create `_api.py` with the three-form `run_experiment()`, stub `run_study()`, and internal `_run()`. Wire `__init__.py` as the stable public API surface with `__all__` and stability contract docstring. Write unit tests covering all Phase 3 success criteria.

Purpose: This is the plan that makes `llenergymeasure` a usable library -- after this, `from llenergymeasure import run_experiment` works and the API contract is established for all downstream phases.

Output: `_api.py` (API implementation), rewritten `__init__.py` (public surface), `test_api.py` (validation).
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/03-library-api/03-CONTEXT.md
@.planning/phases/03-library-api/03-RESEARCH.md
@.planning/phases/03-library-api/03-01-SUMMARY.md
@src/llenergymeasure/__init__.py
@src/llenergymeasure/exceptions.py
@src/llenergymeasure/config/loader.py

<interfaces>
<!-- Types created by Plan 01 (dependency) -->

From src/llenergymeasure/domain/experiment.py (after Plan 01):
```python
class ExperimentResult(BaseModel):
    """Experiment result -- renamed from AggregatedResult."""
    schema_version: str
    experiment_id: str
    backend: str
    # ... many fields ...
    model_config = {"frozen": True}

class StudyResult(BaseModel):
    """M1 stub -- experiments + name only."""
    experiments: list[ExperimentResult] = Field(default_factory=list)
    name: str | None = Field(default=None)

AggregatedResult = ExperimentResult  # v1.x alias
```

From src/llenergymeasure/config/models.py (after Plan 01):
```python
class ExperimentConfig(BaseModel):
    model_config = {"extra": "forbid"}
    model: str = Field(...)
    backend: Literal["pytorch", "vllm", "tensorrt"] = Field(default="pytorch")
    n: int = Field(default=100)
    dataset: str | SyntheticDatasetConfig = Field(default="aienergyscore")
    precision: Literal["fp32", "fp16", "bf16"] = Field(default="bf16")
    output_dir: str | None = Field(default=None)
    # ... more fields ...

class StudyConfig(BaseModel):
    model_config = {"extra": "forbid"}
    experiments: list[ExperimentConfig] = Field(..., min_length=1)
    name: str | None = Field(default=None)
```

From src/llenergymeasure/config/loader.py:
```python
def load_experiment_config(
    path: Path | str | None = None,
    cli_overrides: dict[str, Any] | None = None,
    user_config_defaults: dict[str, Any] | None = None,
) -> ExperimentConfig:
```

From src/llenergymeasure/exceptions.py:
```python
class LLEMError(Exception): ...
class ConfigError(LLEMError): ...
class BackendError(LLEMError): ...
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create _api.py with run_experiment, run_study, and _run stub</name>
  <files>src/llenergymeasure/_api.py</files>
  <action>
Create `src/llenergymeasure/_api.py` -- a new file containing the public API function implementations.

**Full implementation specification:**

```python
"""Internal API implementation for llenergymeasure.

This module is internal (underscore prefix). Import via llenergymeasure.__init__ only.
"""

from __future__ import annotations

from pathlib import Path
from typing import overload

from llenergymeasure.config.loader import load_experiment_config
from llenergymeasure.config.models import ExperimentConfig, StudyConfig
from llenergymeasure.domain.experiment import ExperimentResult, StudyResult
from llenergymeasure.exceptions import ConfigError


# ---------------------------------------------------------------------------
# run_experiment — three overloaded forms
# ---------------------------------------------------------------------------

@overload
def run_experiment(config: str | Path) -> ExperimentResult: ...


@overload
def run_experiment(config: ExperimentConfig) -> ExperimentResult: ...


@overload
def run_experiment(
    config: None = None,
    *,
    model: str,
    backend: str | None = None,
    n: int = 100,
    dataset: str = "aienergyscore",
    **kwargs,
) -> ExperimentResult: ...


def run_experiment(
    config: str | Path | ExperimentConfig | None = None,
    *,
    model: str | None = None,
    backend: str | None = None,
    n: int = 100,
    dataset: str = "aienergyscore",
    **kwargs,
) -> ExperimentResult:
    """Run a single LLM inference efficiency experiment.

    Side-effect free: no disk writes unless output_dir is specified in the config.

    Three call forms:
        run_experiment("config.yaml")              # YAML path
        run_experiment(ExperimentConfig(...))       # config object
        run_experiment(model="gpt2", backend="Y")  # kwargs convenience

    Args:
        config: YAML file path, ExperimentConfig object, or None (use kwargs).
        model: Model name/path (kwargs form only).
        backend: Inference backend (kwargs form only, defaults to ExperimentConfig default).
        n: Number of prompts (kwargs form only, default 100).
        dataset: Dataset name (kwargs form only, default "aienergyscore").
        **kwargs: Additional ExperimentConfig fields (kwargs form only).

    Returns:
        ExperimentResult: Experiment measurements and metadata.

    Raises:
        ConfigError: Invalid config path, missing model in kwargs form.
        pydantic.ValidationError: Invalid field values (passes through unchanged).
    """
    study = _to_study_config(
        config, model=model, backend=backend, n=n, dataset=dataset, **kwargs
    )
    study_result = _run(study)
    return study_result.experiments[0]


# ---------------------------------------------------------------------------
# run_study — stub for M1 (surface completeness)
# ---------------------------------------------------------------------------


def run_study(config: str | Path | StudyConfig) -> StudyResult:
    """Run a study (multiple experiments). Available in M2.

    Exported from v2.0.0 for API surface stability. Implementation ships in M2.

    Raises:
        NotImplementedError: Study execution is not yet implemented.
    """
    raise NotImplementedError(
        "Study execution is available in M2. "
        "Use run_experiment() for single experiments."
    )


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------


def _to_study_config(
    config: str | Path | ExperimentConfig | None,
    *,
    model: str | None = None,
    backend: str | None = None,
    n: int = 100,
    dataset: str = "aienergyscore",
    **kwargs,
) -> StudyConfig:
    """Convert any run_experiment() input form to a degenerate StudyConfig."""
    if isinstance(config, ExperimentConfig):
        experiment = config
    elif isinstance(config, (str, Path)):
        experiment = load_experiment_config(path=Path(config))
    elif config is None:
        if model is None:
            raise ConfigError(
                "run_experiment() requires either a config argument or model= keyword.\n"
                "Example: run_experiment(model='meta-llama/Llama-3.1-8B')"
            )
        # Build kwargs dict for ExperimentConfig — only include non-default values
        # to let Pydantic defaults apply for omitted fields.
        ec_kwargs: dict = {"model": model, "n": n, "dataset": dataset}
        if backend is not None:
            ec_kwargs["backend"] = backend
        ec_kwargs.update(kwargs)
        experiment = ExperimentConfig(**ec_kwargs)
    else:
        raise ConfigError(
            f"Expected str, Path, ExperimentConfig, or None; got {type(config).__name__}"
        )
    return StudyConfig(experiments=[experiment])


def _run(study: StudyConfig) -> StudyResult:
    """Internal runner -- always receives StudyConfig, returns StudyResult.

    M1 stub: raises NotImplementedError. Phase 4 (PyTorch Backend) implements
    the real measurement engine.
    """
    raise NotImplementedError(
        "Core measurement engine not yet implemented (Phase 4). "
        "This stub exists to satisfy the type contract."
    )
```

**Key decisions embedded in the implementation:**

- **`_to_study_config` kwargs form:** When `backend=None`, it is NOT passed to `ExperimentConfig`, letting Pydantic's default (`"pytorch"`) apply. This avoids the need for a `_detect_default_backend()` function in M1.
- **`_run()` raises `NotImplementedError`:** Tests monkeypatch this. Phase 4 replaces the body.
- **Imports at top level:** `load_experiment_config` is imported at module top because `_api.py` is only imported when `__init__.py` is loaded, and config is a lightweight import.
- **`@overload` grouping:** All three stubs grouped together, immediately followed by the implementation function. No code between them.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure._api import run_experiment, run_study, _to_study_config, _run
from llenergymeasure.config.models import ExperimentConfig, StudyConfig

# Test _to_study_config with ExperimentConfig
ec = ExperimentConfig(model='gpt2')
sc = _to_study_config(ec)
assert isinstance(sc, StudyConfig)
assert sc.experiments[0].model == 'gpt2'

# Test _to_study_config with kwargs
sc2 = _to_study_config(None, model='gpt2', n=50)
assert sc2.experiments[0].model == 'gpt2'
assert sc2.experiments[0].n == 50

# Test run_study raises NotImplementedError
try:
    run_study(sc)
    assert False
except NotImplementedError as e:
    assert 'M2' in str(e)

# Test _run raises NotImplementedError
try:
    _run(sc)
    assert False
except NotImplementedError:
    pass

print('OK: _api.py functions work correctly')
"</automated>
  </verify>
  <done>
    - `_api.py` exists with `run_experiment()`, `run_study()`, `_to_study_config()`, `_run()`
    - Three `@overload` stubs for `run_experiment()` with correct type signatures
    - `_to_study_config()` handles all three input forms (ExperimentConfig, str/Path, kwargs)
    - `run_study()` raises `NotImplementedError` with M2 message
    - `_run()` raises `NotImplementedError` (Phase 4 stub)
    - No disk writes in any code path
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire __init__.py as stable public API surface and write tests</name>
  <files>
    src/llenergymeasure/__init__.py
    tests/unit/test_api.py
  </files>
  <action>
**Part A: Rewrite `src/llenergymeasure/__init__.py`**

Replace the entire file content with the public API surface:

```python
"""LLenergyMeasure -- LLM inference efficiency measurement framework.

Public API (stable from v2.0.0):
    run_experiment, run_study, ExperimentConfig, StudyConfig,
    ExperimentResult, StudyResult, __version__

Stability contract: exports in __all__ follow SemVer. Names not in __all__
are internal and may change without notice. One minor version deprecation
window before removing any __all__ export (removed in v2.x+1 at earliest).
"""

from llenergymeasure._api import run_experiment, run_study
from llenergymeasure.config.models import ExperimentConfig, StudyConfig
from llenergymeasure.domain.experiment import ExperimentResult, StudyResult

__version__: str = "2.0.0"

__all__ = [
    "run_experiment",
    "run_study",
    "ExperimentConfig",
    "StudyConfig",
    "ExperimentResult",
    "StudyResult",
    "__version__",
]
```

This is the **complete file** -- nothing else belongs in `__init__.py`. Internal names like `load_experiment_config`, `ConfigError`, `AggregatedResult` are NOT imported here. They remain accessible via their internal module paths but are not part of the stable public surface.

**Critical:** `__version__` must be in `__all__` (LA-10). The stability contract docstring satisfies LA-09.

**Part B: Create `tests/unit/test_api.py`**

Create a comprehensive test file covering all Phase 3 success criteria. The tests must work **without GPU hardware** by monkeypatching `_run()`.

Test cases to implement:

1. **`test_public_imports_resolve`** -- Import all 7 names from `llenergymeasure`. Assert each is not None and `__version__ == "2.0.0"`.

2. **`test_internal_name_raises_attribute_error`** -- Assert that `llenergymeasure.load_experiment_config`, `llenergymeasure.ConfigError`, `llenergymeasure.AggregatedResult` all raise `AttributeError`. Use `getattr(module, name)` pattern with `pytest.raises(AttributeError)`.

3. **`test_run_experiment_returns_experiment_result`** -- Monkeypatch `llenergymeasure._api._run` to return a `StudyResult` containing a mock `ExperimentResult`. Call `run_experiment(ExperimentConfig(model="gpt2"))`. Assert the return type is `ExperimentResult` (not union, not None). The mock `ExperimentResult` needs to satisfy the required fields of the v1.x `AggregatedResult` model (it has many required fields). Use a helper function to build a minimal valid `ExperimentResult`.

4. **`test_run_experiment_yaml_path_form`** -- Create a temp YAML file with `model: gpt2`, call `run_experiment("path/to/file.yaml")` with `_run` monkeypatched. Assert it resolves.

5. **`test_run_experiment_kwargs_form`** -- Call `run_experiment(model="gpt2", n=50)` with `_run` monkeypatched. Assert the experiment config in the study has `model="gpt2"` and `n=50`.

6. **`test_run_experiment_no_config_no_model_raises`** -- Call `run_experiment()` with no arguments. Assert it raises `ConfigError` (not `TypeError`, not `ValidationError`).

7. **`test_run_experiment_no_disk_writes`** -- Monkeypatch `_run`. Call `run_experiment(ExperimentConfig(model="gpt2"))`. Assert `tmp_path` directory is empty (no files written).

8. **`test_run_study_raises_not_implemented`** -- Call `run_study(StudyConfig(experiments=[ExperimentConfig(model="gpt2")]))`. Assert `NotImplementedError` with "M2" in the message.

9. **`test_all_list_matches_exports`** -- Import `llenergymeasure`, check that every name in `__all__` is actually importable from the module.

**Helper for mock ExperimentResult:** The `ExperimentResult` (formerly `AggregatedResult`) has many required fields (`experiment_id`, `aggregation`, `total_tokens`, etc.). Create a `_make_experiment_result()` helper at the top of the test file that builds a minimal valid instance:

```python
from datetime import datetime
from llenergymeasure.domain.experiment import (
    AggregationMetadata,
    ExperimentResult,
    StudyResult,
    Timestamps,
)
from llenergymeasure.domain.metrics import (
    ComputeMetrics,
    EnergyMetrics,
    InferenceMetrics,
)


def _make_experiment_result(**overrides):
    """Build a minimal valid ExperimentResult for testing."""
    defaults = {
        "experiment_id": "test-001",
        "aggregation": AggregationMetadata(num_processes=1),
        "total_tokens": 1000,
        "total_energy_j": 10.0,
        "total_inference_time_sec": 5.0,
        "avg_tokens_per_second": 200.0,
        "avg_energy_per_token_j": 0.01,
        "total_flops": 1e9,
        "start_time": datetime(2026, 1, 1),
        "end_time": datetime(2026, 1, 1, 0, 1),
    }
    defaults.update(overrides)
    return ExperimentResult(**defaults)
```

Use this helper in every test that needs an ExperimentResult. The monkeypatch for `_run` should return `StudyResult(experiments=[_make_experiment_result()])`.

For the YAML path test (test 4), use `tmp_path` to create a temporary YAML file:
```python
config_path = tmp_path / "test_config.yaml"
config_path.write_text("model: gpt2\n")
```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_api.py -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>
    - `__init__.py` exports exactly 7 names in `__all__`
    - Stability contract docstring present in `__init__.py`
    - `from llenergymeasure import run_experiment, ExperimentConfig, ExperimentResult` resolves
    - `llenergymeasure.__version__ == "2.0.0"` and `"__version__"` is in `__all__`
    - Internal names raise `AttributeError` when accessed on the module
    - All 9+ test cases pass in `test_api.py`
    - No test requires GPU hardware
  </done>
</task>

</tasks>

<verification>
Run the full verification sequence:

```bash
cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool

# 1. Public API surface (Phase 3 success criterion #1)
python -c "
from llenergymeasure import run_experiment, ExperimentConfig, ExperimentResult
print('SC1: All public imports resolve')
"

# 2. No union return types -- verified via test (Phase 3 SC #2)
# Covered by test_run_experiment_returns_experiment_result

# 3. No disk writes -- verified via test (Phase 3 SC #3)
# Covered by test_run_experiment_no_disk_writes

# 4. Internal names raise AttributeError (Phase 3 SC #4)
python -c "
import llenergymeasure
try:
    _ = llenergymeasure.load_experiment_config
    print('FAIL: internal name accessible')
    exit(1)
except AttributeError:
    print('SC4: Internal names correctly inaccessible')
"

# 5. Version correct (Phase 3 SC #5)
python -c "
import llenergymeasure
assert llenergymeasure.__version__ == '2.0.0'
print('SC5: Version correct')
"

# 6. Full test suite
python -m pytest tests/unit/test_api.py -x -v

# 7. Existing tests still pass
python -m pytest tests/unit/test_domain_experiment.py tests/unit/test_config_models.py tests/unit/test_config_loader.py -x -q 2>&1 | tail -5
```
</verification>

<success_criteria>
1. `from llenergymeasure import run_experiment, ExperimentConfig, ExperimentResult` all resolve without error
2. `run_experiment(config)` returns exactly `ExperimentResult` -- no union types, no `None`
3. `run_experiment()` with no `output_dir` produces no disk writes (side-effect-free)
4. Any name not in `__init__.py.__all__` raises `AttributeError` on direct import
5. `llenergymeasure.__version__ == "2.0.0"`
6. `run_study()` raises `NotImplementedError` with clear M2 message
7. All tests in `test_api.py` pass
8. All existing tests continue to pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/03-library-api/03-02-SUMMARY.md`
</output>
