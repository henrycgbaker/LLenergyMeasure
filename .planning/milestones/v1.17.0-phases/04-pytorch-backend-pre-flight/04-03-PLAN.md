---
phase: 04-pytorch-backend-pre-flight
plan: 03
type: execute
wave: 3
depends_on: [04-01, 04-02]
files_modified:
  - src/llenergymeasure/_api.py
  - src/llenergymeasure/core/backends/pytorch.py
  - src/llenergymeasure/domain/experiment.py
  - tests/unit/test_api.py
autonomous: true
requirements: [CM-34]

must_haves:
  truths:
    - "_run() implements the real measurement pipeline: preflight → backend.run() → StudyResult"
    - "run_experiment() with a valid config flows through _run() to produce an ExperimentResult"
    - "Thermal throttle detection is carried forward from v1.x and integrated"
    - "Pre-flight runs once per experiment config in the study loop"
  artifacts:
    - path: "src/llenergymeasure/_api.py"
      provides: "Real _run() implementation replacing NotImplementedError stub"
      contains: "run_preflight"
    - path: "tests/unit/test_api.py"
      provides: "Updated API tests covering _run() wiring"
  key_links:
    - from: "src/llenergymeasure/_api.py"
      to: "src/llenergymeasure/orchestration/preflight.py"
      via: "calls run_preflight(config)"
      pattern: "run_preflight"
    - from: "src/llenergymeasure/_api.py"
      to: "src/llenergymeasure/core/backends/__init__.py"
      via: "calls get_backend(config.backend)"
      pattern: "get_backend"
---

<objective>
Wire the measurement pipeline together: replace the `_run()` NotImplementedError stub with the real implementation that calls preflight → backend.run() → StudyResult. Carry forward thermal throttle detection from v1.x.

Purpose: This is the final wiring plan that makes `run_experiment()` functional end-to-end. After this plan, calling `run_experiment(ExperimentConfig(model="gpt2"))` on a GPU machine produces a real ExperimentResult.

Output: Working `_run()` in `_api.py`, thermal throttle integration in the PyTorch backend, updated API tests.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/phases/04-pytorch-backend-pre-flight/04-CONTEXT.md
@.planning/phases/04-pytorch-backend-pre-flight/04-RESEARCH.md
@.planning/phases/04-pytorch-backend-pre-flight/04-01-SUMMARY.md
@.planning/phases/04-pytorch-backend-pre-flight/04-02-SUMMARY.md

<interfaces>
<!-- Key contracts from Plan 01 and Plan 02. -->

From src/llenergymeasure/orchestration/preflight.py (Plan 01):
```python
def run_preflight(config: ExperimentConfig) -> None:
    """Raises PreFlightError listing ALL failures, or returns None."""
```

From src/llenergymeasure/core/backends/__init__.py (Plan 02):
```python
def get_backend(name: str) -> InferenceBackend: ...
def detect_default_backend() -> str: ...
```

From src/llenergymeasure/core/backends/pytorch.py (Plan 02):
```python
class PyTorchBackend:
    name = "pytorch"
    def run(self, config: ExperimentConfig) -> ExperimentResult: ...
```

From src/llenergymeasure/_api.py (current):
```python
def _run(study: StudyConfig) -> StudyResult:
    """Currently raises NotImplementedError."""
    raise NotImplementedError(...)

def run_experiment(...) -> ExperimentResult:
    study = _to_study_config(...)
    study_result = _run(study)
    return study_result.experiments[0]
```

From src/llenergymeasure/core/power_thermal.py (v1.x carry-forward):
```python
class PowerThermalSampler:
    """Background sampler using pynvml. Context manager pattern."""
    def __init__(self, device_index=0, sample_interval_ms=100): ...
    def __enter__(self) -> PowerThermalSampler: ...
    def __exit__(self, *args) -> None: ...
    def get_thermal_throttle_info(self) -> ThermalThrottleInfo: ...
```

From src/llenergymeasure/domain/metrics.py:
```python
class ThermalThrottleInfo(BaseModel):
    detected: bool
    max_temperature_c: float | None
    throttle_count: int
    total_throttle_duration_sec: float | None
    throttle_reasons: list[str]
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace _run() stub and integrate thermal throttle detection</name>
  <files>
    src/llenergymeasure/_api.py
    src/llenergymeasure/core/backends/pytorch.py
    src/llenergymeasure/domain/experiment.py
  </files>
  <action>
**1. Replace `_run()` in `src/llenergymeasure/_api.py`:**

Remove the `NotImplementedError` stub and implement the real pipeline:

```python
def _run(study: StudyConfig) -> StudyResult:
    """Internal runner -- always receives StudyConfig, returns StudyResult."""
    from llenergymeasure.core.backends import get_backend
    from llenergymeasure.orchestration.preflight import run_preflight

    results = []
    for config in study.experiments:
        # Pre-flight runs once per experiment config (per CONTEXT.md: "always runs before every experiment")
        run_preflight(config)

        # Get and run the backend
        backend = get_backend(config.backend)
        result = backend.run(config)
        results.append(result)

    return StudyResult(experiments=results, name=study.name)
```

Key points:
- Pre-flight runs per experiment config (each config may differ — different model, different backend).
- `get_backend()` returns the appropriate backend instance.
- `backend.run(config)` does the full lifecycle (snapshot, load, warmup, measure, cleanup).
- All imports are deferred (inside function body) to keep `_api.py` lightweight at import time.
- No try/except wrapping — `PreFlightError`, `BackendError` propagate naturally. Phase 7 (CLI) adds error display.

**2. Verify `ExperimentResult.thermal_throttle` field exists in `src/llenergymeasure/domain/experiment.py`:**

`ExperimentResult` already has `thermal_throttle: ThermalThrottleInfo | None = Field(default=None)` (line ~234) and imports `ThermalThrottleInfo` from `llenergymeasure.domain.metrics`. **No code change needed** — just confirm the field exists before wiring it in the backend. If for any reason it's missing, add it:

```python
# In ExperimentResult class body:
thermal_throttle: ThermalThrottleInfo | None = Field(
    default=None,
    description="GPU thermal and power throttling information",
)
```

**3. Integrate thermal throttle detection into PyTorchBackend (`src/llenergymeasure/core/backends/pytorch.py`):**

The v1.x `PowerThermalSampler` in `core/power_thermal.py` already implements thermal throttle detection via pynvml (CM-34). It uses loguru, but since we're importing it (not rewriting it), that's fine for now.

Modify the `_run_measurement()` method to wrap inference with `PowerThermalSampler`:

```python
def _run_measurement(self, model, tokenizer, config, prompts):
    from llenergymeasure.core.power_thermal import PowerThermalSampler

    with PowerThermalSampler(device_index=0) as sampler:
        # ... existing measurement loop ...
        pass

    thermal_info = sampler.get_thermal_throttle_info()
    # Include thermal_info in the measurement result data
```

Then in `_build_result()`, attach `thermal_throttle=thermal_info` to the `ExperimentResult`.

The `PowerThermalSampler` gracefully handles pynvml unavailability (returns default `ThermalThrottleInfo` with `detected=False`), so no additional error handling needed.

**IMPORTANT constraints:**
- Keep all imports deferred in `_api.py` — no new module-level imports.
- `PowerThermalSampler` is a v1.x carry-forward with loguru — don't rewrite it in this plan. Future cleanup can migrate it to stdlib logging.
- The measurement loop itself is a placeholder in M1 (no energy measurement yet). Phase 5 adds energy backends. The key structural wiring must be correct.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure._api import _run
from llenergymeasure.config.models import StudyConfig, ExperimentConfig
# Verify _run no longer raises NotImplementedError
import inspect
source = inspect.getsource(_run)
assert 'NotImplementedError' not in source, '_run still has NotImplementedError stub'
assert 'run_preflight' in source, '_run does not call run_preflight'
assert 'get_backend' in source, '_run does not call get_backend'
print('_run() wiring verified')
"</automated>
  </verify>
  <done>
    - `_run()` calls `run_preflight(config)` before each experiment
    - `_run()` calls `get_backend(config.backend).run(config)` for each experiment
    - `_run()` returns `StudyResult(experiments=results, name=study.name)`
    - `ExperimentResult` in `domain/experiment.py` has `thermal_throttle: ThermalThrottleInfo | None` field (verified or added)
    - PyTorch backend wraps measurement with `PowerThermalSampler` for thermal throttle detection
    - `ExperimentResult.thermal_throttle` is populated from the sampler
    - No `NotImplementedError` remains in `_run()`
  </done>
</task>

<task type="auto">
  <name>Task 2: Update API tests for real _run() pipeline</name>
  <files>
    tests/unit/test_api.py
  </files>
  <action>
Update the existing `tests/unit/test_api.py` to test the real `_run()` wiring. The existing tests monkeypatch `_run` — some of those tests should still work (they test `run_experiment()` input validation which doesn't touch `_run`). Add new tests for the wiring.

**Tests to ADD (keep existing passing tests):**

```python
# Test: _run calls run_preflight for each config
def test_run_calls_preflight(monkeypatch):
    # monkeypatch run_preflight to track calls
    # monkeypatch get_backend to return a mock backend
    # Call _run with a StudyConfig containing 2 experiments
    # Assert run_preflight was called twice (once per config)
    ...

# Test: _run calls get_backend with correct backend name
def test_run_calls_get_backend(monkeypatch):
    # monkeypatch run_preflight to no-op
    # monkeypatch get_backend to track calls and return mock backend
    # Call _run with backend="pytorch"
    # Assert get_backend("pytorch") was called
    ...

# Test: _run returns StudyResult with experiment results
def test_run_returns_study_result(monkeypatch):
    # monkeypatch run_preflight to no-op
    # monkeypatch get_backend to return mock backend that returns a mock ExperimentResult
    # Call _run
    # Assert returned StudyResult has correct experiments list and name
    ...

# Test: _run propagates PreFlightError
def test_run_propagates_preflight_error(monkeypatch):
    # monkeypatch run_preflight to raise PreFlightError
    # Assert _run raises PreFlightError (not caught)
    ...

# Test: _run propagates BackendError
def test_run_propagates_backend_error(monkeypatch):
    # monkeypatch run_preflight to no-op
    # monkeypatch get_backend to return mock backend that raises BackendError
    # Assert _run raises BackendError (not caught)
    ...

# Test: run_experiment end-to-end with mocked backend
def test_run_experiment_end_to_end(monkeypatch):
    # monkeypatch run_preflight to no-op
    # monkeypatch get_backend to return mock backend with valid ExperimentResult
    # Call run_experiment(model="gpt2")
    # Assert returns ExperimentResult (not StudyResult)
    ...
```

**Update existing tests:**
- Tests that monkeypatch `_run` directly should still work since `_run` is still in `_api.py`.
- Tests that expect `NotImplementedError` from `_run()` should be updated or removed — `_run()` no longer raises that.
- Check the existing `test_api.py` for any tests asserting `NotImplementedError` and update them.

Use `monkeypatch` to mock `run_preflight` and `get_backend` inside `_api._run`. This avoids needing a GPU.

For mock `ExperimentResult`, create a minimal valid instance:
```python
from datetime import datetime
from llenergymeasure.domain.experiment import ExperimentResult, AggregationMetadata

def _mock_result():
    return ExperimentResult(
        experiment_id="test_123",
        backend="pytorch",
        total_tokens=100,
        total_energy_j=0.0,
        total_inference_time_sec=1.0,
        avg_tokens_per_second=100.0,
        avg_energy_per_token_j=0.0,
        total_flops=0.0,
        aggregation=AggregationMetadata(num_processes=1),
        start_time=datetime.now(),
        end_time=datetime.now(),
    )
```
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_api.py -x -v 2>&1 | tail -30</automated>
  </verify>
  <done>
    - All existing API tests that test input validation still pass
    - New tests verify _run() calls run_preflight per config
    - New tests verify _run() calls get_backend with correct name
    - New tests verify _run() returns StudyResult
    - New tests verify PreFlightError and BackendError propagate
    - End-to-end test verifies run_experiment() returns ExperimentResult through the real pipeline (mocked backend)
    - No test requires a GPU
  </done>
</task>

</tasks>

<verification>
- `python -c "from llenergymeasure._api import _run; print('_run imported')"` succeeds
- `python -c "import inspect; from llenergymeasure._api import _run; assert 'NotImplementedError' not in inspect.getsource(_run)"` passes
- `pytest tests/unit/test_api.py -x` all pass
- `pytest tests/unit/test_preflight.py tests/unit/test_environment_snapshot.py tests/unit/test_backend_protocol.py tests/unit/test_api.py -x` all pass (full Phase 4 test suite)
</verification>

<success_criteria>
1. `_run()` no longer raises `NotImplementedError` — it calls `run_preflight()` then `get_backend().run()`
2. Pre-flight runs once per experiment config in the study loop
3. `PowerThermalSampler` is integrated into the PyTorch backend measurement path (CM-34)
4. `ExperimentResult.thermal_throttle` is populated from the sampler output
5. `run_experiment(ExperimentConfig(model="gpt2"))` flows through the real pipeline (with mocked backend, produces ExperimentResult)
6. All API tests pass (existing input validation + new wiring tests)
7. Errors propagate naturally — no swallowing of PreFlightError or BackendError
</success_criteria>

<output>
After completion, create `.planning/phases/04-pytorch-backend-pre-flight/04-03-SUMMARY.md`
</output>
