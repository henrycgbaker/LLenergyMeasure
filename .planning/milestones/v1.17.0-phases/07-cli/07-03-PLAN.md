---
phase: 07-cli
plan: 03
type: execute
wave: 3
depends_on: ["07-02"]
files_modified:
  - src/llenergymeasure/cli/config_cmd.py
  - src/llenergymeasure/cli/__init__.py
  - tests/unit/test_cli_config.py
autonomous: true
requirements: [CLI-01, CLI-06]

must_haves:
  truths:
    - "`llem config` prints GPU info, installed backends, energy backends, and user config path"
    - "`llem config --verbose` adds per-backend version detail and full user config dump"
    - "Missing GPU produces a clear 'No GPU detected' message, not a crash"
    - "Missing backends listed as 'not installed' with install hint"
  artifacts:
    - path: "src/llenergymeasure/cli/config_cmd.py"
      provides: "llem config command implementation"
      min_lines: 60
    - path: "src/llenergymeasure/cli/__init__.py"
      provides: "Updated CLI app with config command registered"
      contains: "config_cmd"
    - path: "tests/unit/test_cli_config.py"
      provides: "Unit tests for config command"
      min_lines: 30
  key_links:
    - from: "src/llenergymeasure/cli/config_cmd.py"
      to: "pynvml"
      via: "_probe_gpu() calls nvmlInit/nvmlDeviceGetCount"
      pattern: "nvmlInit"
    - from: "src/llenergymeasure/cli/config_cmd.py"
      to: "importlib.util.find_spec"
      via: "_probe_backend() checks package availability"
      pattern: "find_spec"
    - from: "src/llenergymeasure/cli/config_cmd.py"
      to: "llenergymeasure.config.user_config"
      via: "Loads and displays user config"
      pattern: "load_user_config"
    - from: "src/llenergymeasure/cli/__init__.py"
      to: "src/llenergymeasure/cli/config_cmd.py"
      via: "Registers config command on Typer app"
      pattern: "config_cmd"
---

<objective>
Implement the `llem config` command — the environment display and setup guidance command. Shows GPU info, installed backends, energy backend status, and user config path.

Purpose: Gives researchers a quick way to verify their environment is correctly set up before running experiments. `llem config` is the diagnostic entry point.

Output: `cli/config_cmd.py` (~80 LOC), updated `cli/__init__.py`, unit tests
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cli/07-CONTEXT.md
@.planning/phases/07-cli/07-RESEARCH.md
@.planning/phases/07-cli/07-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/llenergymeasure/config/user_config.py:
```python
def load_user_config(config_path: Path | None = None) -> UserConfig: ...
def get_user_config_path() -> Path: ...

class UserConfig(BaseModel):
    output: UserOutputConfig
    runners: UserRunnersConfig
    measurement: UserMeasurementConfig
    ui: UserUIConfig
    advanced: UserAdvancedConfig
    execution: UserExecutionConfig
```

From src/llenergymeasure/orchestration/preflight.py:
```python
_BACKEND_PACKAGES: dict[str, str] = {
    "pytorch": "transformers",
    "vllm": "vllm",
    "tensorrt": "tensorrt_llm",
}
```

From src/llenergymeasure/cli/_display.py (built in Plan 01):
```python
def format_error(error: LLEMError, verbose: bool = False) -> str: ...
```

From src/llenergymeasure/cli/__init__.py:
```python
app = typer.Typer(name="llem", ...)
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement cli/config_cmd.py — llem config command</name>
  <files>src/llenergymeasure/cli/config_cmd.py, src/llenergymeasure/cli/__init__.py</files>
  <action>
**Create `src/llenergymeasure/cli/config_cmd.py`:**

1. **`config_command` function** — the Typer command:
   ```python
   def config_command(
       verbose: Annotated[bool, typer.Option("--verbose", "-v", help="Show detailed backend versions and full config")] = False,
   ) -> None:
   ```

2. **GPU section**: Call `_probe_gpu()` (internal helper). Print:
   - "GPU" header
   - If GPUs found: for each GPU, print `  {name}  {vram:.1f} GB`. If multiple, print count.
   - If no GPU: `  No GPU detected`
   - If verbose: also print driver version (from pynvml `nvmlSystemGetDriverVersion()`)

3. **Backends section**: For each backend in `["pytorch", "vllm", "tensorrt"]`:
   - Use `importlib.util.find_spec(package)` to check if installed (package mapping: pytorch -> transformers, vllm -> vllm, tensorrt -> tensorrt_llm).
   - Print `  {backend}: installed` or `  {backend}: not installed  (pip install llenergymeasure[{backend}])`
   - If verbose and installed: try to import and print version. For pytorch: `import torch; torch.__version__`. For vllm: `import vllm; vllm.__version__`. For tensorrt: `import tensorrt_llm; tensorrt_llm.__version__`. Wrap each in try/except.

4. **Energy backends section**: Check availability of energy measurement backends:
   - NVML: `importlib.util.find_spec("pynvml") is not None`
   - Zeus: `importlib.util.find_spec("zeus") is not None`
   - CodeCarbon: `importlib.util.find_spec("codecarbon") is not None`
   - Print `  Energy: {selected}` where selected is the highest-priority available (Zeus > NVML > CodeCarbon). If none: "none (install nvidia-ml-py for NVML)"

5. **User config section**:
   - Import `get_user_config_path` (deferred) and print the path.
   - If the file exists, print `  Status: loaded`. If not, print `  Status: using defaults (no config file)`.
   - If verbose: load user config with `load_user_config()` and print a dump of non-default values. Use `model_dump()` and filter out default values by comparing against `UserConfig()` defaults.

6. **Python section**: Print Python version (`sys.version.split()[0]`).

7. All output goes to stdout (this is informational, not progress). No Rich imports.

**`_probe_gpu()` helper** (internal to this module):
```python
def _probe_gpu() -> list[dict[str, Any]] | None:
    """Return list of GPU info dicts or None if unavailable."""
    try:
        import pynvml  # noqa: PLC0415
        pynvml.nvmlInit()
        count = pynvml.nvmlDeviceGetCount()
        gpus = []
        for i in range(count):
            h = pynvml.nvmlDeviceGetHandleByIndex(i)
            name = pynvml.nvmlDeviceGetName(h)
            if isinstance(name, bytes):
                name = name.decode()
            mem = pynvml.nvmlDeviceGetMemoryInfo(h)
            gpus.append({"name": name, "vram_gb": mem.total / 1e9})
        pynvml.nvmlShutdown()
        return gpus if gpus else None
    except Exception:
        return None
```

**Update `src/llenergymeasure/cli/__init__.py` (APPEND ONLY — Plan 02 wrote this file):**

Plan 02 already created `__init__.py` with the `run` command registered. Do NOT rewrite the file. APPEND the config command registration after the existing `run` registration line. Add these two lines immediately after the `app.command(name="run", ...)` line:

```python
from llenergymeasure.cli.config_cmd import config_command as _config_cmd
app.command(name="config", help="Show environment and configuration status")(_config_cmd)
```

IMPORTANT: Preserve ALL existing content (version callback, main callback, run registration, `__all__`, `if __name__`). Only add the two config lines. The `__init__.py` must NOT import from v1.x modules (`cli/config.py`, `cli/experiment.py`, `cli/display/`). Only import from `run.py` and `config_cmd.py`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.cli.config_cmd import config_command; print('config command loaded')" && python -m llenergymeasure.cli --help 2>&1 | head -20</automated>
  </verify>
  <done>config_cmd.py exists and imports cleanly, __init__.py registers both run and config commands</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for llem config command</name>
  <files>tests/unit/test_cli_config.py</files>
  <action>
Create `tests/unit/test_cli_config.py` with tests using `typer.testing.CliRunner`.

Tests:

1. **`test_config_help`**: `runner.invoke(app, ["config", "--help"])` — exits 0, contains "--verbose".

2. **`test_config_basic_output`**: Mock `_probe_gpu` to return `[{"name": "NVIDIA A100", "vram_gb": 80.0}]`. Mock `importlib.util.find_spec` to return a truthy value for "transformers" and None for "vllm" and "tensorrt_llm". Invoke `runner.invoke(app, ["config"])`. Assert exit code 0. Assert output contains "GPU", "A100", "pytorch: installed", "vllm: not installed".

3. **`test_config_no_gpu`**: Mock `_probe_gpu` to return `None`. Invoke `runner.invoke(app, ["config"])`. Assert output contains "No GPU detected".

4. **`test_config_verbose_shows_python_version`**: Invoke `runner.invoke(app, ["config", "--verbose"])`. Assert output contains "Python" and a version string.

5. **`test_config_user_config_path_shown`**: Mock `get_user_config_path` to return a known path. Invoke `runner.invoke(app, ["config"])`. Assert output contains the config path string.

6. **`test_config_exits_0`**: Basic invocation exits 0 (config is always informational — never errors).

Use `from unittest.mock import patch, MagicMock`. Use `from typer.testing import CliRunner`. Import `app` from `llenergymeasure.cli`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_cli_config.py -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>All config command tests pass, environment display verified</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.cli import app"` succeeds
2. `llem config` shows GPU info, backends, energy, user config path
3. `llem config --verbose` shows additional detail (versions, config dump)
4. `python -m pytest tests/unit/test_cli_config.py -x -v` — all tests pass
5. `grep -r "import rich" src/llenergymeasure/cli/config_cmd.py` returns nothing
6. `llem --help` shows both "run" and "config" commands listed
</verification>

<success_criteria>
- `llem config` displays environment state without errors (even without GPU)
- `llem config --verbose` adds per-backend version and full user config
- No Rich dependency in new code
- Config command always exits 0 (informational)
- Both `run` and `config` commands appear in `llem --help`
</success_criteria>

<output>
After completion, create `.planning/phases/07-cli/07-03-SUMMARY.md`
</output>
