---
phase: 07-cli
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/llenergymeasure/cli/run.py
  - src/llenergymeasure/cli/__init__.py
  - tests/unit/test_cli_run.py
autonomous: true
requirements: [CLI-01, CLI-02, CLI-03, CLI-04, CLI-07, CLI-08, CLI-10, CLI-12, CLI-13]

must_haves:
  truths:
    - "`llem run --model gpt2 --backend pytorch` invokes run_experiment() and prints result summary"
    - "`llem run experiment.yaml` loads YAML, validates, runs, and writes output"
    - "`llem run --dry-run experiment.yaml` validates config and shows VRAM estimate, exits 0"
    - "`llem run --dry-run --verbose` adds source annotations to config echo"
    - "ConfigError exits with code 2; PreFlightError/ExperimentError exits with code 1"
    - "SIGINT exits with code 130 via SystemExit(130)"
    - "`--quiet` suppresses tqdm progress spinner but keeps final summary"
    - "tqdm indeterminate spinner wraps run_experiment() call, disabled when quiet or non-TTY"
    - "`--verbose` shows traceback on errors"
    - "Pydantic ValidationError formatted with friendly header, exits code 2"
  artifacts:
    - path: "src/llenergymeasure/cli/run.py"
      provides: "llem run command implementation"
      min_lines: 100
    - path: "src/llenergymeasure/cli/__init__.py"
      provides: "Updated CLI app with run command registered"
      contains: "from llenergymeasure.cli.run import"
    - path: "tests/unit/test_cli_run.py"
      provides: "Unit tests for run command error handling and flag parsing"
      min_lines: 50
  key_links:
    - from: "src/llenergymeasure/cli/run.py"
      to: "llenergymeasure._api.run_experiment"
      via: "Calls run_experiment() for actual execution"
      pattern: "run_experiment"
    - from: "src/llenergymeasure/cli/run.py"
      to: "llenergymeasure.config.loader.load_experiment_config"
      via: "Loads YAML config with CLI overrides"
      pattern: "load_experiment_config"
    - from: "src/llenergymeasure/cli/run.py"
      to: "llenergymeasure.cli._display"
      via: "Uses display utilities for output formatting"
      pattern: "from llenergymeasure.cli._display import"
    - from: "src/llenergymeasure/cli/run.py"
      to: "tqdm.auto"
      via: "Indeterminate spinner wraps run_experiment() call"
      pattern: "from tqdm.auto import tqdm"
    - from: "src/llenergymeasure/cli/__init__.py"
      to: "src/llenergymeasure/cli/run.py"
      via: "Registers run command on Typer app"
      pattern: "app.command"
---

<objective>
Implement the `llem run` command — the primary user-facing command for running experiments. This wires CLI flags to `run_experiment()`, handles all error paths with correct exit codes, supports `--dry-run` for config validation and VRAM estimation, and displays results via the _display utilities.

Purpose: This is the main CLI entry point for running LLM efficiency measurements. All flags, error handling, SIGINT, progress display, and dry-run are delivered here.

Output: `cli/run.py` (~150 LOC), updated `cli/__init__.py`, unit tests
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cli/07-CONTEXT.md
@.planning/phases/07-cli/07-RESEARCH.md
@.planning/phases/07-cli/07-01-SUMMARY.md

<interfaces>
<!-- Key types and contracts the executor needs. -->

From src/llenergymeasure/cli/_display.py (built in Plan 01):
```python
def print_result_summary(result: ExperimentResult) -> None: ...
def print_dry_run(config: ExperimentConfig, vram: dict | None, gpu_vram_gb: float | None, verbose: bool = False) -> None: ...
def format_error(error: LLEMError, verbose: bool = False) -> str: ...
def format_validation_error(e: ValidationError) -> str: ...
def print_experiment_header(config: ExperimentConfig) -> None: ...
```

From src/llenergymeasure/cli/_vram.py (built in Plan 01):
```python
def estimate_vram(config: ExperimentConfig) -> dict[str, float] | None: ...
def get_gpu_vram_gb() -> float | None: ...
```

From src/llenergymeasure/_api.py:
```python
def run_experiment(config: str | Path | ExperimentConfig | None = None, *, model: str | None = None, backend: str | None = None, n: int = 100, dataset: str = "aienergyscore", **kwargs) -> ExperimentResult: ...
```

From src/llenergymeasure/config/loader.py:
```python
def load_experiment_config(path: Path | str | None = None, cli_overrides: dict[str, Any] | None = None, user_config_defaults: dict[str, Any] | None = None) -> ExperimentConfig: ...
```

From src/llenergymeasure/config/user_config.py:
```python
def load_user_config(config_path: Path | None = None) -> UserConfig: ...
```

From src/llenergymeasure/exceptions.py:
```python
class LLEMError(Exception): ...
class ConfigError(LLEMError): ...
class BackendError(LLEMError): ...
class PreFlightError(LLEMError): ...
class ExperimentError(LLEMError): ...
class StudyError(LLEMError): ...
```

From src/llenergymeasure/cli/__init__.py:
```python
app = typer.Typer(name="llem", ...)
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement cli/run.py — llem run command</name>
  <files>src/llenergymeasure/cli/run.py, src/llenergymeasure/cli/__init__.py</files>
  <action>
**Create `src/llenergymeasure/cli/run.py`:**

Structure the file as follows:

1. **Imports** (all deferred except typing/typer):
   ```python
   from __future__ import annotations
   import signal
   import sys
   from pathlib import Path
   from typing import Annotated, Any
   import typer
   ```

2. **`run` function** — the Typer command (decorated with nothing here; registered from `__init__.py`):

   ```python
   def run(
       config: Annotated[Path | None, typer.Argument(help="Path to experiment YAML config")] = None,
       model: Annotated[str | None, typer.Option("--model", "-m", help="Model name or HF path")] = None,
       backend: Annotated[str | None, typer.Option("--backend", "-b", help="Inference backend")] = None,
       dataset: Annotated[str | None, typer.Option("--dataset", "-d", help="Dataset name")] = None,
       n: Annotated[int | None, typer.Option("-n", help="Number of prompts")] = None,
       batch_size: Annotated[int | None, typer.Option("--batch-size", help="Batch size (PyTorch)")] = None,
       precision: Annotated[str | None, typer.Option("--precision", "-p", help="Floating point precision")] = None,
       output: Annotated[str | None, typer.Option("--output", "-o", help="Output directory")] = None,
       dry_run: Annotated[bool, typer.Option("--dry-run", help="Validate config and estimate VRAM without running")] = False,
       quiet: Annotated[bool, typer.Option("--quiet", "-q", help="Suppress progress bars")] = False,
       verbose: Annotated[bool, typer.Option("--verbose", "-v", help="Show detailed output and tracebacks")] = False,
   ) -> None:
   ```

3. **SIGINT handler**: Install at start of function body:
   ```python
   def _handle_sigint(signum, frame):
       print("\nInterrupted.", file=sys.stderr)
       raise SystemExit(130)
   signal.signal(signal.SIGINT, _handle_sigint)
   ```

4. **Error dispatch try/except**: Wrap entire body in:
   ```python
   try:
       _run_impl(config, model, backend, dataset, n, batch_size, precision, output, dry_run, quiet, verbose)
   except ConfigError as e:
       print(format_error(e, verbose=verbose), file=sys.stderr)
       raise typer.Exit(code=2)
   except (PreFlightError, ExperimentError, BackendError) as e:
       print(format_error(e, verbose=verbose), file=sys.stderr)
       raise typer.Exit(code=1)
   except ValidationError as e:
       print(format_validation_error(e), file=sys.stderr)
       raise typer.Exit(code=2)
   except KeyboardInterrupt:
       print("\nInterrupted.", file=sys.stderr)
       raise SystemExit(130)
   ```
   CRITICAL ordering: `ConfigError` before `ValidationError` (ConfigError is not a subclass of ValidationError). Import `ValidationError` from `pydantic` inside the function body (deferred import) to avoid loading Pydantic at CLI import time. Actually, Pydantic is already loaded by Typer, so top-level import is fine for ValidationError.

5. **`_run_impl` function** — the actual logic:
   - Build `cli_overrides` dict from all non-None flag values. Map flag names to ExperimentConfig field names: `model` -> `model`, `backend` -> `backend`, `dataset` -> `dataset`, `n` -> `n`, `precision` -> `precision`, `output` -> `output_dir`. For `batch_size`, map to `pytorch.batch_size` (dotted key for unflatten in loader).
   - If `config` path is provided: call `load_experiment_config(path=config, cli_overrides=cli_overrides)` to get resolved ExperimentConfig. If no path and no model flag: raise ConfigError with helpful message.
   - If no config path but model flag: call `load_experiment_config(cli_overrides=cli_overrides)` — loader handles building from overrides.
   - **Dry-run branch**: If `dry_run`:
     - Import `estimate_vram` and `get_gpu_vram_gb` from `cli._vram` (deferred)
     - Import `print_dry_run` from `cli._display` (deferred)
     - Call `estimate_vram(experiment_config)` and `get_gpu_vram_gb()`
     - Call `print_dry_run(experiment_config, vram, gpu_vram_gb, verbose=verbose)`
     - Return (exit 0 naturally)
   - **Run branch**: If not dry_run:
     - Import `run_experiment` from `llenergymeasure` (deferred — this avoids loading torch at CLI init)
     - Import `print_experiment_header`, `print_result_summary` from `cli._display` (deferred)
     - Import `tqdm` from `tqdm.auto` (deferred)
     - Print experiment header to stderr
     - Wrap the run_experiment call in a tqdm indeterminate spinner (satisfies CLI-08 and CLI-10):
       ```python
       with tqdm(total=None, desc="Measuring", file=sys.stderr,
                 disable=quiet or not sys.stderr.isatty()) as pbar:
           result = run_experiment(experiment_config)
       ```
       This gives visual feedback during measurement. `disable=quiet` satisfies `--quiet` suppression (CLI-10). `not sys.stderr.isatty()` satisfies non-TTY suppression per CONTEXT.md. tqdm already respects NO_COLOR natively.
     - Call `print_result_summary(result)` — prints to stdout
     - If `experiment_config.output_dir`: call `result.save(Path(experiment_config.output_dir))` and print save path to stderr

**Update `src/llenergymeasure/cli/__init__.py`:**

Replace the current skeleton to register the run command. Keep `--version` as is. Add deferred import registration:

```python
"""Command-line interface for llem."""
from __future__ import annotations
from typing import Annotated
import typer
from llenergymeasure import __version__

app = typer.Typer(
    name="llem",
    help="LLM inference efficiency measurement framework",
    add_completion=False,
)

def version_callback(value: bool) -> None:
    if value:
        print(f"llem v{__version__}")
        raise typer.Exit()

@app.callback()
def main(
    version: Annotated[bool, typer.Option("--version", callback=version_callback, is_eager=True)] = False,
) -> None:
    """LLM inference efficiency measurement framework."""

# Register commands — deferred imports inside command functions keep startup fast
from llenergymeasure.cli.run import run as _run_cmd
app.command(name="run", help="Run an LLM efficiency experiment")(_run_cmd)

__all__ = ["app"]

if __name__ == "__main__":
    app()
```

IMPORTANT: Do NOT import from `cli/experiment.py`, `cli/config.py`, or `cli/display/`. These are v1.x modules. Only import from the new `run.py` and (in Plan 03) `config_cmd.py`.
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "from llenergymeasure.cli import app; print('CLI app loaded')" && python -c "from llenergymeasure.cli.run import run; print('run command loaded')"</automated>
  </verify>
  <done>run.py exists with full command implementation, __init__.py registers it, both import cleanly</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for llem run command</name>
  <files>tests/unit/test_cli_run.py</files>
  <action>
Create `tests/unit/test_cli_run.py` with tests using `typer.testing.CliRunner` to invoke the CLI without actually running experiments.

Tests (all must mock `run_experiment` to avoid GPU/model loading):

1. **`test_run_help`**: `runner.invoke(app, ["run", "--help"])` — exits 0, contains "--model", "--backend", "--dry-run".

2. **`test_run_no_args_exits_2`**: `runner.invoke(app, ["run"])` — should exit code 2 (no config, no --model = ConfigError).

3. **`test_run_config_error_exits_2`**: Mock `load_experiment_config` to raise `ConfigError("bad config")`. Invoke `runner.invoke(app, ["run", "nonexistent.yaml"])`. Assert exit code 2, "ConfigError" in output.

4. **`test_run_validation_error_exits_2`**: Invoke `runner.invoke(app, ["run", "--model", "gpt2", "--backend", "pytorh"])` — the misspelled backend should trigger Pydantic ValidationError. Assert exit code 2, "Config validation failed" in stderr output.

5. **`test_run_preflight_error_exits_1`**: Mock `run_experiment` to raise `PreFlightError("no GPU")`. Invoke with `--model gpt2`. Assert exit code 1.

6. **`test_run_experiment_error_exits_1`**: Mock `run_experiment` to raise `ExperimentError("crashed")`. Invoke with `--model gpt2`. Assert exit code 1.

7. **`test_run_dry_run_exits_0`**: Mock `load_experiment_config` to return a valid config, mock `estimate_vram` to return `{"weights_gb": 0.24, "kv_cache_gb": 0.01, "overhead_gb": 0.04, "total_gb": 0.29}`. Invoke `runner.invoke(app, ["run", "--model", "gpt2", "--dry-run"])`. Assert exit code 0, "Config (resolved)" in output.

8. **`test_run_version`**: `runner.invoke(app, ["--version"])`. Assert exit code 0, contains "llem v".

9. **`test_run_quiet_flag_accepted`**: Mock `run_experiment` to succeed. Patch `tqdm.auto.tqdm` and invoke with `--quiet --model gpt2`. Assert exit 0. Assert tqdm was called with `disable=True` (verifying --quiet suppresses the progress spinner). Use `@patch("llenergymeasure.cli.run.tqdm")` to capture the call args.

Use `from unittest.mock import patch, MagicMock` for mocking. Use `from typer.testing import CliRunner`. Import `app` from `llenergymeasure.cli`.

For tests that need a mock ExperimentResult, create a minimal fixture using `MagicMock(spec=ExperimentResult)` with the key attributes set (experiment_id, total_energy_j, avg_tokens_per_second, duration_sec, measurement_warnings=[], etc.).
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_cli_run.py -x -v 2>&1 | tail -25</automated>
  </verify>
  <done>All CLI run tests pass, exit codes verified for all error paths</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.cli import app"` succeeds without importing Rich or torch
2. `python -m pytest tests/unit/test_cli_run.py -x -v` — all tests pass
3. `grep -r "import rich" src/llenergymeasure/cli/run.py src/llenergymeasure/cli/__init__.py` returns nothing
4. `llem run --help` shows all expected flags (--model, --backend, --dry-run, --quiet, --verbose, etc.)
5. Exit code mapping: ConfigError → 2, PreFlightError → 1, ExperimentError → 1, ValidationError → 2, SIGINT → 130
</verification>

<success_criteria>
- `llem run --help` works and lists all CLI-04 flags
- `llem run --model gpt2 --dry-run` shows config echo and VRAM estimate (or "unavailable")
- Error exit codes match CLI-12 specification
- No Rich imports in new CLI code
- SIGINT handled via SystemExit(130), not typer.Exit
</success_criteria>

<output>
After completion, create `.planning/phases/07-cli/07-02-SUMMARY.md`
</output>
