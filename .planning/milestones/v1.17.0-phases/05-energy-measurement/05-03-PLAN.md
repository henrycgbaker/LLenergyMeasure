---
phase: 05-energy-measurement
plan: 03
type: execute
wave: 2
depends_on: ["05-01", "05-02"]
files_modified:
  - src/llenergymeasure/core/backends/pytorch.py
  - src/llenergymeasure/core/timeseries.py
  - src/llenergymeasure/core/measurement_warnings.py
  - tests/unit/test_measurement_integration.py
autonomous: true
requirements: [CM-15, CM-16, CM-17, CM-18, CM-19, CM-20, CM-25]

must_haves:
  truths:
    - "PyTorchBackend.run() measures baseline power before warmup when config.baseline.enabled is True"
    - "Energy tracking starts AFTER warmup + thermal floor, BEFORE measurement loop"
    - "torch.cuda.synchronize() is called before stopping energy tracking"
    - "Measurement window timeseries is written to timeseries.parquet in 1 Hz Parquet format"
    - "ExperimentResult.total_energy_j, avg_energy_per_token_j, and total_flops contain real values (not 0.0 placeholders)"
    - "ExperimentResult.energy_breakdown has baseline_power_w, raw_j, and adjusted_j populated"
    - "Measurement warnings are assembled and stored in result.measurement_warnings"
    - "Baseline cache persists across calls within a session (module-level dict in baseline.py)"
  artifacts:
    - path: "src/llenergymeasure/core/backends/pytorch.py"
      provides: "Full energy measurement integration into PyTorchBackend.run()"
      contains: "select_energy_backend"
    - path: "src/llenergymeasure/core/timeseries.py"
      provides: "write_timeseries_parquet() — 1 Hz Parquet timeseries writer"
      contains: "def write_timeseries_parquet"
    - path: "src/llenergymeasure/core/measurement_warnings.py"
      provides: "collect_measurement_warnings() — four warning flags"
      contains: "def collect_measurement_warnings"
    - path: "tests/unit/test_measurement_integration.py"
      provides: "Unit tests for measurement integration, timeseries, warnings"
  key_links:
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/core/energy_backends/__init__.py"
      via: "select_energy_backend import"
      pattern: "from llenergymeasure.core.energy_backends import select_energy_backend"
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/core/flops.py"
      via: "estimate_flops_palm import"
      pattern: "from llenergymeasure.core.flops import estimate_flops_palm"
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/core/baseline.py"
      via: "measure_baseline_power and create_energy_breakdown imports"
      pattern: "from llenergymeasure.core.baseline import"
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/core/timeseries.py"
      via: "write_timeseries_parquet import"
      pattern: "from llenergymeasure.core.timeseries import write_timeseries_parquet"
    - from: "src/llenergymeasure/core/backends/pytorch.py"
      to: "src/llenergymeasure/core/measurement_warnings.py"
      via: "collect_measurement_warnings import"
      pattern: "from llenergymeasure.core.measurement_warnings import collect_measurement_warnings"
---

<objective>
Wire energy measurement, FLOPs, timeseries, and warnings into PyTorchBackend.run() — replacing all 0.0 placeholder values with real measurements. This is the integration plan that connects Plans 01 and 02 into the actual experiment lifecycle.

Purpose: Without this plan, experiments produce empty energy data. This wires the energy backends (Plan 01), warmup/FLOPs (Plan 02), and existing baseline/thermal infrastructure into a complete measurement pipeline.
Output: PyTorchBackend produces real energy measurements, Parquet timeseries, FLOPs via PaLM formula, and measurement quality warnings.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-energy-measurement/05-CONTEXT.md
@.planning/phases/05-energy-measurement/05-RESEARCH.md
@.planning/phases/05-energy-measurement/05-01-SUMMARY.md
@.planning/phases/05-energy-measurement/05-02-SUMMARY.md

<interfaces>
<!-- Key types and contracts. From Plan 01 outputs (energy backends): -->

From src/llenergymeasure/core/energy_backends/__init__.py (created in Plan 01):
```python
def select_energy_backend(explicit: str | None) -> EnergyBackend | None: ...
```

From src/llenergymeasure/core/energy_backends/nvml.py (created in Plan 01):
```python
@dataclass
class EnergyMeasurement:
    total_j: float
    duration_sec: float
    samples: list  # PowerThermalSamples
    per_gpu_j: dict[int, float] | None = None

class NVMLBackend:
    def start_tracking(self) -> PowerThermalSampler: ...
    def stop_tracking(self, tracker: PowerThermalSampler) -> EnergyMeasurement: ...
```

<!-- From Plan 02 outputs (warmup/flops): -->

From src/llenergymeasure/core/flops.py (updated in Plan 02):
```python
def estimate_flops_palm(
    model: Any,
    n_input_tokens: int,
    n_output_tokens: int,
    batch_size: int = 1,
) -> FlopsResult: ...
```

From src/llenergymeasure/config/models.py (updated in Plan 02):
```python
class WarmupConfig(BaseModel):
    n_warmup: int = Field(default=5, ge=1)
    thermal_floor_seconds: float = Field(default=60.0, ge=30.0)
    convergence_detection: bool = Field(default=False)
    enabled: bool = Field(default=True)
    # ... CV fields ...

class EnergyConfig(BaseModel):
    backend: Literal["auto", "nvml", "zeus", "codecarbon"] | None = Field(default="auto")
```

<!-- Existing infrastructure: -->

From src/llenergymeasure/core/baseline.py:
```python
def measure_baseline_power(device_index=0, duration_sec=30.0, ...) -> BaselineCache | None: ...
def create_energy_breakdown(total_j, baseline, duration_sec) -> EnergyBreakdown: ...
```

From src/llenergymeasure/core/warmup.py:
```python
def warmup_until_converged(run_single_inference, config, *, show_progress=True) -> WarmupResult: ...
def create_warmup_inference_fn(model, tokenizer, prompt, max_new_tokens=32) -> Callable[[], float]: ...
```

From src/llenergymeasure/core/power_thermal.py:
```python
class PowerThermalSampler: ...
@dataclass
class PowerThermalSample:
    timestamp: float
    power_w: float | None = None
    temperature_c: float | None = None
    memory_used_mb: float | None = None
    memory_total_mb: float | None = None
    sm_utilisation: float | None = None
    throttle_reasons: int = 0
```

From src/llenergymeasure/core/backends/pytorch.py (CURRENT, needs modification):
```python
class PyTorchBackend:
    def run(self, config) -> ExperimentResult: ...
    def _run_warmup(self, model, tokenizer, config, prompts) -> None: ...  # returns None now
    def _run_measurement(self, ...) -> tuple[_MeasurementData, ThermalThrottleInfo]: ...
    def _build_result(self, ...) -> ExperimentResult: ...  # has 0.0 placeholders
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create timeseries writer and measurement warnings module</name>
  <files>
    src/llenergymeasure/core/timeseries.py
    src/llenergymeasure/core/measurement_warnings.py
  </files>
  <action>
  **Create `src/llenergymeasure/core/timeseries.py`:**

  Implement `write_timeseries_parquet()` — downsamples 100ms NVML samples to 1 Hz and writes Parquet.

  ```python
  """Parquet timeseries writer for energy measurement telemetry.

  Downsamples 100ms NVML power/thermal samples to 1 Hz and writes
  a Parquet sidecar file alongside the result JSON. Uses pyarrow
  directly (no pandas dependency).
  """
  from __future__ import annotations
  from pathlib import Path
  from typing import TYPE_CHECKING

  if TYPE_CHECKING:
      from llenergymeasure.core.power_thermal import PowerThermalSample
  ```

  Function signature:
  ```python
  def write_timeseries_parquet(
      samples: list[PowerThermalSample],
      output_path: Path,
      gpu_index: int = 0,
  ) -> Path:
  ```

  Implementation:
  - If no samples, write an empty Parquet with the correct schema and return.
  - Group samples into 1-second buckets: `bucket = int(s.timestamp - base_ts)` where `base_ts = samples[0].timestamp`.
  - For each bucket, compute mean of `power_w`, `temperature_c`, `memory_used_mb`, `memory_total_mb`, `sm_utilisation`. OR all `throttle_reasons` bitmasks.
  - Build rows as list of dicts.
  - Schema (locked in CONTEXT.md):
    - `timestamp_s` (float64) — seconds from measurement start
    - `gpu_index` (int32)
    - `power_w` (float32, nullable)
    - `temperature_c` (float32, nullable)
    - `memory_used_mb` (float32, nullable)
    - `memory_total_mb` (float32, nullable)
    - `sm_utilisation_pct` (float32, nullable)
    - `throttle_reasons` (int64)
  - Use `pa.Table.from_pylist(rows, schema=schema)` and `pq.write_table(table, output_path)`.
  - Create parent directory (`output_path.parent.mkdir(parents=True, exist_ok=True)`).
  - All pyarrow imports deferred to function body.

  Also add `_timeseries_schema() -> pa.Schema` helper.

  **Create `src/llenergymeasure/core/measurement_warnings.py`:**

  Implement `collect_measurement_warnings()` — assembles the four warning flags.

  ```python
  """Measurement quality warnings for energy experiments.

  Four warning flags, all purely informational (never block experiments).
  Each includes actionable remediation advice per CONTEXT.md.
  """
  ```

  Function signature:
  ```python
  def collect_measurement_warnings(
      duration_sec: float,
      gpu_persistence_mode: bool,
      temp_start_c: float | None,
      temp_end_c: float | None,
      nvml_sample_count: int,
      thermal_drift_threshold_c: float = 10.0,  # Confidence LOW — engineering judgement
  ) -> list[str]:
  ```

  Four warnings:
  1. `short_measurement_duration` — if `duration_sec < 10.0`:
     `"short_measurement_duration: measurement < 10s; energy values may be unreliable. Use more prompts or longer sequences."`
  2. `gpu_persistence_mode_off` — if not `gpu_persistence_mode`:
     `"gpu_persistence_mode_off: power state variation may inflate measurements. Run 'nvidia-smi -pm 1' to enable persistence mode."`
  3. `thermal_drift_detected` — if start/end temps exist and `abs(end - start) > threshold`:
     `"thermal_drift_detected: {drift:.1f}C change during measurement (threshold {threshold}C). Increase thermal_floor_seconds or check cooling."`
  4. `nvml_low_sample_count` — if `nvml_sample_count < 10`:
     `"nvml_low_sample_count: fewer than 10 NVML power samples collected; energy integration may be inaccurate."`

  Add code comment: `# thermal_drift_threshold default 10C — confidence LOW, no peer citation, flagged for validation`
  Add code comment: `# baseline_duration 30s — confidence MEDIUM (similar to VILE paper 22-33s windows)`
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -c "
from llenergymeasure.core.timeseries import write_timeseries_parquet
from llenergymeasure.core.measurement_warnings import collect_measurement_warnings
print('Imports OK')
# Test warnings
w = collect_measurement_warnings(5.0, False, 30.0, 45.0, 5, thermal_drift_threshold_c=10.0)
assert len(w) == 4, f'Expected 4 warnings, got {len(w)}: {w}'
print(f'Warnings OK: {len(w)} warnings')
w2 = collect_measurement_warnings(60.0, True, 30.0, 32.0, 100)
assert len(w2) == 0, f'Expected 0 warnings, got {len(w2)}'
print('Clean measurement OK: 0 warnings')
"</automated>
  </verify>
  <done>write_timeseries_parquet() creates 1 Hz Parquet sidecar with locked column schema. collect_measurement_warnings() returns the four warning flags with actionable remediation advice.</done>
</task>

<task type="auto">
  <name>Task 2: Wire energy, FLOPs, timeseries, and warnings into PyTorchBackend.run()</name>
  <files>
    src/llenergymeasure/core/backends/pytorch.py
    tests/unit/test_measurement_integration.py
  </files>
  <action>
  **Rewrite `src/llenergymeasure/core/backends/pytorch.py` — the run() lifecycle:**

  The run() method must be restructured to follow this sequence (per CONTEXT.md / RESEARCH.md):

  ```
  1. Environment snapshot (BEFORE model load — already done, keep)
  2. Baseline power measurement (BEFORE model load — CM-17)
  3. Load model + tokenizer (keep as-is)
  4. Prepare prompts (keep as-is)
  5. Warmup: use warmup_until_converged() with WarmupResult return (CM-21, CM-24)
  6. Thermal floor: time.sleep(config.warmup.thermal_floor_seconds) (CM-22)
  7. Energy tracking START (CM-14 — select_energy_backend from config.energy.backend)
  8. PowerThermalSampler START (for timeseries — already wraps measurement, keep)
  9. Measurement loop (keep, but track input/output token counts separately)
  10. torch.cuda.synchronize() (CM-15)
  11. Energy tracking STOP → EnergyMeasurement
  12. PowerThermalSampler STOP → samples for timeseries
  13. FLOPs estimation: estimate_flops_palm(model, input_tokens, output_tokens) (CM-26, CM-28)
  14. Timeseries: write_timeseries_parquet() if output_dir set (CM-16)
  15. Warnings: collect_measurement_warnings() (CM-25 implied)
  16. Build result with real values (CM-18, CM-19, CM-25)
  17. Cleanup (keep as-is)
  ```

  Specific changes to `run()`:

  ```python
  def run(self, config: ExperimentConfig) -> ExperimentResult:
      # 1. Environment snapshot
      snapshot = collect_environment_snapshot()

      # 2. Baseline power (CM-17, CM-20)
      baseline = None
      if config.baseline.enabled:
          from llenergymeasure.core.baseline import measure_baseline_power
          baseline = measure_baseline_power(
              duration_sec=config.baseline.duration_seconds,
          )

      # 3. Load model + tokenizer
      model, tokenizer = self._load_model(config)
      try:
          # 4. Prepare prompts
          prompts = self._prepare_prompts(config, tokenizer)

          # 5. Warmup (CM-21, CM-24) — returns WarmupResult
          warmup_result = self._run_warmup(model, tokenizer, config, prompts)

          # 6. Thermal floor (CM-22) — capture duration for WarmupResult provenance
          import time
          thermal_floor_wait_s = 0.0
          if config.warmup.thermal_floor_seconds > 0:
              logger.info("Thermal stabilisation: waiting %.0fs...", config.warmup.thermal_floor_seconds)
              t0 = time.monotonic()
              time.sleep(config.warmup.thermal_floor_seconds)
              thermal_floor_wait_s = time.monotonic() - t0
          warmup_result.thermal_floor_wait_s = thermal_floor_wait_s

          # 7. Select energy backend (CM-14)
          from llenergymeasure.core.energy_backends import select_energy_backend
          energy_backend = select_energy_backend(config.energy.backend)

          # 8-11. Measurement with energy tracking
          energy_tracker = energy_backend.start_tracking() if energy_backend else None
          start_time = datetime.now()
          result_data, thermal_info = self._run_measurement(model, tokenizer, config, prompts)
          # CM-15: sync GPU before stopping energy
          self._cuda_sync()
          energy_measurement = None
          if energy_backend and energy_tracker:
              energy_measurement = energy_backend.stop_tracking(energy_tracker)
          end_time = datetime.now()

          # 13. FLOPs (CM-26, CM-28 — warmup tokens excluded by passing measurement-only counts)
          from llenergymeasure.core.flops import estimate_flops_palm
          flops_result = estimate_flops_palm(
              model=model,
              n_input_tokens=result_data.input_tokens,
              n_output_tokens=result_data.output_tokens,
          )
      finally:
          self._cleanup(model)

      # 14-16. Post-measurement: timeseries, warnings, result
      ...
  ```

  **Update `_run_warmup()` to return `WarmupResult`:**

  Change return type from `None` to `WarmupResult`. Use `warmup_until_converged()` from `core/warmup.py`:

  ```python
  def _run_warmup(self, model, tokenizer, config, prompts) -> WarmupResult:
      from llenergymeasure.core.warmup import warmup_until_converged, create_warmup_inference_fn
      if not config.warmup.enabled:
          return WarmupResult(converged=True, final_cv=0.0, iterations_completed=0,
                              target_cv=config.warmup.cv_threshold, max_prompts=config.warmup.max_warmup_prompts)
      warmup_prompt = prompts[0] if prompts else "Hello, world"
      inference_fn = create_warmup_inference_fn(model, tokenizer, warmup_prompt, config.max_output_tokens)
      return warmup_until_converged(inference_fn, config.warmup, show_progress=False)
  ```

  **Update `_MeasurementData` to track input/output tokens separately:**

  Add `input_tokens: int = 0` and `output_tokens: int = 0` fields. Update `_run_batch()` and `_run_measurement()` to accumulate them. This is needed for CM-28 (warmup tokens excluded from FLOPs — we only count measurement tokens).

  **Update `_build_result()` to use real values:**

  Replace all 0.0 placeholders:
  - `total_energy_j`: from `energy_measurement.total_j` if available, else `0.0`
  - `avg_energy_per_token_j`: `total_energy_j / total_tokens` if both > 0 (CM-25)
  - `total_flops`: from `flops_result.value`
  - `energy_breakdown`: from `create_energy_breakdown(total_j, baseline, duration_sec)` (CM-18, CM-19)
  - `measurement_warnings`: from `collect_measurement_warnings(...)` (CONTEXT.md)
  - `warmup_result`: from the WarmupResult returned by `_run_warmup()` (CM-24)
  - `timeseries_path`: relative path to Parquet file if written

  Add signature parameters for the new data:
  ```python
  def _build_result(self, config, data, snapshot, start_time, end_time, thermal_info,
                    energy_measurement, baseline, flops_result, warmup_result,
                    timeseries_path, measurement_warnings) -> ExperimentResult:
  ```

  **Add `_cuda_sync()` static method (CM-15):**

  ```python
  @staticmethod
  def _cuda_sync() -> None:
      """Synchronize CUDA before stopping energy measurement (CM-15)."""
      import importlib.util
      if importlib.util.find_spec("torch") is not None:
          try:
              import torch
              if torch.cuda.is_available():
                  torch.cuda.synchronize()
          except Exception:
              pass  # Non-fatal — best effort sync
  ```

  **Write timeseries if output_dir is set:**

  After measurement, if `config.output_dir` is not None:
  ```python
  from llenergymeasure.core.timeseries import write_timeseries_parquet
  output_dir = Path(config.output_dir)
  ts_path = write_timeseries_parquet(sampler_samples, output_dir / "timeseries.parquet", gpu_index=0)
  timeseries_path = "timeseries.parquet"  # relative path in result JSON
  ```

  **CRITICAL sequencing:** The PowerThermalSampler (for timeseries + thermal info) should wrap the measurement loop. The energy backend tracking should ALSO wrap the measurement loop. Both must start AFTER warmup + thermal floor. The `with PowerThermalSampler(...) as sampler:` context manager stays around the measurement loop, but the energy backend start/stop calls go outside the sampler if they are different (NVML backend IS the same sampler conceptually, but Zeus is separate).

  For NVMLBackend: the sampler IS the energy tracker (start_tracking returns a sampler). So you get both energy integration AND timeseries from the same sampler. But they're separate objects — the energy backend sampler and the timeseries sampler should be the SAME sampler to avoid double NVML sessions.

  **Resolution:** When using NVMLBackend, the energy backend's start_tracking() creates its own sampler. Use that sampler's samples for timeseries too. When using ZeusBackend, create a separate PowerThermalSampler for timeseries. When no energy backend, create a PowerThermalSampler for timeseries only.

  Simplest approach: always create a PowerThermalSampler for timeseries + thermal info (as it is now). If using NVMLBackend, the sampler inside NVMLBackend is separate but that's fine — NVML polling is read-only (two samplers don't conflict). For Zeus, the ZeusMonitor owns the energy counter and a separate sampler provides telemetry.

  **Create `tests/unit/test_measurement_integration.py`:**

  Write unit tests (all mocked, no GPU):

  1. `test_timeseries_parquet_write()` — create synthetic PowerThermalSamples spanning 3 seconds, call `write_timeseries_parquet()`, read back with pyarrow, assert schema matches and has 3-4 rows
  2. `test_timeseries_parquet_empty()` — write with empty samples list, assert Parquet file exists with 0 rows
  3. `test_warnings_short_duration()` — 5s duration, assert "short_measurement_duration" in warnings
  4. `test_warnings_persistence_mode()` — persistence off, assert "gpu_persistence_mode_off" in warnings
  5. `test_warnings_thermal_drift()` — 15C drift, assert "thermal_drift_detected" in warnings
  6. `test_warnings_low_sample_count()` — 5 samples, assert "nvml_low_sample_count" in warnings
  7. `test_warnings_clean_measurement()` — good values, assert 0 warnings
  8. `test_cuda_sync_called_before_stop()` — mock torch.cuda.synchronize, verify it's called in the correct position (before energy stop)
  9. `test_baseline_before_warmup()` — mock measure_baseline_power, verify it's called before model load
  10. `test_flops_uses_measurement_tokens_only()` — verify estimate_flops_palm receives measurement token counts, not warmup tokens
  </action>
  <verify>
    <automated>cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && python -m pytest tests/unit/test_measurement_integration.py -x -v 2>&1 | tail -20</automated>
  </verify>
  <done>PyTorchBackend.run() produces real energy measurements, writes Parquet timeseries, computes PaLM FLOPs, and assembles measurement warnings. All 0.0 placeholders replaced. 10+ integration tests pass.</done>
</task>

</tasks>

<verification>
1. `python -c "from llenergymeasure.core.timeseries import write_timeseries_parquet"` — timeseries module exists
2. `python -c "from llenergymeasure.core.measurement_warnings import collect_measurement_warnings"` — warnings module exists
3. `python -m pytest tests/unit/test_measurement_integration.py -x -v` — all tests pass
4. Review `pytorch.py` run() method to confirm:
   - baseline measurement happens before model load
   - energy tracking starts after warmup + thermal floor
   - torch.cuda.synchronize() before energy stop
   - FLOPs use measurement-only token counts
   - Result has real energy values, not 0.0
</verification>

<success_criteria>
- PyTorchBackend.run() follows correct sequence: snapshot → baseline → load → warmup → thermal floor → energy start → measurement → cuda sync → energy stop → FLOPs → timeseries → warnings → result
- ExperimentResult.total_energy_j populated from energy backend (not 0.0)
- ExperimentResult.energy_breakdown populated with baseline adjustment
- ExperimentResult.total_flops populated from PaLM formula
- ExperimentResult.measurement_warnings populated with quality flags
- Timeseries written as 1 Hz Parquet with locked column schema
- torch.cuda.synchronize() called before energy stop
- Warmup tokens excluded from FLOPs calculation
- 10+ integration tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-energy-measurement/05-03-SUMMARY.md`
</output>
