---
phase: 08-testing-and-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/fakes.py
  - tests/unit/test_api.py
  - tests/unit/test_backend_protocol.py
  - tests/unit/test_warmup_v2.py
  - tests/unit/test_measurement_integration.py
  - pyproject.toml
autonomous: true
requirements: [INF-09, INF-10]

must_haves:
  truths:
    - "pytest tests/unit/ collects all v2.0 tests without ImportError — no v1.x files polluting collection"
    - "All 16 currently failing v2.0 tests pass after make_result factory fix and backend_protocol field fix"
    - "tests/fakes.py contains FakeInferenceBackend and FakeEnergyBackend implementing protocols structurally"
    - "tests/conftest.py provides make_config() and make_result() factory functions usable by all test files"
    - "pyproject.toml has [tool.pytest.ini_options] markers section with 'gpu' mark registered"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Shared fixtures and factory functions"
      contains: "make_config"
    - path: "tests/fakes.py"
      provides: "Protocol injection fakes"
      contains: "FakeInferenceBackend"
    - path: "pyproject.toml"
      provides: "pytest gpu marker registration"
      contains: "markers"
  key_links:
    - from: "tests/fakes.py"
      to: "src/llenergymeasure/protocols.py"
      via: "structural protocol implementation"
      pattern: "class FakeInferenceBackend"
    - from: "tests/conftest.py"
      to: "src/llenergymeasure/config/models.py"
      via: "make_config factory"
      pattern: "ExperimentConfig"
---

<objective>
Clean the test suite of all v1.x artefacts, create shared test infrastructure (fakes, factories, fixtures), fix the 16 failing v2.0 tests, and register pytest marks.

Purpose: Establish a clean test foundation so that all subsequent test writing (Plan 02) and CI wiring (Plan 03) operates on a working, collection-clean test suite.
Output: Zero collection errors, zero test failures, shared `tests/conftest.py` and `tests/fakes.py` ready for use.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-testing-and-integration/08-CONTEXT.md
@.planning/phases/08-testing-and-integration/08-RESEARCH.md

<interfaces>
<!-- Key protocols that fakes must implement -->

From src/llenergymeasure/protocols.py:
```python
class InferenceEngine(Protocol):
    def run(self, config: ExperimentConfig, model: Any, tokenizer: Any) -> "RawProcessResult": ...

class EnergyBackend(Protocol):
    @property
    def name(self) -> str: ...
    def start_tracking(self) -> Any: ...
    def stop_tracking(self, tracker: Any) -> Any: ...
    def is_available(self) -> bool: ...

class ResultsRepository(Protocol):
    def save(self, result: ExperimentResult, output_dir: Path) -> Path: ...
    def load(self, path: Path) -> ExperimentResult: ...

class ModelLoader(Protocol):
    def load(self, config: ExperimentConfig) -> tuple[Any, Any]: ...

class MetricsCollector(Protocol):
    def collect(self, config: ExperimentConfig, model: Any, tokenizer: Any, inference_result: Any) -> Any: ...
```

From src/llenergymeasure/core/backends/protocol.py:
```python
class InferenceBackend(Protocol):
    name: str
    def run(self, config: ExperimentConfig) -> ExperimentResult: ...
```

From src/llenergymeasure/domain/experiment.py:
```python
class ExperimentResult(BaseModel):
    # Required fields (no defaults — MUST be in make_result):
    experiment_id: str
    measurement_config_hash: str  # 16-char hex
    measurement_methodology: Literal["total", "steady_state", "windowed"]
    aggregation: AggregationMetadata
    total_tokens: int
    total_energy_j: float
    total_inference_time_sec: float
    avg_tokens_per_second: float
    avg_energy_per_token_j: float
    total_flops: float
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Delete v1.x test files and directories, register pytest gpu mark</name>
  <files>
    tests/unit/cli/
    tests/unit/config/
    tests/unit/notifications/
    tests/unit/orchestration/
    tests/unit/results/
    tests/e2e/
    tests/runtime/
    tests/fixtures/
    tests/configs/
    tests/conftest_backends.py
    tests/unit/test_campaign_group_by.py (via cli/)
    tests/unit/test_campaign.py
    tests/unit/test_campaign_context.py
    tests/unit/test_config_models.py
    tests/unit/test_config_loader.py
    tests/unit/test_config_backend_configs.py
    tests/unit/test_config_introspection.py
    tests/unit/test_constants.py
    tests/unit/test_container_strategy.py
    tests/unit/test_core_baseline.py
    tests/unit/test_core_compute_metrics.py
    tests/unit/test_core_distributed.py
    tests/unit/test_core_energy_backends.py
    tests/unit/test_core_environment.py
    tests/unit/test_core_implementations.py
    tests/unit/test_core_inference.py
    tests/unit/test_core_model_loader.py
    tests/unit/test_core_parallelism.py
    tests/unit/test_core_power_thermal.py
    tests/unit/test_core_prompts.py
    tests/unit/test_core_traffic.py
    tests/unit/test_core_warmup.py
    tests/unit/test_dataset_loader.py
    tests/unit/test_docker_detection.py
    tests/unit/test_domain_experiment.py
    tests/unit/test_domain_metrics.py
    tests/unit/test_domain_model_info.py
    tests/unit/test_domain_schema_v3.py
    tests/unit/test_env_setup.py
    tests/unit/test_exceptions.py
    tests/unit/test_extended_metrics.py
    tests/unit/test_flops_estimator.py
    tests/unit/test_inference_generation.py
    tests/unit/test_logging.py
    tests/unit/test_orchestration_context.py
    tests/unit/test_orchestration_factory.py
    tests/unit/test_orchestration_launcher.py
    tests/unit/test_orchestration_lifecycle.py
    tests/unit/test_orchestration_runner.py
    tests/unit/test_pytorch_streaming.py
    tests/unit/test_repository.py
    tests/unit/test_resilience.py
    tests/unit/test_results_aggregation.py
    tests/unit/test_results_exporters.py
    tests/unit/test_results_exporters_v3.py
    tests/unit/test_results_timeseries.py
    tests/unit/test_schema_version.py
    tests/unit/test_security.py
    tests/unit/test_state.py
    tests/unit/test_streaming_latency.py
    tests/unit/test_tensorrt_streaming.py
    tests/unit/test_user_config.py
    tests/unit/test_backend_detection.py
    tests/unit/test_cli.py
    tests/integration/test_config_aggregation_pipeline.py
    tests/integration/test_config_params_wired.py
    tests/integration/test_error_handling.py
    tests/integration/test_cli_workflows.py
    tests/integration/test_docker_naming.py
    tests/integration/test_entrypoint_puid.py
    tests/integration/test_repository_operations.py
    pyproject.toml
  </files>
  <action>
    **Delete all v1.x test artefacts.** The decision is locked: start fresh, delete v1.x tests entirely.

    1. **Delete entire v1.x subdirectories** inside `tests/unit/`:
       - `tests/unit/cli/` (entire directory — test_campaign_group_by.py, test_config_list.py, test_docker_gpu_propagation.py, test_init_cmd.py, test_resume.py)
       - `tests/unit/config/` (entire directory — test_campaign_config.py, test_parallelism_validation.py)
       - `tests/unit/notifications/` (entire directory — test_webhook.py)
       - `tests/unit/orchestration/` (entire directory — test_grid.py, test_manifest.py)
       - `tests/unit/results/` (entire directory — test_bootstrap.py)

    2. **Delete v1.x test files** from `tests/unit/` root. Delete every `.py` file that is NOT in this keep list:
       **KEEP these v2.0 files (do NOT delete):**
       - `tests/unit/__init__.py`
       - `tests/unit/test_api.py`
       - `tests/unit/test_backend_protocol.py`
       - `tests/unit/test_preflight.py`
       - `tests/unit/test_environment_snapshot.py`
       - `tests/unit/test_energy_backends_v2.py`
       - `tests/unit/test_flops_v2.py`
       - `tests/unit/test_warmup_v2.py`
       - `tests/unit/test_experiment_result_v2.py`
       - `tests/unit/test_persistence_v2.py`
       - `tests/unit/test_aggregation_v2.py`
       - `tests/unit/test_cli_run.py`
       - `tests/unit/test_cli_config.py`
       - `tests/unit/test_cli_display.py`
       - `tests/unit/test_measurement_integration.py`

       Delete everything else in `tests/unit/` (all the v1.x files listed in the files section above).

    3. **Delete v1.x top-level test directories:**
       - `tests/e2e/` (entire directory)
       - `tests/runtime/` (entire directory)
       - `tests/fixtures/` (entire directory)
       - `tests/configs/` (entire directory if it exists)
       - `tests/conftest_backends.py`

    4. **Delete all v1.x integration test files** (keep `tests/integration/__init__.py` only):
       - `tests/integration/test_config_aggregation_pipeline.py`
       - `tests/integration/test_config_params_wired.py`
       - `tests/integration/test_error_handling.py`
       - `tests/integration/test_cli_workflows.py`
       - `tests/integration/test_docker_naming.py`
       - `tests/integration/test_entrypoint_puid.py`
       - `tests/integration/test_repository_operations.py`

    5. **Update `tests/CLAUDE.md`** — replace the entire contents with a v2.0 description reflecting the new two-tier structure: `tests/unit/` (GPU-free) + `tests/integration/` (@pytest.mark.gpu). Remove all references to e2e/, runtime/, fixtures/, configs/, Makefile targets, runtime-test-orchestrator, and v1.x patterns.

    6. **Register pytest `gpu` mark in `pyproject.toml`** — add a `markers` entry to `[tool.pytest.ini_options]`:
       ```toml
       markers = [
           "gpu: marks tests as requiring GPU hardware (deselect with '-m \"not gpu\"')",
       ]
       ```
       Also update `addopts` to exclude gpu tests by default: `addopts = "-v --tb=short -m \"not gpu\""`.

    7. Run `pytest tests/unit/ --collect-only -q 2>&1 | tail -5` to confirm zero collection errors.
  </action>
  <verify>
    <automated>pytest tests/unit/ --collect-only -q 2>&1 | tail -5</automated>
  </verify>
  <done>All v1.x test files and directories deleted. Only 15 v2.0 unit test files remain in tests/unit/. pytest collects without ImportError. `gpu` mark registered in pyproject.toml.</done>
</task>

<task type="auto">
  <name>Task 2: Create shared test infrastructure and fix all 16 failing v2.0 tests</name>
  <files>
    tests/conftest.py
    tests/fakes.py
    tests/unit/test_api.py
    tests/unit/test_backend_protocol.py
    tests/unit/test_warmup_v2.py
    tests/unit/test_measurement_integration.py
  </files>
  <action>
    **Create `tests/fakes.py`** with protocol injection fakes (INF-10). These are transparent fake classes that implement the Protocol interfaces structurally — NO MagicMock, NO unittest.mock.patch on internals.

    ```python
    """Protocol injection fakes for unit tests.

    Each fake implements a Protocol structurally (duck typing). Behaviour is explicit
    in the class body — no implicit MagicMock returns. Tests inject fakes via
    constructor args or function parameters, never via unittest.mock.patch on
    internal modules.
    """
    from __future__ import annotations
    from pathlib import Path
    from typing import Any

    from llenergymeasure.config.models import ExperimentConfig
    from llenergymeasure.domain.experiment import ExperimentResult


    class FakeInferenceBackend:
        """Minimal InferenceBackend fake — returns a pre-built ExperimentResult."""

        name = "fake"

        def __init__(self, result: ExperimentResult | None = None):
            self._result = result
            self.run_calls: list[ExperimentConfig] = []

        def run(self, config: ExperimentConfig) -> ExperimentResult:
            self.run_calls.append(config)
            if self._result is None:
                raise ValueError("FakeInferenceBackend: set result before calling run()")
            return self._result


    class FakeEnergyBackend:
        """Minimal EnergyBackend fake — returns fixed EnergyMeasurement."""

        name = "fake-energy"

        def __init__(self, total_j: float = 10.0, duration_sec: float = 5.0):
            self._total_j = total_j
            self._duration_sec = duration_sec
            self._tracking = False

        def start_tracking(self) -> str:
            self._tracking = True
            return "fake-tracker"

        def stop_tracking(self, tracker: Any) -> Any:
            self._tracking = False
            from llenergymeasure.core.energy_backends.nvml import EnergyMeasurement
            return EnergyMeasurement(total_j=self._total_j, duration_sec=self._duration_sec)

        def is_available(self) -> bool:
            return True


    class FakeResultsRepository:
        """Minimal ResultsRepository fake — stores results in memory."""

        def __init__(self):
            self.saved: list[tuple[ExperimentResult, Path]] = []

        def save(self, result: ExperimentResult, output_dir: Path) -> Path:
            self.saved.append((result, output_dir))
            return output_dir / "result.json"

        def load(self, path: Path) -> ExperimentResult:
            if self.saved:
                return self.saved[-1][0]
            raise FileNotFoundError(f"No saved results: {path}")
    ```

    **Rewrite `tests/conftest.py`** — replace the entire v1.x conftest with v2.0 factories and shared fixtures:

    ```python
    """Pytest configuration and shared fixtures for v2.0 tests."""
    from __future__ import annotations

    import pytest

    from llenergymeasure.config.models import ExperimentConfig
    from llenergymeasure.domain.experiment import AggregationMetadata, ExperimentResult


    def make_config(**overrides) -> ExperimentConfig:
        """Return a valid ExperimentConfig with sensible defaults.

        Tests override only what they care about.
        """
        defaults: dict = {
            "model": "gpt2",
            "backend": "pytorch",
        }
        defaults.update(overrides)
        return ExperimentConfig(**defaults)


    def make_result(**overrides) -> ExperimentResult:
        """Return a valid ExperimentResult with sensible defaults.

        Includes all required fields (measurement_config_hash,
        measurement_methodology) to prevent ValidationError.
        """
        defaults: dict = {
            "experiment_id": "test-001",
            "measurement_config_hash": "abc123def4567890",
            "measurement_methodology": "total",
            "aggregation": AggregationMetadata(num_processes=1),
            "total_tokens": 1000,
            "total_energy_j": 10.0,
            "total_inference_time_sec": 5.0,
            "avg_tokens_per_second": 200.0,
            "avg_energy_per_token_j": 0.01,
            "total_flops": 1e9,
        }
        defaults.update(overrides)
        return ExperimentResult(**defaults)


    @pytest.fixture
    def sample_config() -> ExperimentConfig:
        return make_config()


    @pytest.fixture
    def sample_result() -> ExperimentResult:
        return make_result()


    @pytest.fixture
    def tmp_results_dir(tmp_path):
        d = tmp_path / "results"
        d.mkdir()
        return d
    ```

    **Fix the 16 failing v2.0 tests.** All failures fall into two categories:

    **Category A — ExperimentResult missing required fields (14 tests):**
    In `test_api.py`, `test_warmup_v2.py`, and `test_measurement_integration.py`, the local `_make_experiment_result()` / `make_result()` factory functions are missing `measurement_config_hash` and `measurement_methodology`. Fix each file:

    - **`test_api.py`**: Find the local `_make_experiment_result` function (around line 59). Add `"measurement_config_hash": "abc123def4567890"` and `"measurement_methodology": "total"` to its defaults dict. Alternatively, replace the local factory with an import: `from tests.conftest import make_result` and use `make_result()` throughout. The latter is preferred to centralise the factory.

    - **`test_warmup_v2.py`**: Find where `ExperimentResult(...)` is constructed directly (around lines 293 and 314). Add the two missing fields to those constructions, or import and use `make_result()` from conftest.

    - **`test_measurement_integration.py`**: Same issue — ExperimentResult constructions missing the two required fields. The failures at lines where `_build_result()` is called suggest the source code's `_build_result()` in `pytorch.py` (line 757) is constructing ExperimentResult without these fields. Check if the fix belongs in the test (mocked return values) or in the source. If `_build_result()` in `src/llenergymeasure/core/backends/pytorch.py` is genuinely missing these fields, add them there (measurement_config_hash from `compute_config_hash(config)`, measurement_methodology from `"total"`). But if the test is calling `_build_result()` with mocked inputs that lack these fields, fix the test inputs.

    **Category B — KeyError on quantisation fields (2 tests):**
    In `test_backend_protocol.py`, two tests (`test_model_load_kwargs_pytorch_config_load_in_4bit` and `test_model_load_kwargs_pytorch_config_load_in_8bit`) access `load_in_4bit`/`load_in_8bit` as direct PyTorchConfig fields, but Phase 4.1 moved these into `BitsAndBytesConfig` nested under `pytorch.bnb_config`. Fix these tests to access the correct field path. Check `PyTorchConfig` in `config/backend_configs.py` for the actual field names, then update the test assertions to match the v2.0 schema.
  </action>
  <verify>
    <automated>pytest tests/unit/ -v --tb=short -q 2>&1 | tail -10</automated>
  </verify>
  <done>All 16 previously failing tests now pass. `pytest tests/unit/` reports 0 failures. `tests/conftest.py` has make_config() and make_result() factories. `tests/fakes.py` has FakeInferenceBackend, FakeEnergyBackend, and FakeResultsRepository. Protocol fakes satisfy `isinstance()` checks against `@runtime_checkable` protocols.</done>
</task>

</tasks>

<verification>
1. `pytest tests/unit/ --collect-only -q` — collects all v2.0 tests, zero ImportError
2. `pytest tests/unit/ -v --tb=short` — all tests pass (0 failures)
3. `python -c "from tests.fakes import FakeInferenceBackend, FakeEnergyBackend"` — fakes importable
4. `python -c "from tests.conftest import make_config, make_result; make_config(); make_result()"` — factories produce valid objects
5. `grep -c 'gpu' pyproject.toml` — marker registered
</verification>

<success_criteria>
- `pytest tests/unit/ -q` exits 0 with all tests passing
- No v1.x test files remain in the repository
- tests/fakes.py and tests/conftest.py exist with the specified factories and fakes
- pyproject.toml has gpu marker registered
</success_criteria>

<output>
After completion, create `.planning/phases/08-testing-and-integration/08-01-SUMMARY.md`
</output>
