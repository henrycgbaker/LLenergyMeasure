---
phase: 03-parameter-completeness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/backend_configs.py
autonomous: true

must_haves:
  truths:
    - "PyTorch backend exposes 95%+ of energy-impactful generation parameters"
    - "New PyTorch parameters have docstring-documented constraints"
    - "Introspection auto-discovers all new PyTorch parameters"
  artifacts:
    - path: "src/llenergymeasure/config/backend_configs.py"
      provides: "Extended PyTorchConfig with new generation parameters"
      contains: "guidance_scale"
  key_links:
    - from: "src/llenergymeasure/config/backend_configs.py"
      to: "introspection.py"
      via: "get_backend_params('pytorch')"
      pattern: "get_backend_params"
---

<objective>
Add missing energy/throughput-impactful parameters to PyTorchConfig Pydantic model.

Purpose: Expand PyTorch parameter coverage from 93.8% to 95%+ by adding generation parameters from HuggingFace transformers GenerationConfig that affect energy/compute.
Output: Extended PyTorchConfig with new fields that introspection auto-discovers.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-parameter-completeness/03-CONTEXT.md
@.planning/phases/03-parameter-completeness/03-RESEARCH.md
@src/llenergymeasure/config/backend_configs.py
@src/llenergymeasure/config/introspection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add missing PyTorch generation parameters</name>
  <files>src/llenergymeasure/config/backend_configs.py</files>
  <action>
Add the following missing parameters to PyTorchConfig (after the existing decoder extensions section):

**High-impact generation parameters:**
1. `guidance_scale: float | None = Field(default=None, ge=1.0, description="Classifier-free guidance scale. >1.0 increases prompt adherence at cost of diversity. Impacts compute significantly.")`
2. `low_memory: bool = Field(default=False, description="Enable memory-efficient generation mode. Reduces peak memory at slight throughput cost.")`
3. `exponential_decay_length_penalty: tuple[int, float] | None = Field(default=None, description="(start_index, decay_factor) tuple for length-based score decay. start_index >= 0, decay_factor in (0, 1].")`
4. `diversity_penalty: float = Field(default=0.0, ge=0.0, description="Diversity penalty for beam search (0.0 = disabled). Higher values encourage diverse beam hypotheses.")`
5. `encoder_repetition_penalty: float = Field(default=1.0, ge=1.0, description="Repetition penalty applied to encoder hidden states. For encoder-decoder models only.")`

**Sampling control parameters:**
6. `typical_p: float = Field(default=1.0, ge=0.0, le=1.0, description="Typical decoding mass parameter. 1.0 = disabled. Lower values are more deterministic.")`
7. `epsilon_cutoff: float = Field(default=0.0, ge=0.0, le=1.0, description="Epsilon sampling cutoff. 0.0 = disabled. Filters tokens with probability below epsilon.")`
8. `eta_cutoff: float = Field(default=0.0, ge=0.0, le=1.0, description="Eta sampling cutoff for adaptive epsilon. 0.0 = disabled.")`

Add these after the existing `return_dict_in_generate` field but before the beam_search section.

Document hardware/version requirements in docstrings where applicable. Follow existing Field() pattern with ge/le constraints where appropriate.
  </action>
  <verify>
Run `python -c "from llenergymeasure.config.backend_configs import PyTorchConfig; print(len(PyTorchConfig.model_fields))"` - should show increased field count.
Run `python -c "from llenergymeasure.config.introspection import get_backend_params; print('guidance_scale' in str(get_backend_params('pytorch')))"` - should print True.
  </verify>
  <done>
PyTorchConfig has 8 new generation parameters. Introspection auto-discovers them. No import errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify extra escape hatch exists and document</name>
  <files>src/llenergymeasure/config/backend_configs.py</files>
  <action>
Verify that PyTorchConfig already has the `extra: dict[str, Any]` escape hatch field (it should exist from prior implementation).

If exists: Update the description to be more explicit about passthrough behaviour:
```python
extra: dict[str, Any] = Field(
    default_factory=dict,
    description="Escape hatch: kwargs passed directly to model.generate() without validation. "
    "Use for undocumented/niche parameters not yet in schema. "
    "Example: extra: {sequence_bias: [[token_ids], bias_value]}"
)
```

If NOT exists: Add it at the end of the class before the validators section.

The escape hatch is critical for PARAM-03 requirement (undocumented/niche parameter passthrough).
  </action>
  <verify>
Run `python -c "from llenergymeasure.config.backend_configs import PyTorchConfig; print('extra' in PyTorchConfig.model_fields)"` - should print True.
  </verify>
  <done>
PyTorchConfig.extra field exists with clear passthrough documentation.
  </done>
</task>

</tasks>

<verification>
- [ ] `python -c "from llenergymeasure.config.backend_configs import PyTorchConfig"` succeeds
- [ ] `ruff check src/llenergymeasure/config/backend_configs.py` passes
- [ ] `python -c "from llenergymeasure.config.introspection import get_backend_params; p=get_backend_params('pytorch'); print(len(p))"` shows increased param count
- [ ] New params have test_values auto-generated by introspection
</verification>

<success_criteria>
PyTorchConfig expanded with 8 new generation parameters. All parameters have docstrings with constraints. Introspection auto-discovers all new fields. Extra escape hatch documented.
</success_criteria>

<output>
After completion, create `.planning/phases/03-parameter-completeness/03-01-SUMMARY.md`
</output>
