---
phase: 03-parameter-completeness
plan: 05
type: execute
wave: 3
depends_on: ["03-04"]
files_modified:
  - configs/examples/pytorch_example.yaml
  - configs/examples/vllm_example.yaml
  - configs/examples/tensorrt_example.yaml
autonomous: true

must_haves:
  truths:
    - "Each backend example config demonstrates all commonly-used parameters"
    - "Example configs are valid and can be used for actual experiments"
    - "Configs serve as living documentation of backend capabilities"
  artifacts:
    - path: "configs/examples/pytorch_example.yaml"
      provides: "Comprehensive PyTorch example config"
      contains: "guidance_scale"
    - path: "configs/examples/vllm_example.yaml"
      provides: "Comprehensive vLLM example config"
      contains: "tokenizer_mode"
    - path: "configs/examples/tensorrt_example.yaml"
      provides: "Comprehensive TensorRT example config"
      contains: "gemm_plugin"
  key_links:
    - from: "configs/examples/*.yaml"
      to: "src/llenergymeasure/config/backend_configs.py"
      via: "Pydantic validation at load time"
      pattern: "ExperimentConfig"
---

<objective>
Create comprehensive example configs showcasing each backend's full parameter capabilities.

Purpose: Provide users with living documentation of what each backend can do, demonstrating all supported parameters (not just basics).
Output: Three comprehensive example configs that validate and serve as parameter reference.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-parameter-completeness/03-CONTEXT.md
@src/llenergymeasure/config/backend_configs.py
@configs/examples/pytorch_example.yaml
@configs/examples/vllm_example.yaml
@configs/examples/tensorrt_example.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update PyTorch example config</name>
  <files>configs/examples/pytorch_example.yaml</files>
  <action>
Read the existing pytorch_example.yaml and expand it to demonstrate comprehensive PyTorch capabilities.

Structure the config with clear sections and comments explaining each parameter group:

```yaml
# PyTorch/Transformers Backend - Comprehensive Example
# Demonstrates all major parameter categories for energy/throughput measurement

config_name: pytorch-comprehensive-example
model_name: Qwen/Qwen2.5-0.5B-Instruct
backend: pytorch
gpus: [0]

# Universal settings
fp_precision: float16
max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 50
streaming: false

# Decoder settings (universal)
decoder:
  preset: standard
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true

# PyTorch-specific settings
pytorch:
  # Batching (application-level)
  batch_size: 4
  batching_strategy: sorted_dynamic  # sorted_dynamic for efficiency
  max_tokens_per_batch: 2048         # Token budget for dynamic batching

  # Attention
  attn_implementation: sdpa          # sdpa | flash_attention_2 | eager

  # Compilation (torch.compile)
  torch_compile: false               # default | reduce-overhead | max-autotune
  # torch_compile_backend: inductor  # Uncomment if torch_compile enabled

  # KV Caching
  use_cache: true
  cache_implementation: dynamic      # dynamic | static | hybrid | sliding_window

  # Memory
  low_cpu_mem_usage: true
  low_memory: false                  # Memory-efficient generation mode

  # Quantization (BitsAndBytes) - uncomment ONE option
  # load_in_4bit: true
  # bnb_4bit_quant_type: nf4
  # bnb_4bit_use_double_quant: false

  # Sampling extensions
  min_p: 0.0                         # Minimum probability threshold
  no_repeat_ngram_size: 0            # N-gram repetition penalty (0 = disabled)
  typical_p: 1.0                     # Typical decoding (1.0 = disabled)

  # Advanced generation
  # guidance_scale: 1.5              # Classifier-free guidance (uncomment for CFG models)
  diversity_penalty: 0.0             # Beam search diversity (0 = disabled)
  output_scores: false               # Return generation scores
  return_dict_in_generate: false

  # Beam search (alternative to sampling)
  beam_search:
    enabled: false
    num_beams: 1
    length_penalty: 1.0
    early_stopping: false

  # Escape hatch for undocumented parameters
  # extra:
  #   sequence_bias: null
  #   pad_token_id: 50256

# Dataset
dataset:
  name: ai_energy_score
  sample_size: 50
```

Key principles:
- Show ALL major parameter categories
- Use comments to explain non-obvious parameters
- Provide sensible defaults that work out-of-box
- Show how to enable optional features (commented out)
  </action>
  <verify>
Run `lem config validate configs/examples/pytorch_example.yaml` - should pass validation.
  </verify>
  <done>
PyTorch example config demonstrates comprehensive parameter coverage with explanatory comments.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update vLLM example config</name>
  <files>configs/examples/vllm_example.yaml</files>
  <action>
Read the existing vllm_example.yaml and expand it to demonstrate comprehensive vLLM capabilities.

```yaml
# vLLM Backend - Comprehensive Example
# Demonstrates all major parameter categories for high-throughput inference

config_name: vllm-comprehensive-example
model_name: Qwen/Qwen2.5-0.5B-Instruct
backend: vllm
gpus: [0]

# Universal settings
fp_precision: float16
max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 100
streaming: false

# Decoder settings (universal)
decoder:
  preset: standard
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true

# vLLM-specific settings
vllm:
  # Memory & Concurrency (continuous batching)
  max_num_seqs: 256                    # Max concurrent sequences
  max_num_batched_tokens: 4096         # Max tokens per iteration
  gpu_memory_utilization: 0.9          # KV cache memory fraction
  swap_space: 4.0                      # CPU swap space (GiB)
  cpu_offload_gb: 0.0                  # CPU weight offload (GiB)

  # Model loading
  dtype: auto                          # auto | half | float16 | bfloat16
  tokenizer_mode: auto                 # auto | slow | mistral
  trust_remote_code: false             # Enable for custom architectures
  # revision: null                     # Model checkpoint revision
  # download_dir: null                 # Model download directory
  max_model_len: 2048                  # Max context length

  # KV Cache
  enable_prefix_caching: false         # Cache repeated prefixes
  enable_chunked_prefill: false        # Chunk large prefills
  kv_cache_dtype: auto                 # auto | fp8 (Hopper GPUs)
  block_size: 16                       # PagedAttention block size

  # Execution
  enforce_eager: false                 # Disable CUDA graphs
  # num_scheduler_steps: 1             # Multi-step scheduling (>1)

  # Parallelism (multi-GPU)
  tensor_parallel_size: 1              # Tensor parallelism
  pipeline_parallel_size: 1            # Pipeline parallelism
  distributed_backend: mp              # mp | ray

  # Attention
  attention:
    backend: auto                      # auto | FLASH_ATTN | FLASHINFER
    # flash_version: 2                 # 2 | 3 (3 for Hopper)
    disable_sliding_window: false

  # Sampling extensions
  min_p: 0.0                           # Minimum probability threshold
  logprobs: null                       # Return top-k logprobs (1-20)

  # Quantization (requires pre-quantized model)
  # quantization: awq                  # awq | gptq | fp8 | marlin
  load_format: auto                    # auto | safetensors

  # Speculative decoding (advanced)
  # speculative:
  #   model: null                      # Draft model path
  #   num_tokens: 5
  #   method: ngram

  # LoRA adapters
  # lora:
  #   enabled: false
  #   max_loras: 1
  #   max_rank: 16

  # Escape hatch
  # extra:
  #   seed: 42

# Dataset
dataset:
  name: ai_energy_score
  sample_size: 100
```
  </action>
  <verify>
Run `lem config validate configs/examples/vllm_example.yaml` - should pass validation.
  </verify>
  <done>
vLLM example config demonstrates comprehensive parameter coverage with explanatory comments.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update TensorRT example config</name>
  <files>configs/examples/tensorrt_example.yaml</files>
  <action>
Read the existing tensorrt_example.yaml and expand it to demonstrate comprehensive TensorRT-LLM capabilities.

```yaml
# TensorRT-LLM Backend - Comprehensive Example
# Demonstrates all major parameter categories for maximum performance

config_name: tensorrt-comprehensive-example
model_name: Qwen/Qwen2.5-0.5B-Instruct
backend: tensorrt
gpus: [0]

# Universal settings
fp_precision: float16                  # float32 NOT supported by TensorRT
max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 100
streaming: false

# Decoder settings (universal)
decoder:
  preset: deterministic                # TensorRT has limited sampling support
  temperature: 1.0
  top_p: 1.0
  top_k: 0                             # 0 = disabled

# TensorRT-specific settings
tensorrt:
  # Engine source
  engine_path: null                    # Pre-built engine path (optional)
  force_rebuild: false                 # Force engine rebuild
  engine_cache_dir: null               # Engine cache directory

  # Build configuration (compile-time)
  max_batch_size: 8                    # Max batch size for engine
  max_input_len: 512                   # Max input length
  max_output_len: 128                  # Max output length
  builder_opt_level: 3                 # Optimisation level (0-5)
  strongly_typed: true                 # Strong typing for FP8

  # Build plugins
  gemm_plugin: null                    # GEMM plugin precision
  gpt_attention_plugin: null           # Attention plugin precision
  use_paged_context_fmha: true         # Paged attention for context
  # use_fp8_context_fmha: false        # FP8 attention (Hopper only)
  multiple_profiles: false             # Multiple TRT profiles

  # Parallelism
  tp_size: 1                           # Tensor parallel size
  pp_size: 1                           # Pipeline parallel size
  use_custom_all_reduce: true          # Custom NCCL kernels

  # Quantization
  quantization: none                   # none | fp8 | int8_sq | int8_weight_only | int4_awq
  # calibration:                       # Required for int8_sq
  #   dataset: wikitext
  #   num_samples: 512
  #   max_length: 2048

  # Runtime
  kv_cache_type: paged                 # paged | continuous
  enable_chunked_context: true         # Chunked context for long sequences
  enable_kv_cache_reuse: false         # Prefix caching
  gpu_memory_utilization: 0.9          # KV cache memory fraction
  max_num_tokens: null                 # Max tokens per iteration
  batching_type: inflight              # inflight | static

  # Speculative decoding
  # draft_model: null                  # Draft model path
  # num_draft_tokens: 5
  # max_draft_len: 5

  # Advanced build options
  # weight_sparsity: false             # Sparse weights
  # weight_streaming: false            # CPU weight offload
  # strip_plan: false                  # Strip weights from engine

  # Escape hatches
  # extra_build_args: {}
  # extra_runtime_args: {}

# Dataset
dataset:
  name: ai_energy_score
  sample_size: 100
```
  </action>
  <verify>
Run `lem config validate configs/examples/tensorrt_example.yaml` - should pass validation.
  </verify>
  <done>
TensorRT example config demonstrates comprehensive parameter coverage with explanatory comments.
  </done>
</task>

</tasks>

<verification>
- [ ] `lem config validate configs/examples/pytorch_example.yaml` passes
- [ ] `lem config validate configs/examples/vllm_example.yaml` passes
- [ ] `lem config validate configs/examples/tensorrt_example.yaml` passes
- [ ] Each config has comments explaining parameter categories
- [ ] Configs use sensible defaults that work out-of-box
</verification>

<success_criteria>
All three example configs are comprehensive, validated, and serve as living documentation of backend capabilities. Each shows all major parameter categories with explanatory comments.
</success_criteria>

<output>
After completion, create `.planning/phases/03-parameter-completeness/03-05-SUMMARY.md`
</output>
