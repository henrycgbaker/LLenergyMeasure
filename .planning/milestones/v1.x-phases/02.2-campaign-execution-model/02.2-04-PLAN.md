---
phase: 02.2-campaign-execution-model
plan: 04
type: execute
wave: 3
depends_on: ["02.2-01", "02.2-02", "02.2-03"]
files_modified:
  - tests/unit/test_campaign_context.py
  - tests/unit/test_container_strategy.py
  - tests/unit/test_user_config.py
autonomous: false

must_haves:
  truths:
    - "Unit tests verify campaign context propagation via environment variables"
    - "Unit tests verify user config loading with missing file gracefully returns defaults"
    - "Unit tests verify container strategy selection (CLI > config > default)"
    - "All Phase 2.2 success criteria manually verified"
  artifacts:
    - path: "tests/unit/test_campaign_context.py"
      provides: "Tests for campaign context environment variables"
      contains: "LEM_CAMPAIGN_ID"
    - path: "tests/unit/test_user_config.py"
      provides: "Tests for user config loading"
      contains: "load_user_config"
    - path: "tests/unit/test_container_strategy.py"
      provides: "Tests for container strategy selection"
      contains: "ContainerManager"
  key_links:
    - from: "tests/unit/test_campaign_context.py"
      to: "src/llenergymeasure/cli/campaign.py"
      via: "testing campaign context propagation"
      pattern: "def test_campaign_context"
---

<objective>
Add unit tests for Phase 2.2 features and perform UAT verification.

Purpose: Validate that all Phase 2.2 features work correctly: campaign context propagation, user config loading, container strategy selection, and container routing. This plan creates unit tests and includes a human verification checkpoint for end-to-end testing.

Output: Unit tests for Phase 2.2 features; UAT verification of all success criteria.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02.2-CONTEXT.md

@tests/CLAUDE.md
@src/llenergymeasure/config/user_config.py
@src/llenergymeasure/orchestration/container.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unit tests for user config loading</name>
  <files>tests/unit/test_user_config.py</files>
  <action>
Create unit tests for the user config loading module:

```python
"""Tests for user configuration loading."""

from pathlib import Path
from tempfile import NamedTemporaryFile

import pytest

from llenergymeasure.config.user_config import (
    DockerConfig,
    ThermalGapConfig,
    UserConfig,
    load_user_config,
)


class TestUserConfig:
    """Tests for UserConfig model."""

    def test_default_values(self) -> None:
        """UserConfig has sensible defaults."""
        config = UserConfig()

        assert config.thermal_gaps.between_experiments == 60.0
        assert config.thermal_gaps.between_cycles == 300.0
        assert config.docker.strategy == "ephemeral"
        assert config.docker.warmup_delay == 0.0
        assert config.docker.auto_teardown is True
        assert config.default_backend == "pytorch"
        assert config.results_dir == "results"

    def test_thermal_gaps_config(self) -> None:
        """ThermalGapConfig validates values."""
        config = ThermalGapConfig(between_experiments=30.0, between_cycles=120.0)
        assert config.between_experiments == 30.0
        assert config.between_cycles == 120.0

    def test_docker_config_strategy(self) -> None:
        """DockerConfig validates strategy values."""
        config = DockerConfig(strategy="persistent")
        assert config.strategy == "persistent"

        config = DockerConfig(strategy="ephemeral")
        assert config.strategy == "ephemeral"

    def test_docker_config_invalid_strategy(self) -> None:
        """DockerConfig rejects invalid strategy."""
        with pytest.raises(ValueError, match="Input should be 'ephemeral' or 'persistent'"):
            DockerConfig(strategy="invalid")


class TestLoadUserConfig:
    """Tests for load_user_config function."""

    def test_missing_file_returns_defaults(self) -> None:
        """Missing config file returns defaults without error."""
        config = load_user_config(Path("/nonexistent/path.yaml"))

        assert config.thermal_gaps.between_experiments == 60.0
        assert config.docker.strategy == "ephemeral"

    def test_load_valid_config(self, tmp_path: Path) -> None:
        """Valid config file is loaded correctly."""
        config_path = tmp_path / ".lem-config.yaml"
        config_path.write_text("""
thermal_gaps:
  between_experiments: 30
  between_cycles: 180
docker:
  strategy: persistent
  warmup_delay: 5.0
default_backend: vllm
""")

        config = load_user_config(config_path)

        assert config.thermal_gaps.between_experiments == 30.0
        assert config.thermal_gaps.between_cycles == 180.0
        assert config.docker.strategy == "persistent"
        assert config.docker.warmup_delay == 5.0
        assert config.default_backend == "vllm"

    def test_partial_config_uses_defaults(self, tmp_path: Path) -> None:
        """Partial config file fills in missing values with defaults."""
        config_path = tmp_path / ".lem-config.yaml"
        config_path.write_text("""
thermal_gaps:
  between_experiments: 45
""")

        config = load_user_config(config_path)

        assert config.thermal_gaps.between_experiments == 45.0
        assert config.thermal_gaps.between_cycles == 300.0  # Default
        assert config.docker.strategy == "ephemeral"  # Default

    def test_empty_file_returns_defaults(self, tmp_path: Path) -> None:
        """Empty config file returns defaults."""
        config_path = tmp_path / ".lem-config.yaml"
        config_path.write_text("")

        config = load_user_config(config_path)

        assert config.thermal_gaps.between_experiments == 60.0
        assert config.docker.strategy == "ephemeral"

    def test_invalid_yaml_returns_defaults(self, tmp_path: Path) -> None:
        """Invalid YAML returns defaults without crashing."""
        config_path = tmp_path / ".lem-config.yaml"
        config_path.write_text("invalid: yaml: content: [")

        config = load_user_config(config_path)

        assert config.thermal_gaps.between_experiments == 60.0
        assert config.docker.strategy == "ephemeral"
```
  </action>
  <verify>
```bash
pytest tests/unit/test_user_config.py -v
```
  </verify>
  <done>Unit tests for user config loading covering defaults, valid config, partial config, empty file, and invalid YAML cases.</done>
</task>

<task type="auto">
  <name>Task 2: Create unit tests for ContainerManager</name>
  <files>tests/unit/test_container_strategy.py</files>
  <action>
Create unit tests for the ContainerManager class:

```python
"""Tests for container strategy and ContainerManager."""

from unittest.mock import MagicMock, patch

import pytest

from llenergymeasure.orchestration.container import ContainerManager, ContainerState


class TestContainerState:
    """Tests for ContainerState dataclass."""

    def test_initial_state(self) -> None:
        """ContainerState initializes correctly."""
        state = ContainerState(service="pytorch", status="stopped")
        assert state.service == "pytorch"
        assert state.status == "stopped"
        assert state.restart_count == 0


class TestContainerManager:
    """Tests for ContainerManager class."""

    def test_initialization(self) -> None:
        """ContainerManager initializes with services."""
        manager = ContainerManager(services=["pytorch", "vllm"])

        assert manager.services == ["pytorch", "vllm"]
        assert "pytorch" in manager._states
        assert "vllm" in manager._states
        assert manager._states["pytorch"].status == "stopped"
        assert manager._started is False

    def test_unknown_service_raises(self) -> None:
        """exec() with unknown service raises ValueError."""
        manager = ContainerManager(services=["pytorch"])

        with pytest.raises(ValueError, match="Unknown service"):
            manager.exec("vllm", ["echo", "test"])

    @patch("subprocess.run")
    def test_start_all_success(self, mock_run: MagicMock) -> None:
        """start_all() calls docker compose up."""
        mock_run.return_value = MagicMock(returncode=0)

        manager = ContainerManager(services=["pytorch", "vllm"])
        result = manager.start_all()

        assert result is True
        assert manager._started is True
        assert manager._states["pytorch"].status == "running"
        mock_run.assert_called_once()
        call_args = mock_run.call_args[0][0]
        assert "docker" in call_args
        assert "compose" in call_args
        assert "up" in call_args
        assert "-d" in call_args

    @patch("subprocess.run")
    def test_start_all_failure(self, mock_run: MagicMock) -> None:
        """start_all() returns False on failure."""
        mock_run.return_value = MagicMock(returncode=1, stderr="Error")

        manager = ContainerManager(services=["pytorch"])
        result = manager.start_all()

        assert result is False
        assert manager._started is False
        assert manager._states["pytorch"].status == "error"

    @patch("subprocess.run")
    def test_stop_all(self, mock_run: MagicMock) -> None:
        """stop_all() calls docker compose down."""
        mock_run.return_value = MagicMock(returncode=0)

        manager = ContainerManager(services=["pytorch"])
        manager._started = True
        manager._states["pytorch"].status = "running"

        result = manager.stop_all()

        assert result is True
        assert manager._started is False
        assert manager._states["pytorch"].status == "stopped"

    @patch("subprocess.run")
    def test_exec_builds_correct_command(self, mock_run: MagicMock) -> None:
        """exec() builds correct docker compose exec command."""
        mock_run.return_value = MagicMock(returncode=0)

        manager = ContainerManager(services=["pytorch"])
        manager._started = True
        manager._states["pytorch"].status = "running"

        manager.exec("pytorch", ["lem", "experiment", "config.yaml"])

        call_args = mock_run.call_args[0][0]
        assert call_args[:3] == ["docker", "compose", "exec"]
        assert "pytorch" in call_args
        assert "lem" in call_args

    @patch("subprocess.run")
    def test_exec_with_env_vars(self, mock_run: MagicMock) -> None:
        """exec() passes environment variables with -e flags."""
        mock_run.return_value = MagicMock(returncode=0)

        manager = ContainerManager(services=["pytorch"])
        manager._started = True
        manager._states["pytorch"].status = "running"

        manager.exec(
            "pytorch",
            ["lem", "experiment"],
            env={"LEM_CAMPAIGN_ID": "abc123"},
        )

        call_args = mock_run.call_args[0][0]
        assert "-e" in call_args
        assert "LEM_CAMPAIGN_ID=abc123" in call_args

    @patch("subprocess.run")
    def test_context_manager(self, mock_run: MagicMock) -> None:
        """ContainerManager works as context manager."""
        mock_run.return_value = MagicMock(returncode=0)

        with ContainerManager(services=["pytorch"]) as manager:
            assert manager._started is True

        # stop_all called on exit
        assert mock_run.call_count == 2  # start + stop

    @patch("subprocess.run")
    def test_restart_service(self, mock_run: MagicMock) -> None:
        """restart_service() increments restart count."""
        mock_run.return_value = MagicMock(returncode=0)

        manager = ContainerManager(services=["pytorch"])
        manager._states["pytorch"].status = "running"

        result = manager.restart_service("pytorch")

        assert result is True
        assert manager._states["pytorch"].restart_count == 1
```
  </action>
  <verify>
```bash
pytest tests/unit/test_container_strategy.py -v
```
  </verify>
  <done>Unit tests for ContainerManager covering initialization, start_all, stop_all, exec, env var passing, context manager, and restart functionality.</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for campaign context propagation</name>
  <files>tests/unit/test_campaign_context.py</files>
  <action>
Create unit tests for campaign context propagation:

```python
"""Tests for campaign context propagation."""

import os
from unittest.mock import MagicMock, patch

import pytest


class TestCampaignContextEnvironment:
    """Tests for campaign context environment variables."""

    def test_campaign_context_detection(self) -> None:
        """Experiment detects campaign context from environment."""
        # Simulate running inside a campaign
        with patch.dict(os.environ, {
            "LEM_CAMPAIGN_ID": "abc12345",
            "LEM_CAMPAIGN_NAME": "test-campaign",
            "LEM_CYCLE": "2",
            "LEM_TOTAL_CYCLES": "3",
        }):
            campaign_id = os.environ.get("LEM_CAMPAIGN_ID")
            campaign_name = os.environ.get("LEM_CAMPAIGN_NAME")
            cycle = os.environ.get("LEM_CYCLE")
            total_cycles = os.environ.get("LEM_TOTAL_CYCLES")

            assert campaign_id == "abc12345"
            assert campaign_name == "test-campaign"
            assert cycle == "2"
            assert total_cycles == "3"

    def test_no_campaign_context(self) -> None:
        """Without campaign context, env vars are None."""
        # Ensure env vars are not set
        env = os.environ.copy()
        for key in ["LEM_CAMPAIGN_ID", "LEM_CAMPAIGN_NAME", "LEM_CYCLE", "LEM_TOTAL_CYCLES"]:
            env.pop(key, None)

        with patch.dict(os.environ, env, clear=True):
            assert os.environ.get("LEM_CAMPAIGN_ID") is None

    def test_in_campaign_detection_logic(self) -> None:
        """in_campaign flag correctly detects campaign context."""
        # With campaign context
        with patch.dict(os.environ, {"LEM_CAMPAIGN_ID": "abc123"}):
            in_campaign = os.environ.get("LEM_CAMPAIGN_ID") is not None
            assert in_campaign is True

        # Without campaign context (ensure it's cleared)
        original = os.environ.pop("LEM_CAMPAIGN_ID", None)
        try:
            in_campaign = os.environ.get("LEM_CAMPAIGN_ID") is not None
            assert in_campaign is False
        finally:
            if original:
                os.environ["LEM_CAMPAIGN_ID"] = original


class TestCampaignContextInDocker:
    """Tests for campaign context in Docker commands."""

    def test_docker_command_includes_env_vars(self) -> None:
        """Docker command should include campaign context env vars."""
        # This is a design test - verify the expected command structure
        campaign_context = {
            "LEM_CAMPAIGN_ID": "abc123",
            "LEM_CAMPAIGN_NAME": "test",
            "LEM_CYCLE": "1",
            "LEM_TOTAL_CYCLES": "3",
        }

        # Expected command structure with -e flags
        cmd = ["docker", "compose", "run", "--rm"]
        for key, value in campaign_context.items():
            cmd.extend(["-e", f"{key}={value}"])
        cmd.extend(["pytorch", "lem", "experiment", "config.yaml"])

        # Verify structure
        assert "-e" in cmd
        assert "LEM_CAMPAIGN_ID=abc123" in cmd
        assert "LEM_CYCLE=1" in cmd
        assert "pytorch" in cmd


class TestCycleWarningSuppress:
    """Tests for cycle warning suppression in campaign context."""

    def test_should_show_campaign_context_not_warning(self) -> None:
        """When in campaign, show campaign context instead of warning."""
        with patch.dict(os.environ, {
            "LEM_CAMPAIGN_ID": "abc123",
            "LEM_CAMPAIGN_NAME": "my-campaign",
            "LEM_CYCLE": "2",
            "LEM_TOTAL_CYCLES": "5",
        }):
            in_campaign = os.environ.get("LEM_CAMPAIGN_ID") is not None
            assert in_campaign is True

            # Should show: "Part of campaign: my-campaign (cycle 2/5)"
            # NOT: "Single cycle: confidence intervals require >= 3 cycles"
            expected_message = "Part of campaign"
            assert in_campaign  # Indicates warning should be suppressed

    def test_should_show_warning_when_not_in_campaign(self) -> None:
        """When not in campaign, show single cycle warning."""
        # Clear campaign context
        env = {k: v for k, v in os.environ.items() if not k.startswith("LEM_")}
        with patch.dict(os.environ, env, clear=True):
            in_campaign = os.environ.get("LEM_CAMPAIGN_ID") is not None
            assert in_campaign is False
            # Should show single cycle warning
```
  </action>
  <verify>
```bash
pytest tests/unit/test_campaign_context.py -v
```
  </verify>
  <done>Unit tests for campaign context environment variables, detection logic, Docker command structure, and cycle warning suppression.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Phase 2.2 Campaign Execution Model implementation:
1. Campaign context propagation via environment variables (LEM_CAMPAIGN_ID, LEM_CYCLE, etc.)
2. CI warning suppression for experiments running in campaigns
3. --cycles flag removed from experiment CLI
4. Container routing fix (TensorRT -> tensorrt service)
5. User config loading from .lem-config.yaml with thermal gap defaults
6. Dual container strategy: ephemeral (default) and persistent (opt-in)
7. --container-strategy CLI flag
8. ContainerManager for persistent container lifecycle
  </what-built>
  <how-to-verify>
**Success Criteria Verification:**

1. **TensorRT routing**: Create a tensorrt experiment config and verify it routes to correct container:
   ```bash
   # Check that backend='tensorrt' in config routes to tensorrt service
   grep -n "tensorrt" src/llenergymeasure/cli/campaign.py
   ```

2. **Campaign context display**: Run a campaign with --dry-run and verify context would be passed:
   ```bash
   lem campaign configs/examples/pytorch_example.yaml \
       --campaign-name test --cycles 3 --dry-run
   # Should show execution plan without "single cycle" warnings
   ```

3. **--cycles flag removed**: Verify --cycles is not in experiment help:
   ```bash
   lem experiment --help | grep -i "cycle"
   # Should NOT show --cycles option
   ```

4. **User config loading**: Create a test .lem-config.yaml and verify it's loaded:
   ```bash
   echo "thermal_gaps:" > .lem-config.yaml
   echo "  between_experiments: 30" >> .lem-config.yaml
   python -c "from llenergymeasure.config.user_config import load_user_config; c = load_user_config(); print(c.thermal_gaps.between_experiments)"
   # Should print 30.0
   rm .lem-config.yaml
   ```

5. **Container strategy CLI flag**:
   ```bash
   lem campaign --help | grep -i "container-strategy"
   # Should show --container-strategy option
   ```

6. **Unit tests pass**:
   ```bash
   pytest tests/unit/test_user_config.py tests/unit/test_container_strategy.py tests/unit/test_campaign_context.py -v
   ```

**Optional E2E test** (if Docker available):
- Run a small campaign with 2 cycles to verify campaign context is actually passed
- Check experiment output shows "Part of campaign X (cycle Y/Z)"
  </how-to-verify>
  <resume-signal>Type "approved" if all success criteria verified, or describe any issues found.</resume-signal>
</task>

</tasks>

<verification>
1. All unit tests pass: `pytest tests/unit/test_user_config.py tests/unit/test_container_strategy.py tests/unit/test_campaign_context.py -v`
2. Linting passes: `ruff check tests/unit/test_*.py`
3. Formatting passes: `ruff format tests/unit/test_*.py`
4. Human verification of all 6 success criteria from ROADMAP.md
</verification>

<success_criteria>
- Unit tests created for user config loading (test_user_config.py)
- Unit tests created for ContainerManager (test_container_strategy.py)
- Unit tests created for campaign context propagation (test_campaign_context.py)
- All unit tests pass
- Human verification confirms all Phase 2.2 success criteria are met
</success_criteria>

<output>
After completion, create `.planning/phases/02.2-campaign-execution-model/02.2-04-SUMMARY.md`
</output>
