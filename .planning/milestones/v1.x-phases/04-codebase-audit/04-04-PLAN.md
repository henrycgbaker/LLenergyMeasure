---
phase: 04-codebase-audit
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/04-codebase-audit/04-04-REPORT-orchestration-results.md
autonomous: true

must_haves:
  truths:
    - "Orchestration layer (runner, factory, lifecycle, launcher, context) assessed for complexity and necessity"
    - "Campaign system (campaign.py, grid.py, manifest.py, container.py) evaluated against industry patterns"
    - "Results system (aggregation, exporters, bootstrap, timeseries, repository) assessed for completeness and wiring"
    - "State management (experiment_state.py) evaluated for over-engineering"
    - "Domain models (experiment.py, metrics.py, environment.py, model_info.py) checked for bloat and accuracy"
    - "All findings documented with severity classification and industry comparison"
  artifacts:
    - path: ".planning/phases/04-codebase-audit/04-04-REPORT-orchestration-results.md"
      provides: "Orchestration, campaign, results, state, and domain audit"
  key_links: []
---

<objective>
Audit the orchestration layer, campaign system, results pipeline, state management, and domain models for over-engineering and correctness.

Purpose: The orchestration layer is the most complex part of the codebase (campaign.py alone is 1754 lines). Industry research suggests comparable tools don't have built-in campaign orchestration. This audit determines what's essential and what's over-engineered.

Output: Report section covering (1) orchestration complexity assessment, (2) campaign system evaluation, (3) results pipeline audit, (4) state management assessment, (5) domain model review.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/agents/gsd-planner.md
</execution_context>

<context>
@.planning/phases/04-codebase-audit/04-CONTEXT.md
@.planning/phases/04-codebase-audit/04-RESEARCH.md
@.planning/ROADMAP.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Orchestration and campaign system audit</name>
  <files>.planning/phases/04-codebase-audit/04-04-REPORT-orchestration-results.md</files>
  <action>
Audit the orchestration layer and campaign system:

1. **Orchestration modules:**
   Read and assess each module:
   - `orchestration/runner.py` (471 lines) — ExperimentOrchestrator
     - What does it coordinate? Is the abstraction necessary?
     - How many responsibilities does it have?
     - Compare to lm-eval-harness's evaluator.py
   - `orchestration/factory.py` (179 lines) — Component factory
     - Is dependency injection justified? How many components are injected?
     - Compare to direct instantiation in nanoGPT
   - `orchestration/lifecycle.py` (151 lines) — Experiment lifecycle
     - State transitions documented? Over-abstracted?
   - `orchestration/launcher.py` (849 lines) — Subprocess launch
     - THIS IS CRITICAL: Trace the launch path for accelerate/torchrun
     - Is subprocess launching necessary or an archaeological layer?
     - How does lm-eval-harness handle distributed launch?
   - `orchestration/context.py` (262 lines) — Execution context
     - What context does this provide? Is it used?
   - `orchestration/__init__.py` (45 lines) — Public API

2. **Campaign system (the biggest module):**
   - `orchestration/campaign.py` (585 lines) — Campaign runner
   - `cli/campaign.py` (1754 lines) — Campaign CLI (LARGEST FILE)
     - WHY is the CLI layer 1754 lines? This is unusual.
     - What functionality is in the CLI that should be in the runner?
     - Is there significant logic duplication between campaign.py CLI and orchestration?
   - `orchestration/grid.py` (363 lines) — Grid expansion
   - `orchestration/manifest.py` (194 lines) — Campaign manifest
   - `orchestration/container.py` (253 lines) — Container management

   **Industry comparison (CRITICAL):**
   - Research: Do lm-eval-harness, vLLM benchmarks, LLMPerf have campaign/grid systems?
   - Search GitHub repos for campaign, grid, sweep functionality
   - Compare against Hydra sweeps, W&B Sweeps (external orchestration tools)
   - "Industry does X, we do Y" assessment

3. **Trace the campaign execution path:**
   - lem campaign config.yaml -> ... -> result files
   - Document every module touched
   - Count total lines of code in the campaign path
   - Compare to experiment path — what's the overhead?

4. **Assess campaign system verdict:**
   - Is campaign functionality core to energy measurement?
   - Could users achieve the same with a bash loop + experiment command?
   - Recommendation: keep as-is / simplify heavily / extract to separate tool / remove

Document all findings with module-level references and severity classifications.
  </action>
  <verify>
Report contains orchestration layer assessment with per-module evaluation, campaign system industry comparison, execution path trace for campaign workflow, and clear recommendation on campaign system future.
  </verify>
  <done>Orchestration and campaign systems fully assessed, with evidence-based recommendation on simplification.</done>
</task>

<task type="auto">
  <name>Task 2: Results, state, and domain model audit</name>
  <files>.planning/phases/04-codebase-audit/04-04-REPORT-orchestration-results.md</files>
  <action>
Audit the results pipeline, state management, and domain models:

1. **Results system:**
   - `results/aggregation.py` (758 lines) — Result aggregation
     - What aggregation is performed? Is it all used?
     - Late aggregation pattern: is it working as designed?
     - Compare to lm-eval-harness result handling (simple JSON/CSV output)
   - `results/exporters.py` (338 lines) — Export to CSV/JSON
     - What formats supported? All functional?
   - `results/bootstrap.py` (118 lines) — Bootstrap CI
     - Is this used? Where is it called from?
   - `results/timeseries.py` (201 lines) — Time-series data
     - Is timeseries wired up end-to-end?
     - CONTEXT.md mentions timeseries as potentially unwired
   - `results/repository.py` (220 lines) — Result storage
     - File-based storage: is it over-abstracted?
   - `results/__init__.py` (31 lines) — Public API

2. **State management:**
   - `state/experiment_state.py` (422 lines) — State machine
     - What states exist? How many transitions?
     - Is a state machine justified for what's essentially: running -> done | failed?
     - Compare to how nanoGPT tracks experiment state (it doesn't — just checkpoints)
     - Flag if over-engineered for current usage

3. **Domain models:**
   - `domain/experiment.py` (255 lines) — Experiment model
     - Schema v3 assessment: are all fields used?
   - `domain/metrics.py` (692 lines) — Metrics models
     - This is large for domain models. What's in here?
     - Are all metric fields populated by actual measurement code?
     - Check for fields that exist in the model but are never set
   - `domain/environment.py` (132 lines) — Environment capture
     - Is environment data actually captured and stored?
   - `domain/model_info.py` (80 lines) — Model metadata

4. **Top-level modules:**
   Assess necessity of these root-level modules:
   - `protocols.py` (176 lines) — Protocol definitions. Used?
   - `resilience.py` (97 lines) — Retry/resilience. Used?
   - `security.py` (90 lines) — Security checks. What security?
   - `exceptions.py` (112 lines) — Custom exceptions. How many used?
   - `constants.py` (312 lines) — Constants. Are all referenced?
   - `progress.py` (250 lines) — Progress tracking. Used where?
   - `logging.py` (240 lines) — Logging setup. Necessary complexity?
   - `notifications/webhook.py` (114 lines) — Webhook sender. Used?

**For each module produce status:**
| Module | Lines | Assessment | Reasoning |
|--------|-------|------------|-----------|

Append all findings to the report.
  </action>
  <verify>
Report contains results pipeline assessment, state management evaluation, domain model field audit, and top-level module assessments. Each module has a clear keep/simplify/remove recommendation.
  </verify>
  <done>Results pipeline, state management, domain models, and top-level modules all assessed with severity classifications.</done>
</task>

</tasks>

<verification>
- Report file exists at .planning/phases/04-codebase-audit/04-04-REPORT-orchestration-results.md
- Orchestration layer has per-module assessments
- Campaign system has industry comparison and recommendation
- Results pipeline traced end-to-end
- State management complexity evaluated
- Domain model fields checked for usage
- Top-level modules assessed for necessity
</verification>

<success_criteria>
- Campaign system assessed with clear recommendation (backed by industry evidence)
- Orchestration complexity quantified (total lines, number of abstraction layers)
- Results pipeline wiring verified or gaps identified
- All top-level modules classified with keep/simplify/remove
</success_criteria>

<output>
After completion, create `.planning/phases/04-codebase-audit/04-04-SUMMARY.md`
</output>
