---
phase: 04-codebase-audit
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/04-codebase-audit/04-02-REPORT-cli-config.md
autonomous: true

must_haves:
  truths:
    - "Every CLI command and subcommand has been catalogued with its purpose, usage frequency assessment, and industry comparison"
    - "Every Pydantic config field has been checked for end-to-end wiring (defined in model -> loaded -> used in execution -> appears in results)"
    - "Config inheritance and precedence chain documented (CLI > config file > preset > defaults)"
    - "SSOT introspection system evaluated for accuracy and maintainability"
    - "Unwired config fields identified with evidence of where the chain breaks"
  artifacts:
    - path: ".planning/phases/04-codebase-audit/04-02-REPORT-cli-config.md"
      provides: "CLI surface audit + configuration system audit"
  key_links: []
---

<objective>
Audit the CLI command surface and configuration system for over-engineering, unwired fields, and gaps versus industry norms.

Purpose: The CLI is the user's primary interface; config is the primary input format. Identifying unnecessary commands, unwired config fields, and over-complex config processing directly improves user experience and reduces maintenance burden.

Output: Report section covering (1) CLI command surface audit with per-command assessment, (2) config model field wiring audit, (3) SSOT introspection evaluation, (4) config loader/validation complexity assessment.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/agents/gsd-planner.md
</execution_context>

<context>
@.planning/phases/04-codebase-audit/04-CONTEXT.md
@.planning/phases/04-codebase-audit/04-RESEARCH.md
@.planning/ROADMAP.md
@src/llenergymeasure/cli/__init__.py
@src/llenergymeasure/config/models.py
@src/llenergymeasure/config/introspection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: CLI command surface audit</name>
  <files>.planning/phases/04-codebase-audit/04-02-REPORT-cli-config.md</files>
  <action>
Systematically audit every CLI command and subcommand:

1. **Catalogue all commands** by reading:
   - `src/llenergymeasure/cli/__init__.py` (app registration)
   - Each CLI module: experiment.py, campaign.py, config.py, results.py, doctor.py, init_cmd.py, resume.py, batch.py, schedule.py, listing.py, lifecycle.py

2. **For each command, document:**
   - Command name and full invocation syntax
   - Purpose (one sentence)
   - Implementation status: fully functional / partially functional / stub / dead
   - Lines of code in the implementing module
   - Which phase introduced it
   - Whether comparable tools have equivalent functionality
   - Assessment: keep / simplify / remove (with reasoning)

3. **Identify command groups that could be merged or removed:**
   - Are `experiment` and `campaign` distinct enough to justify separate commands?
   - Is `batch` / `schedule` used or functional?
   - Does `config` have too many subcommands?
   - Is `resume` justified vs a flag on `campaign`?

4. **Execution path analysis:**
   - Trace the path from `lem experiment <config>` to result output. Document each module touched.
   - Trace the path from `lem campaign <config>` to result output. Document each module touched.
   - Identify divergence points and dead branches in these paths.

5. **Compare against industry research** from 04-RESEARCH.md:
   - "Industry does X, we do Y" for each command group
   - Flag commands that no comparable tool offers (unless justified by energy measurement domain)

Document findings in structured format with clear recommendations.
  </action>
  <verify>
Report contains a complete command catalogue table, execution path diagrams for experiment and campaign paths, and per-command keep/simplify/remove recommendations with evidence.
  </verify>
  <done>Every CLI command catalogued, execution paths traced, and recommendations made with industry comparison evidence.</done>
</task>

<task type="auto">
  <name>Task 2: Configuration system audit</name>
  <files>.planning/phases/04-codebase-audit/04-02-REPORT-cli-config.md</files>
  <action>
Audit the configuration system end-to-end:

1. **Config model field wiring audit:**
   Read these files and trace every field:
   - `config/models.py` (UniversalConfig, GenerationConfig, WarmupConfig, BaselineConfig, TimeSeriesConfig, etc.)
   - `config/backend_configs.py` (PyTorchConfig, VLLMConfig, TensorRTConfig)
   - `config/campaign_config.py` (CampaignConfig, GridConfig, etc.)
   - `config/user_config.py` (UserConfig)

   For each config field, check:
   - Is it loaded from YAML? (check config/loader.py)
   - Is it used in execution code? (grep for field name in core/, orchestration/)
   - Does it appear in results output? (check results/, domain/)
   - Is it validated? (check config/validation.py)
   - Is it covered by SSOT introspection? (check config/introspection.py)

   Produce a wiring status table:
   | Field | Model | Loaded | Used | In Results | Validated | Introspected | Status |
   Focus on fields that are DEFINED but NOT USED (unwired).

2. **Config loader complexity:**
   Read `config/loader.py` (488 lines) and assess:
   - How many distinct loading paths exist?
   - Is there unnecessary complexity in merging/inheritance?
   - Compare against lm-eval-harness config loading (simple YAML load + CLI override)

3. **SSOT introspection evaluation:**
   Read `config/introspection.py` (851 lines) and assess:
   - What does it actually provide? (list functions and their consumers)
   - Is it genuinely SSOT or does it have hand-maintained mappings?
   - How much of it is boilerplate vs genuine logic?
   - Could it be simplified while maintaining correctness?
   - Note: CONTEXT.md says to KEEP introspection â€” focus on simplification, not removal

4. **Supplementary config modules:**
   Check these for necessity and wiring:
   - `config/quantization.py` (125 lines)
   - `config/speculative.py` (105 lines)
   - `config/provenance.py` (226 lines)
   - `config/naming.py` (304 lines)
   - `config/validation.py` (176 lines)
   - `config/backend_detection.py` (58 lines)
   - `config/docker_detection.py` (58 lines)
   - `config/env_setup.py` (68 lines)

   For each: Is it imported? By what? Is the functionality used end-to-end?

Append all findings to the report, with clear "unwired" vs "dead" vs "functional" classifications.
  </action>
  <verify>
Report contains config field wiring table covering ALL Pydantic model fields, loader complexity assessment, introspection evaluation, and per-module assessment for supplementary config modules.
  </verify>
  <done>Every config field traced through the system, unwired fields identified, config complexity assessed with industry comparison.</done>
</task>

</tasks>

<verification>
- Report file exists at .planning/phases/04-codebase-audit/04-02-REPORT-cli-config.md
- CLI command catalogue is complete (every command listed)
- Config field wiring audit covers all Pydantic model fields
- SSOT introspection has been evaluated
- Findings classified with industry comparison evidence
</verification>

<success_criteria>
- Complete CLI surface map with keep/simplify/remove recommendations
- Config field wiring table identifying all unwired fields
- Execution path diagrams for main workflows
- Industry comparison evidence for all recommendations
</success_criteria>

<output>
After completion, create `.planning/phases/04-codebase-audit/04-02-SUMMARY.md`
</output>
