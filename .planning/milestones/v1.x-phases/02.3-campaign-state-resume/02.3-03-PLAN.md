---
phase: 02.3-campaign-state-resume
plan: 03
type: execute
wave: 2
depends_on: ["02.3-01", "02.3-02"]
files_modified:
  - tests/unit/cli/test_resume.py
  - tests/unit/cli/test_init_cmd.py
  - tests/unit/notifications/test_webhook.py
  - tests/unit/config/test_user_config.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Unit tests verify resume command discovers manifests correctly"
    - "Unit tests verify init wizard creates valid config"
    - "Unit tests verify webhook sender handles errors gracefully"
    - "Unit tests verify UserConfig validation rejects invalid configs"
  artifacts:
    - path: "tests/unit/cli/test_resume.py"
      provides: "Resume command unit tests"
      min_lines: 50
    - path: "tests/unit/cli/test_init_cmd.py"
      provides: "Init wizard unit tests"
      min_lines: 40
    - path: "tests/unit/notifications/test_webhook.py"
      provides: "Webhook sender unit tests"
      min_lines: 40
  key_links:
    - from: "tests/unit/cli/test_resume.py"
      to: "cli/resume.py"
      via: "import and invoke"
      pattern: "from llenergymeasure.cli.resume import"
---

<objective>
Create unit tests for Phase 2.3 components: resume command, init wizard, webhook sender.

Purpose: Verify all new functionality works correctly and handles edge cases (no state, invalid config, network errors).

Output: Comprehensive unit test coverage for Phase 2.3 additions.
</objective>

<execution_context>
@/Users/henrybaker/.claude/get-shit-done/workflows/execute-plan.md
@/Users/henrybaker/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Implementation from Plans 01 and 02
@.planning/phases/02.3-campaign-state-resume/02.3-01-SUMMARY.md
@.planning/phases/02.3-campaign-state-resume/02.3-02-SUMMARY.md

# Existing test patterns
@tests/CLAUDE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create resume command tests</name>
  <files>
    tests/unit/cli/test_resume.py
  </files>
  <action>
Create `test_resume.py` with tests following existing test patterns:

```python
"""Unit tests for lem resume command."""
import pytest
from pathlib import Path
from typer.testing import CliRunner
from llenergymeasure.cli import app

runner = CliRunner()
```

Test cases:
1. `test_resume_no_state_directory`: No `.state/` exists -> exits with "No interrupted work found"
2. `test_resume_no_manifests`: `.state/` exists but empty -> exits with "No interrupted campaigns found"
3. `test_resume_dry_run_shows_campaign`: Create mock manifest, run `--dry-run`, verify output shows campaign info
4. `test_resume_wipe_clears_state`: Create `.state/` with files, run `--wipe`, confirm, verify directory deleted
5. `test_resume_wipe_cancelled`: Run `--wipe`, decline confirmation, verify state still exists
6. `test_resume_help_shows_options`: Run `--help`, verify --dry-run and --wipe documented

Use pytest fixtures:
- `tmp_path` for isolated state directory
- Mock `Path.cwd()` or use monkeypatch to redirect `.state/` lookups to tmp_path
- Create mock campaign_manifest.json files with ManifestManager

Use `runner.invoke(app, ["resume", ...])` pattern.
  </action>
  <verify>
    `pytest tests/unit/cli/test_resume.py -v` passes all tests
  </verify>
  <done>
    Resume command tests cover no-state, dry-run, wipe, and help scenarios.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create init wizard tests</name>
  <files>
    tests/unit/cli/test_init_cmd.py
  </files>
  <action>
Create `test_init_cmd.py` with tests:

```python
"""Unit tests for lem init command."""
import pytest
from pathlib import Path
from typer.testing import CliRunner
from llenergymeasure.cli import app

runner = CliRunner()
```

Test cases:
1. `test_init_non_interactive_creates_config`: Run `--non-interactive`, verify `.lem-config.yaml` created with defaults
2. `test_init_non_interactive_with_flags`: Run `--non-interactive --results-dir /custom`, verify custom value in config
3. `test_init_non_interactive_with_webhook`: Run `--non-interactive --webhook-url https://example.com/hook`, verify webhook in config
4. `test_init_help_shows_options`: Run `--help`, verify --non-interactive, --results-dir, --webhook-url documented
5. `test_init_existing_config_prompts`: Create existing config, run init, verify prompts about update (use input simulation)

Use pytest fixtures:
- `tmp_path` for isolated config file
- `monkeypatch.chdir(tmp_path)` to run in temp directory
- For interactive tests, may need to mock questionary or skip with `@pytest.mark.skip("interactive")`

Focus on non-interactive mode for reliable CI testing.
  </action>
  <verify>
    `pytest tests/unit/cli/test_init_cmd.py -v` passes all tests
  </verify>
  <done>
    Init wizard tests cover non-interactive mode and help output.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create webhook sender and UserConfig tests</name>
  <files>
    tests/unit/notifications/test_webhook.py
    tests/unit/config/test_user_config.py
  </files>
  <action>
1. Create `tests/unit/notifications/` directory if needed.

2. Create `test_webhook.py`:

```python
"""Unit tests for webhook notification sender."""
import pytest
from unittest.mock import patch, MagicMock
from llenergymeasure.notifications.webhook import send_webhook_notification
```

Test cases:
- `test_webhook_not_configured_returns_false`: No notifications in config -> returns False, no HTTP call
- `test_webhook_disabled_event_returns_false`: on_complete=False, event="complete" -> returns False
- `test_webhook_success_returns_true`: Mock httpx.post success -> returns True
- `test_webhook_timeout_returns_false`: Mock httpx.post raises TimeoutException -> returns False, logs warning
- `test_webhook_http_error_returns_false`: Mock httpx.post raises HTTPStatusError -> returns False
- `test_webhook_request_error_returns_false`: Mock httpx.post raises RequestError -> returns False

Use `@patch("llenergymeasure.notifications.webhook.load_user_config")` and `@patch("httpx.post")`.

3. Update/extend `tests/unit/config/test_user_config.py`:

Add tests:
- `test_user_config_no_default_backend`: Verify `default_backend` not in UserConfig.model_fields
- `test_user_config_has_notifications`: Verify `notifications` field exists
- `test_load_user_config_invalid_raises`: Invalid YAML -> raises ValueError (not silent defaults)
- `test_notifications_config_defaults`: NotificationsConfig has correct defaults (on_complete=True, on_failure=True)
  </action>
  <verify>
    `pytest tests/unit/notifications/test_webhook.py tests/unit/config/test_user_config.py -v` passes all tests
  </verify>
  <done>
    Webhook and UserConfig tests verify error handling, configuration, and validation.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/unit/cli/test_resume.py -v` passes
2. `pytest tests/unit/cli/test_init_cmd.py -v` passes
3. `pytest tests/unit/notifications/test_webhook.py -v` passes
4. `pytest tests/unit/config/test_user_config.py -v` passes
5. Full test suite: `pytest tests/unit/ -v --ignore=tests/unit/test_all_params.py` passes
</verification>

<success_criteria>
- All new test files created and passing
- Resume command edge cases covered (no state, wipe, dry-run)
- Init wizard non-interactive mode tested
- Webhook sender error handling verified with mocks
- UserConfig changes verified (no default_backend, has notifications, validation)
</success_criteria>

<output>
After completion, create `.planning/phases/02.3-campaign-state-resume/02.3-03-SUMMARY.md`
</output>
