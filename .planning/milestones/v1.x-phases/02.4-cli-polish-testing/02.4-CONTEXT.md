# Phase 2.4: CLI Polish & Testing Infrastructure - Context

**Gathered:** 2026-02-04
**Status:** Ready for planning

<domain>
## Phase Boundary

Clean up CLI UX issues, improve developer experience with systematic smoke tests, and ensure all configurations and examples are production-ready with schema v3.0.0. This phase consolidates accumulated todos from previous phases into cohesive improvements.

</domain>

<decisions>
## Implementation Decisions

### Campaign Aggregation group_by
- User specifies any config field path for grouping (e.g., `model.name`, `batching.batch_size`, `backend`)
- Config syntax: `group_by: [backend, model.name, batch_size]` — list of field names
- Output: Both CLI table AND campaign JSON — printed during campaign, saved in results
- Stats per group: Mean, std, 95% CI for key metrics (energy, throughput, latency) using bootstrap

### CLI Output & Verbosity
- Three-tier verbosity already implemented: `--quiet` (errors only), default (progress + summary), `--verbose` (full logs)
- **Backend noise filtering** (NEW — not yet implemented):
  - vLLM: Filter ModelRunner initialization logs (thousands of lines) — show only in `--verbose`
  - TensorRT: Filter engine building logs — show only in `--verbose`
  - PyTorch: Filter HF transformers/tokenizer warnings — show only in `--verbose`
  - Implementation: Configure loguru/logging filters based on verbosity level
- Backend log capture:
  - Always capture to separate log file at `results/<exp_id>/logs/`
  - Default: single progress line/bar (backend noise suppressed)
  - Verbose: show all backend output to terminal
- Log file format: Both plain text AND JSON lines by default; configurable in `lem init`
- Progress bars: Rich progress bars with ETA for prompts/experiments
- JSON output mode: `--json` flag for full machine-readable output
- Verbosity cascade: `lem init` → `.lem-config.yaml` → campaign YAML → experiment YAML (each level can override)
- Terminal compatibility: Rich handles NO_COLOR automatically

### Smoke Test Infrastructure
- Coverage: SSOT-driven parameter matrix from introspection.py (auto-generate test combinations)
- Extend existing `tests/runtime/test_all_params.py` infrastructure (don't create new command)
- Warning/error capture:
  - Capture ALL stdout/stderr during inference
  - Structured report with categorized warnings and errors
  - Track source of each warning (which lib, which feature)
- Pass/fail criteria: STRICT — no warnings OR errors
  - ANY warning from ANY source = failure
  - ANY error from ANY source = failure
  - Examples: missing optional deps (flash_attention_2 fallback), distributed setup issues (Ray warnings), backend compilation warnings, deprecation warnings, etc.
  - The point is comprehensive capture — not a predefined list of known issues
- Known issues tracking: `issues.yaml` file to track known problems that are WIP (allows temporary exceptions while fixes are in progress)
- No CI integration — manual `python -m tests.runtime.test_all_params --smoke` command

### Example Configs
- **Delivered:** Basic examples updated to schema v3.0.0 with key backend features
- Documentation: Minimal comments with links to docs (keep configs clean)
- Campaign examples: Yes, show grid syntax, multi-backend campaigns, group_by
- Test configs: tests/configs/ for specific scenarios; parameter matrix testing uses SSOT introspection
- **DESCOPED to Phase 3:** Comprehensive examples deferred until after parameter audit reveals full param surface
  - `pytorch_full.yaml`, `vllm_full.yaml`, `tensorrt_full.yaml` (comprehensive examples)
  - Graduated examples (minimal → advanced)
  - Use-case based examples (`throughput_benchmark.yaml`, `energy_comparison.yaml`)

### Docker Lifecycle Output (from pending todo)
- Show progress during Docker operations (currently silent)
- **Ephemeral mode** (`docker compose run --rm`):
  - Show which backend container is being used
  - Show image pull/build progress if needed
  - Show experiment dispatch status per experiment
- **Persistent mode** (`docker compose up + exec`):
  - Show container startup progress during `docker compose up`
  - Show health check status (waiting for container ready)
  - Show exec dispatch per experiment
  - Show teardown status at campaign end
- Both modes: Clear indication of which mode is active and why
- Build progress: If images need building, show `docker compose build` output (or progress indicator)

### CLI Args vs Options (AUDITED — verified correct)
- **Audit result:** All commands follow Typer best practices ✓
- `experiment_cmd`: config_path is `typer.Argument` (positional) ✓
- `campaign_cmd`: config_paths is `typer.Argument` (positional) ✓
- `init_cmd`, `resume_cmd`: All options (no required positional args) ✓
- No changes needed

### pyproject.toml & Docker SSOT (from pending todo)
- Audit dependency architecture
- Ensure consistent pattern between pyproject.toml and Dockerfiles

### lem config list command (from pending todo)
- New subcommand: `lem config list`
- Lists available YAML files in configs/ directory
- Shows user config settings from .lem-config.yaml

### Claude's Discretion
- Campaign progress UI design (table vs rolling log vs progress bar)
- Exact smoke test CLI interface design
- Compression algorithm for log files
- Error state handling patterns

</decisions>

<specifics>
## Specific Ideas

- "If ANY lib/feature/functionality/dependency is failing, I want it logged and addressed as a bug"
- Examples like Ray distributed warnings or flash_attention_2 fallbacks are illustrative — the goal is comprehensive capture of ALL warnings/errors, not a predefined checklist
- Smoke tests should leverage existing test_all_params.py SSOT machinery, not reinvent
- Log files should be co-located with experiment results for easy debugging

</specifics>

<deferred>
## Deferred Ideas

None — discussion stayed within phase scope

</deferred>

---

*Phase: 02.4-cli-polish-testing*
*Context gathered: 2026-02-04*
