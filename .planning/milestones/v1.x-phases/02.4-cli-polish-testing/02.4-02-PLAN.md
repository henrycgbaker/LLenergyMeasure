---
phase: 02.4-cli-polish-testing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/config/models.py
  - src/llenergymeasure/config/loader.py
  - configs/examples/pytorch_example.yaml
  - configs/examples/vllm_example.yaml
  - configs/examples/tensorrt_example.yaml
  - configs/examples/campaign_example.yaml
autonomous: true

must_haves:
  truths:
    - "All example configs specify schema_version: '3.0.0'"
    - "Configs without schema_version emit warning during load"
    - "Example configs demonstrate backend-specific features (flash_attention_2, continuous_batching, etc.)"
    - "Campaign example shows grid syntax and group_by usage"
  artifacts:
    - path: "src/llenergymeasure/config/models.py"
      provides: "schema_version field on ExperimentConfig"
      contains: "schema_version"
    - path: "configs/examples/pytorch_example.yaml"
      provides: "Complete PyTorch example with schema v3.0.0"
      contains: "schema_version: '3.0.0'"
    - path: "configs/examples/vllm_example.yaml"
      provides: "Complete vLLM example with schema v3.0.0"
      contains: "schema_version: '3.0.0'"
  key_links:
    - from: "src/llenergymeasure/config/loader.py"
      to: "src/llenergymeasure/config/models.py"
      via: "ExperimentConfig validation with schema_version"
      pattern: "schema_version"
---

<objective>
Add schema_version field to ExperimentConfig and update all example configs to schema v3.0.0 with backend-specific features maximised.

Purpose: Prevent config drift by tracking schema versions, and ensure example configs demonstrate the full capability of each backend for users to learn from.

Output: Schema version tracking in config system, updated example configs.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.4-cli-polish-testing/02.4-CONTEXT.md

@src/llenergymeasure/config/models.py
@src/llenergymeasure/config/loader.py
@configs/examples/pytorch_example.yaml
@configs/examples/vllm_example.yaml
@configs/examples/tensorrt_example.yaml
@configs/examples/campaign_example.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add schema_version field and validation</name>
  <files>
    src/llenergymeasure/config/models.py
    src/llenergymeasure/config/loader.py
  </files>
  <action>
**Part A: models.py - Add schema_version field**

Add optional `schema_version` field to `ExperimentConfig`:

```python
# Near top of ExperimentConfig class
schema_version: str | None = Field(
    default=None,
    description="Schema version (e.g., '3.0.0'). Used for config compatibility tracking.",
)
```

Add constant for current schema version (near top of file):
```python
CURRENT_SCHEMA_VERSION = "3.0.0"
```

**Part B: loader.py - Add schema version warning**

In `validate_config` function, add schema version check:

```python
from llenergymeasure.config.models import CURRENT_SCHEMA_VERSION

# Near start of validate_config, after existing checks:
# Schema version warning (non-blocking)
if config.schema_version is None:
    warnings.append(
        ConfigWarning(
            param="schema_version",
            message=f"No schema_version specified. Current schema is {CURRENT_SCHEMA_VERSION}.",
            severity="info",  # Not blocking - just informational
            code="SCHEMA_VERSION_MISSING",
        )
    )
elif config.schema_version != CURRENT_SCHEMA_VERSION:
    warnings.append(
        ConfigWarning(
            param="schema_version",
            message=f"Config schema_version '{config.schema_version}' differs from current '{CURRENT_SCHEMA_VERSION}'. Some fields may be deprecated or renamed.",
            severity="warning",  # Warning but not error - may still work
            code="SCHEMA_VERSION_MISMATCH",
        )
    )
```

Do NOT make schema_version required - existing configs should continue to work with just a warning.
  </action>
  <verify>
```bash
# Test with existing config (no schema_version)
lem config validate configs/examples/pytorch_example.yaml
# Should show info about missing schema_version

# After updating examples, validate again
lem config validate configs/examples/pytorch_example.yaml
# Should pass without schema warnings
```
  </verify>
  <done>
`schema_version` field exists on ExperimentConfig, loader emits warning for missing/mismatched versions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update example configs to schema v3.0.0</name>
  <files>
    configs/examples/pytorch_example.yaml
    configs/examples/vllm_example.yaml
    configs/examples/tensorrt_example.yaml
    configs/examples/campaign_example.yaml
  </files>
  <action>
Update all example configs with:
1. `schema_version: '3.0.0'` at top
2. Backend-specific features that maximise functionality
3. Minimal inline comments with links to docs
4. Clean YAML structure

**pytorch_example.yaml:**
```yaml
schema_version: '3.0.0'
config_name: pytorch-comprehensive

model_name: Qwen/Qwen2.5-0.5B
backend: pytorch
gpus: [0]

# Token configuration
max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 100

# Precision and determinism
fp_precision: float16
random_seed: 42

# Dataset (simple form)
dataset:
  name: ai_energy_score
  sample_size: 100

# Decoder settings
decoder:
  preset: deterministic

# PyTorch-specific optimizations
pytorch:
  batch_size: 8
  batching_strategy: dynamic
  attn_implementation: sdpa  # Use flash_attention_2 if installed
  torch_compile: false  # Set to 'inductor' for production
  use_cache: true
  low_cpu_mem_usage: true

# Measurement settings (v3.0.0)
warmup:
  enabled: true
  mode: convergence
  min_prompts: 5
  max_prompts: 20
  target_cv: 0.05

baseline:
  enabled: true
  method: cache  # Uses cached baseline if recent
  cache_ttl_seconds: 3600

timeseries:
  enabled: true
  sample_rate_hz: 5.0

# See docs/cli.md for full parameter reference
```

**vllm_example.yaml:**
```yaml
schema_version: '3.0.0'
config_name: vllm-comprehensive

model_name: Qwen/Qwen2.5-0.5B
backend: vllm
gpus: [0]

max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 100

fp_precision: float16
random_seed: 42

dataset:
  name: ai_energy_score
  sample_size: 100

decoder:
  preset: deterministic

# vLLM-specific optimizations
vllm:
  max_num_seqs: 256
  max_model_len: 1024
  gpu_memory_utilization: 0.9
  tensor_parallel_size: 1
  enable_prefix_caching: true
  enable_chunked_prefill: true
  enforce_eager: false  # Allow CUDA graphs
  kv_cache_dtype: auto
  block_size: 16

# Measurement settings
warmup:
  enabled: true
  mode: convergence
  min_prompts: 5
  target_cv: 0.05

baseline:
  enabled: true
  method: cache

timeseries:
  enabled: true
  sample_rate_hz: 5.0

# See docs/backends.md for vLLM tuning guide
```

**tensorrt_example.yaml:**
```yaml
schema_version: '3.0.0'
config_name: tensorrt-comprehensive

model_name: Qwen/Qwen2.5-0.5B
backend: tensorrt
gpus: [0]

max_input_tokens: 512
max_output_tokens: 128
num_input_prompts: 100

fp_precision: float16
random_seed: 42

dataset:
  name: ai_energy_score
  sample_size: 100

decoder:
  preset: deterministic

# TensorRT-LLM optimizations
tensorrt:
  max_batch_size: 8
  builder_opt_level: 3
  tp_size: 1
  quantization: none  # Options: none, fp8, int8_sq
  kv_cache_type: paged
  enable_chunked_context: true
  gpu_memory_utilization: 0.9
  force_rebuild: false  # Set true to regenerate engine

# Measurement settings
warmup:
  enabled: true
  mode: convergence
  min_prompts: 5
  target_cv: 0.05

baseline:
  enabled: true
  method: cache

timeseries:
  enabled: true
  sample_rate_hz: 5.0

# See docs/backends.md for TensorRT requirements (Ampere+ GPU)
```

**campaign_example.yaml:**
```yaml
schema_version: '3.0.0'
campaign_name: backend-comparison

# Model to benchmark across backends
model: Qwen/Qwen2.5-0.5B

# Dataset for all experiments
dataset: ai_energy_score
num_samples: 100

# Grid expansion - generates configs for each combination
grid:
  backends: [pytorch, vllm]
  axes:
    batch_size: [1, 4, 8]
    fp_precision: [float16, bfloat16]

# Execution settings
execution:
  cycles: 3  # Repeat for statistical robustness
  structure: interleaved  # Fair comparison ordering
  warmup_prompts: 5
  warmup_timeout_seconds: 30
  config_gap_seconds: 60  # Thermal recovery between configs
  cycle_gap_seconds: 300  # Full thermal reset between cycles

# Cold start benchmarking (optional)
cold_start:
  force_cold_start: false  # Set true for cold-start benchmarks

# Daemon mode (optional, for scheduled benchmarks)
# daemon:
#   enabled: true
#   at: "02:00"  # Run at 2 AM
#   interval: "24h"

# Results grouping (for summary output)
# Use with: lem campaign campaign.yaml --group-by backend,batch_size

# See docs/cli.md for campaign configuration reference
```
  </action>
  <verify>
```bash
# Validate all updated configs
lem config validate configs/examples/pytorch_example.yaml
lem config validate configs/examples/vllm_example.yaml
lem config validate configs/examples/tensorrt_example.yaml
lem config validate configs/examples/campaign_example.yaml

# All should pass without blocking errors
# May show warnings for missing optional deps (flash_attention, etc.)
```
  </verify>
  <done>
All example configs updated with schema_version: '3.0.0', backend-specific features, and clean documentation links.
  </done>
</task>

</tasks>

<verification>
1. `schema_version` field exists on ExperimentConfig
2. `CURRENT_SCHEMA_VERSION = "3.0.0"` constant defined
3. Configs without schema_version emit info-level warning
4. Configs with wrong schema_version emit warning-level message
5. All example configs validate successfully
6. Example configs showcase backend-specific features
</verification>

<success_criteria>
- schema_version field optional, defaults to None
- Loader warns about missing/mismatched versions (non-blocking)
- pytorch_example.yaml demonstrates dynamic batching, sdpa, warmup convergence
- vllm_example.yaml demonstrates continuous batching, prefix caching, chunked prefill
- tensorrt_example.yaml demonstrates TensorRT optimizations, paged KV cache
- campaign_example.yaml demonstrates grid expansion, cycles, group_by comment
</success_criteria>

<output>
After completion, create `.planning/phases/02.4-cli-polish-testing/02.4-02-SUMMARY.md`
</output>
