---
phase: 02.4-cli-polish-testing
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/logging.py
  - src/llenergymeasure/cli/experiment.py
  - src/llenergymeasure/cli/campaign.py
  - src/llenergymeasure/config/user_config.py
autonomous: true

must_haves:
  truths:
    - "Backend noise (vLLM, TensorRT, HF transformers) suppressed in default mode"
    - "Backend noise shown in --verbose mode"
    - "Experiment logs captured to results/<exp_id>/logs/ directory"
    - "--json flag produces machine-readable output"
  artifacts:
    - path: "src/llenergymeasure/logging.py"
      provides: "Backend-specific log filtering"
      contains: "filter_backend_logs"
    - path: "src/llenergymeasure/cli/experiment.py"
      provides: "Log file capture to results directory"
      contains: "logs_dir"
  key_links:
    - from: "src/llenergymeasure/cli/__init__.py"
      to: "src/llenergymeasure/logging.py"
      via: "setup_logging with backend filtering"
      pattern: "setup_logging"
---

<objective>
Implement backend noise filtering based on verbosity level, log file capture to results directory, and --json output mode.

Purpose: Make CLI output clean by default (suppress vLLM/TensorRT/HF initialization spam) while capturing full logs to files for debugging.

Output: Backend noise filtered in default mode, full logs available in --verbose and in log files.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02.4-cli-polish-testing/02.4-CONTEXT.md

@src/llenergymeasure/logging.py
@src/llenergymeasure/cli/__init__.py
@src/llenergymeasure/cli/experiment.py
@src/llenergymeasure/cli/campaign.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add backend-specific log filtering to logging.py</name>
  <files>src/llenergymeasure/logging.py</files>
  <action>
Extend logging.py to filter backend-specific noise based on verbosity:

1. Add filter function that suppresses noisy loggers in non-verbose mode:

```python
# Noisy loggers to filter in default/quiet modes
BACKEND_NOISY_LOGGERS = [
    # vLLM - ModelRunner initialization spam
    "vllm",
    "vllm.worker",
    "vllm.model_executor",
    "vllm.engine",
    # TensorRT - Engine building logs
    "tensorrt",
    "tensorrt_llm",
    # HuggingFace transformers/tokenizers
    "transformers",
    "transformers.tokenization_utils",
    "transformers.modeling_utils",
    "tokenizers",
    # Ray distributed (if used)
    "ray",
    "ray.worker",
]

def configure_backend_log_filtering(verbosity: VerbosityType) -> None:
    """Configure logging filters for backend libraries.

    In quiet/normal mode: Suppress backend initialization logs (set to WARNING+)
    In verbose mode: Show all backend logs (set to DEBUG)
    """
    import logging

    if verbosity == "verbose":
        # Show everything in verbose mode
        level = logging.DEBUG
    else:
        # Suppress INFO/DEBUG from noisy backends in normal/quiet
        level = logging.WARNING

    for logger_name in BACKEND_NOISY_LOGGERS:
        logging.getLogger(logger_name).setLevel(level)
```

2. Call `configure_backend_log_filtering` at the end of `setup_logging`:

```python
def setup_logging(
    level: str = "INFO",
    json_output: bool = False,
    log_file: str | None = None,
    verbosity: VerbosityType | None = None,
) -> None:
    # ... existing code ...

    # Configure backend-specific filtering
    effective_verbosity = verbosity or _get_verbosity_from_env()
    configure_backend_log_filtering(effective_verbosity)
```

3. Add helper to check if backend filtering is active (for CLI display):

```python
def is_backend_filtering_active() -> bool:
    """Check if backend log filtering is currently active."""
    verbosity = _get_verbosity_from_env()
    return verbosity != "verbose"
```
  </action>
  <verify>
Run a vLLM or PyTorch experiment in default mode - backend init logs should be suppressed.
Run with `--verbose` - full backend logs should appear.
```bash
lem experiment configs/examples/pytorch_example.yaml -n 3  # Should be quiet
lem experiment configs/examples/pytorch_example.yaml -n 3 --verbose  # Should show HF logs
```
  </verify>
  <done>
Backend noise from vLLM, TensorRT, and HuggingFace libraries filtered based on verbosity level.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add log file capture to results directory</name>
  <files>
    src/llenergymeasure/cli/experiment.py
    src/llenergymeasure/logging.py
  </files>
  <action>
Capture full logs (including filtered backend output) to a log file in results:

1. In logging.py, add function to add file handler:

```python
def add_experiment_log_file(log_dir: Path, experiment_id: str) -> Path:
    """Add a log file handler for experiment logs.

    Captures ALL logs (DEBUG level) to file, regardless of console verbosity.

    Args:
        log_dir: Directory for log files (e.g., results/<exp_id>/logs/)
        experiment_id: Experiment ID for log filename

    Returns:
        Path to the log file
    """
    log_dir.mkdir(parents=True, exist_ok=True)
    log_path = log_dir / f"{experiment_id}.log"

    # Add file handler at DEBUG level (capture everything)
    logger.add(
        str(log_path),
        format=VERBOSE_FORMAT,
        level="DEBUG",
        rotation="50 MB",
        retention="7 days",
        enqueue=True,  # Thread-safe
    )

    return log_path
```

2. In experiment.py, call log file setup before running experiment:

After getting `experiment_id` and before subprocess launch:

```python
# Set up log file capture
from llenergymeasure.logging import add_experiment_log_file

logs_dir = actual_results_dir / experiment_id / "logs"
log_file_path = add_experiment_log_file(logs_dir, experiment_id)
console.print(f"[dim]Logs: {log_file_path}[/dim]")
```

3. Pass log file path to subprocess via environment variable:

```python
subprocess_env["LLM_ENERGY_LOG_FILE"] = str(log_file_path)
```

4. In the subprocess entry point, configure logging to write to the passed file.
  </action>
  <verify>
Run an experiment:
```bash
lem experiment configs/examples/pytorch_example.yaml -n 3
```
Check that `results/<exp_id>/logs/<exp_id>.log` exists and contains full debug logs.
  </verify>
  <done>
Full experiment logs captured to `results/<exp_id>/logs/` regardless of console verbosity.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add --json flag for machine-readable output</name>
  <files>
    src/llenergymeasure/cli/__init__.py
    src/llenergymeasure/cli/experiment.py
    src/llenergymeasure/cli/campaign.py
  </files>
  <action>
Add `--json` flag for machine-readable output:

1. In cli/__init__.py, add global `--json` option:

```python
json_output: Annotated[
    bool, typer.Option("--json", help="Output results as JSON (machine-readable)")
] = False,
```

Store in environment: `os.environ["LLM_ENERGY_JSON_OUTPUT"] = "true" if json_output else "false"`

2. In experiment.py, modify output to respect JSON mode:

```python
def _output_result(result: AggregatedResult, json_mode: bool = False) -> None:
    """Output experiment result in appropriate format."""
    if json_mode or os.environ.get("LLM_ENERGY_JSON_OUTPUT") == "true":
        import json
        console.print(json.dumps(result.model_dump(), indent=2, default=str))
    else:
        # Existing Rich table/panel output
        _display_measurement_summary(repo, experiment_id)
```

3. In campaign.py, modify campaign summary output:

```python
def _display_campaign_summary_json(manifest: CampaignManifest, grouped_results: dict) -> None:
    """Output campaign summary as JSON."""
    import json
    output = {
        "campaign_name": manifest.campaign_name,
        "total_experiments": len(manifest.experiments),
        "completed": manifest.completed_count,
        "failed": manifest.failed_count,
        "groups": {
            str(k): v for k, v in grouped_results.items()
        }
    }
    console.print(json.dumps(output, indent=2, default=str))
```

4. Suppress Rich formatting when JSON mode active:
   - Check `LLM_ENERGY_JSON_OUTPUT` before Rich table/panel calls
   - Use plain print for JSON, Rich for human output
  </action>
  <verify>
```bash
lem experiment configs/examples/pytorch_example.yaml -n 3 --json | jq .
```
Should output valid JSON that can be piped to jq.
  </verify>
  <done>
`--json` flag outputs machine-readable JSON instead of Rich formatted tables.
  </done>
</task>

</tasks>

<verification>
1. Run experiment in default mode - no vLLM/HF initialization spam
2. Run experiment with --verbose - full backend logs visible
3. Check results/<exp_id>/logs/ directory exists with log file
4. Run experiment with --json - valid JSON output
5. Run campaign with --json - valid JSON summary
</verification>

<success_criteria>
- Backend noise suppressed by default, shown with --verbose
- Log files captured to results directory
- --json flag produces machine-readable output
- No regressions in existing CLI functionality
</success_criteria>

<output>
After completion, create `.planning/phases/02.4-cli-polish-testing/02.4-05-SUMMARY.md`
</output>
