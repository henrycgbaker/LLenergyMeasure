# Phase 2.4: CLI Polish & Testing Infrastructure - Research

**Researched:** 2026-02-04
**Domain:** CLI UX, Python testing patterns, dependency management
**Confidence:** HIGH

## Summary

Phase 2.4 focuses on five key areas: (1) campaign aggregation with configurable grouping, (2) CLI argument pattern audit following Typer best practices, (3) three-tier verbosity system with backend noise filtering, (4) systematic smoke tests for inference parameter validation, and (5) SSOT dependency architecture.

The standard approach uses Typer's distinction between positional arguments (required inputs) and options/flags (optional modifiers), implements verbosity via loguru level mapping (quiet=WARNING, normal=INFO, verbose=DEBUG), and leverages pytest.mark.parametrize for combinatorial parameter testing. Existing runtime tests provide a strong foundation but need expansion to smoke test format with warning capture.

The codebase already has three-tier logging (quiet/normal/verbose) implemented in `logging.py`, extensive runtime parameter testing in `tests/runtime/test_all_params.py`, and a clear CLI structure in `cli/` package. Campaign aggregation exists (`_display_campaign_ci_summary`) but lacks configurable grouping. Example configs don't specify schema_version field.

**Primary recommendation:** Build on existing patterns - extend campaign aggregation with group_by parameter, audit CLI arguments vs options using Typer conventions, wrap existing runtime tests in smoke test harness with warning capture, and audit pyproject.toml extras for consistency.

## Standard Stack

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| typer | >=0.15.0 | CLI framework | De facto standard for modern Python CLIs, built on Click with better type hints |
| loguru | >=0.7.0 | Structured logging | Already in use, supports verbosity levels cleanly |
| pytest | >=8.0 | Testing framework | Industry standard, powerful parametrization |
| pydantic | ^2.0 | Data validation | Already in use, SSOT for parameters |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| rich | (via typer) | Terminal output | Already available, enhances tables/panels |
| pytest-timeout | Latest | Test timeouts | Optional, for smoke test safety |
| questionary | >=2.0 | Interactive prompts | Already in use for resume/init |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| typer | click | Typer is built on Click with better ergonomics, no reason to switch |
| loguru | stdlib logging | loguru provides cleaner verbosity control, already integrated |
| pytest.mark.parametrize | hypothesis | Parametrize sufficient for smoke tests, hypothesis overkill |

**Installation:**
Already installed via pyproject.toml dependencies.

## Architecture Patterns

### Pattern 1: Arguments vs Options in Typer
**What:** Use positional arguments for required inputs, named options for optional modifiers
**When to use:** All CLI command design
**Example:**
```python
# Source: https://typer.tiangolo.com/tutorial/arguments/optional/
def campaign_cmd(
    config_paths: Annotated[list[Path], typer.Argument(...)],  # Required, positional
    campaign_name: Annotated[str | None, typer.Option("--campaign-name")] = None,  # Optional, flag
    cycles: Annotated[int | None, typer.Option("--cycles", "-c")] = None,  # Optional, flag with short
):
    """Run campaign with required config paths, optional modifiers."""
    pass
```

**Current violations to audit:**
- `experiment.py:experiment_cmd` - check if `config_path` should be Argument not Option
- Config file paths should typically be positional Arguments
- Dataset, sample_size are good as Options (modifiers)

### Pattern 2: Three-Tier Verbosity System
**What:** Map CLI flags to logging levels with format changes
**When to use:** All commands that produce output
**Example:**
```python
# Source: logging.py (already implemented)
# --quiet: WARNING+ only, SIMPLE_FORMAT
# (default): INFO+, SIMPLE_FORMAT
# --verbose: DEBUG+, VERBOSE_FORMAT (with timestamps, module names)

# Implementation:
def setup_logging(verbosity: VerbosityType) -> None:
    if verbosity == "quiet":
        effective_level = "WARNING"
        log_format = SIMPLE_FORMAT
    elif verbosity == "verbose":
        effective_level = "DEBUG"
        log_format = VERBOSE_FORMAT
    else:  # normal
        effective_level = "INFO"
        log_format = SIMPLE_FORMAT
```

**Backend noise filtering:**
- vLLM: Filter ModelRunner initialization logs (INFO level, verbose only)
- TensorRT: Filter engine building logs (thousands of lines, verbose only)
- PyTorch: Filter HF transformers warnings (captured, shown in verbose only)

### Pattern 3: Pytest Parametrization for Smoke Tests
**What:** Use stacked `@pytest.mark.parametrize` decorators to generate test combinations
**When to use:** Testing parameter combinations, smoke tests
**Example:**
```python
# Source: https://docs.pytest.org/en/stable/how-to/parametrize.html
@pytest.mark.parametrize("backend", ["pytorch", "vllm"])
@pytest.mark.parametrize("batch_size", [1, 4, 8])
@pytest.mark.parametrize("precision", ["float16", "bfloat16"])
def test_inference_runs(backend, batch_size, precision):
    """Smoke test: does inference run without errors?"""
    # Run minimal inference, capture warnings
    result = run_inference(backend, batch_size, precision)
    assert result.exit_code == 0
    assert result.tokens_generated > 0
    # Collect warnings for report
```

**Existing foundation:**
`tests/runtime/test_all_params.py` already implements comprehensive parameter testing with auto-discovery from Pydantic models. Smoke tests should wrap this with warning capture and quick-mode defaults.

### Pattern 4: Campaign Aggregation with group_by
**What:** Configurable grouping for campaign result summaries
**When to use:** Multi-config campaigns where user wants to compare by specific dimensions
**Example:**
```python
# Pseudocode pattern (pandas-inspired)
def aggregate_campaign_results(
    results: list[AggregatedResult],
    group_by: list[str] = ["config_name"],  # config_name, backend, model_name, batch_size, etc.
) -> dict[tuple, dict[str, BootstrapCI]]:
    """Group results by specified fields, compute summary stats per group."""
    grouped = {}
    for result in results:
        key = tuple(getattr(result.config, field) for field in group_by)
        grouped.setdefault(key, []).append(result)

    return {
        key: compute_bootstrap_ci(group_results)
        for key, group_results in grouped.items()
    }
```

**CLI interface:**
```bash
lem campaign --group-by backend,batch_size  # Compare backend+batch combinations
lem campaign --group-by model_name          # Compare models
```

### Anti-Patterns to Avoid
- **Mixing Arguments and Options semantics:** Don't use typer.Option for required positional inputs
- **Inconsistent verbosity:** Don't log INFO messages in quiet mode or hide ERROR in quiet
- **Testing implementation details:** Smoke tests should verify observable behavior (inference runs, tokens generated), not internal state
- **Hardcoded parameter lists:** Use introspection from Pydantic models (already done in `config/introspection.py`)

## Don't Hand-Roll

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Parameter discovery | Static lists of valid parameters | `config/introspection.py` SSOT functions | Parameters auto-discovered from Pydantic models, single source of truth |
| CLI argument parsing | Manual sys.argv parsing | Typer decorators with Annotated types | Type safety, automatic validation, help generation |
| Test parametrization | Nested loops generating tests | `pytest.mark.parametrize` stacking | Cleaner, better reporting, skip/xfail per case |
| Verbosity filtering | Per-module logger configuration | Global loguru setup with level mapping | Centralized, consistent across codebase |
| Campaign grouping | Manual dict manipulation | Structured grouping with field extraction | Type-safe, extensible to new group fields |

**Key insight:** The codebase already has SSOT patterns (introspection.py, Pydantic models). Smoke tests should leverage these rather than duplicating parameter lists.

## Common Pitfalls

### Pitfall 1: Argument vs Option Confusion
**What goes wrong:** Using `typer.Option` for inputs that should be positional Arguments, or vice versa
**Why it happens:** Unclear understanding of Typer conventions - options feel more flexible
**How to avoid:** Use this rule: "Is it required input data? Argument. Is it optional modifier? Option."
**Warning signs:** CLI feels awkward ("why do I need --config?"), help text unclear

### Pitfall 2: Verbosity Flag Conflicts
**What goes wrong:** Backend libraries (vLLM, TensorRT) spam logs even in quiet mode
**Why it happens:** They use their own logging systems (C++ in TensorRT, separate loggers in vLLM)
**How to avoid:**
- Capture stdout/stderr in subprocess execution
- Filter based on verbosity BEFORE printing
- Document known noisy backends in user guide
**Warning signs:** "quiet mode" is not quiet, user complaints about log spam

### Pitfall 3: Smoke Test False Positives
**What goes wrong:** Test passes but parameter wasn't actually applied (e.g., flash_attention_2 silently falls back to sdpa)
**Why it happens:** Backend libraries have silent fallbacks without errors
**How to avoid:**
- Parse logs for confirmation messages (existing validation.py pattern)
- Check metrics changed vs baseline (different batch_size should affect throughput)
- Capture and report warnings (flash_attention fallback emits warning)
**Warning signs:** All tests pass but user reports feature not working

### Pitfall 4: Schema Version Drift
**What goes wrong:** Example configs don't match current schema, users copy outdated patterns
**Why it happens:** Schema evolves but examples not updated systematically
**How to avoid:**
- Add `schema_version: "3.0.0"` field to config models (optional but validated)
- CI job validates example configs against current schema
- Script to update all examples when schema bumps
**Warning signs:** Issues filed about "example doesn't work", validation errors from examples

### Pitfall 5: Dependency Conflicts in Docker
**What goes wrong:** pyproject.toml and Dockerfile have different versions, confusion about what's installed
**Why it happens:** Two sources of truth for dependencies
**How to avoid:**
- Dockerfile should install from pyproject.toml: `pip install -e .[backend]`
- Don't duplicate version pins in Dockerfile
- Document conflict between vLLM and TensorRT in both places consistently
**Warning signs:** Works locally but breaks in Docker, "version mismatch" errors

## Code Examples

### CLI Argument Pattern (Audit Target)
```python
# Source: Typer best practices + existing campaign.py

# GOOD: Required input as Argument, modifiers as Options
def experiment_cmd(
    config_path: Annotated[Path, typer.Argument(help="Experiment config YAML")],  # Positional
    dataset: Annotated[str | None, typer.Option("--dataset", "-d")] = None,      # Flag
    sample_size: Annotated[int | None, typer.Option("--sample-size", "-n")] = None,
):
    """Run experiment from config file."""
    pass

# BAD: Required input as Option (current?)
def experiment_cmd(
    config_path: Annotated[Path, typer.Option("--config")],  # Should be Argument
    dataset: Annotated[str | None, typer.Option("--dataset", "-d")] = None,
):
    pass
```

### Verbosity-Aware Output
```python
# Source: Adapt logging.py pattern for CLI output

from llenergymeasure.logging import VerbosityType, get_logger

logger = get_logger(__name__)

def run_with_verbosity(verbosity: VerbosityType):
    # Always show errors
    logger.error("Critical failure")

    # Show warnings in normal/verbose (suppressed in quiet)
    logger.warning("Model fallback: flash_attention_2 -> sdpa")

    # Show info in normal/verbose
    logger.info("Experiment started")

    # Show debug only in verbose
    logger.debug("Parameter values: batch_size=8, precision=float16")
```

### Smoke Test with Warning Capture
```python
# Source: Adapt tests/runtime/test_all_params.py

import pytest
import warnings

@pytest.mark.parametrize("backend", ["pytorch", "vllm"])
@pytest.mark.parametrize("batch_size", [1, 8])
@pytest.mark.timeout(300)
def test_smoke_inference(backend, batch_size, tmp_path):
    """Smoke test: inference completes without errors."""
    with warnings.catch_warnings(record=True) as warning_list:
        warnings.simplefilter("always")

        result = run_minimal_experiment(
            backend=backend,
            batch_size=batch_size,
            model="Qwen/Qwen2.5-0.5B",
            sample_size=5,
            max_output_tokens=32,
            results_dir=tmp_path,
        )

        # Verify inference ran
        assert result.exit_code == 0, f"Process failed: {result.stderr}"
        assert result.tokens_generated > 0, "No tokens generated"
        assert result.throughput_tps > 0, "Zero throughput"

        # Collect warnings for report (don't fail on warnings)
        if warning_list:
            for w in warning_list:
                logger.info(f"Warning: {w.category.__name__}: {w.message}")
```

### Campaign Aggregation with Grouping
```python
# Source: Adapt results/aggregation.py + campaign.py _display_campaign_ci_summary

def aggregate_campaign_with_grouping(
    manifest: CampaignManifest,
    group_by: list[str] = ["config_name"],
) -> dict[tuple, dict[str, BootstrapCI]]:
    """Aggregate campaign results with configurable grouping.

    Args:
        manifest: Campaign manifest with experiment results
        group_by: List of config fields to group by (e.g., ["backend", "batch_size"])

    Returns:
        Grouped results with bootstrap CIs per group
    """
    # Load results
    results_by_key: dict[tuple, list[AggregatedResult]] = {}

    for entry in manifest.experiments:
        if entry.status == "completed" and entry.result_path:
            result = load_result(entry.result_path)

            # Extract grouping key
            key_values = []
            for field in group_by:
                if field == "config_name":
                    key_values.append(entry.config_name)
                else:
                    # Extract from result.config
                    key_values.append(getattr(result.config, field, "unknown"))

            key = tuple(key_values)
            results_by_key.setdefault(key, []).append(result)

    # Compute bootstrap CIs per group
    return {
        key: compute_bootstrap_ci(group_results)
        for key, group_results in results_by_key.items()
    }

# Display with grouped table
def display_grouped_results(grouped: dict[tuple, dict], group_by: list[str]):
    from rich.table import Table

    table = Table(title=f"Campaign Results (grouped by {', '.join(group_by)})")
    for field in group_by:
        table.add_column(field.replace("_", " ").title())
    table.add_column("Cycles", justify="right")
    table.add_column("Energy (J)", justify="right")
    table.add_column("Throughput (tok/s)", justify="right")

    for key, metrics in grouped.items():
        row = [str(v) for v in key]  # Group key values
        row.append(str(metrics["n_cycles"]))
        row.append(format_ci(metrics["energy_j"]))
        row.append(format_ci(metrics["throughput_tps"]))
        table.add_row(*row)

    console.print(table)
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Manual param lists | Pydantic introspection SSOT | Phase 1.x | Parameters auto-discovered, no drift |
| Per-file logging config | Global loguru setup | Phase 1.x | Consistent verbosity control |
| Click CLI | Typer | Original design | Better type hints, modern |
| requirements.txt | pyproject.toml | Original design | Standard packaging |

**Deprecated/outdated:**
- Manual parameter validation: Now use Pydantic models in `config/backend_configs.py`
- Static test parameter lists: Use introspection.py discovery

**Current best practices (2026):**
- pyproject.toml as single source of truth for dependencies ([Writing your pyproject.toml](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/))
- Typer for modern Python CLIs with type hints
- pytest parametrization for combinatorial testing
- Loguru for structured logging with verbosity levels

## Open Questions

1. **Campaign group_by CLI syntax**
   - What we know: Should support multiple group fields
   - What's unclear: Comma-separated string or repeated flag? `--group-by backend,batch_size` vs `--group-by backend --group-by batch_size`
   - Recommendation: Comma-separated single option for simplicity (follows Typer List pattern)

2. **Schema version enforcement**
   - What we know: Configs don't have schema_version field currently
   - What's unclear: Should schema_version be required or optional with validation?
   - Recommendation: Optional field with warning on mismatch, error on incompatible breaking changes

3. **Smoke test scope**
   - What we know: Runtime tests are comprehensive but slow
   - What's unclear: Which parameter combinations are most critical for smoke tests?
   - Recommendation: Quick mode subset - one value per parameter, covering common cases (batch_size=[1,8], precision=[float16], backends=[all])

4. **Docker dependency consistency**
   - What we know: Dockerfiles currently pin versions separately
   - What's unclear: Should Dockerfiles use pyproject.toml directly or maintain separate pins?
   - Recommendation: Use `pip install -e .[backend]` in Dockerfile, document version overrides if needed for stability

## Sources

### Primary (HIGH confidence)
- Typer official docs - Arguments vs Options: https://typer.tiangolo.com/tutorial/arguments/optional/
- Pytest parametrization docs: https://docs.pytest.org/en/stable/how-to/parametrize.html
- Python Packaging Guide - pyproject.toml: https://packaging.python.org/en/latest/guides/writing-pyproject-toml/
- Project codebase:
  - `src/llenergymeasure/logging.py` - Existing verbosity implementation
  - `src/llenergymeasure/cli/` - Current CLI structure
  - `tests/runtime/test_all_params.py` - Existing runtime tests
  - `src/llenergymeasure/config/introspection.py` - SSOT parameter discovery

### Secondary (MEDIUM confidence)
- CLI verbosity patterns: https://xahteiwi.eu/resources/hints-and-kinks/python-cli-logging-options/
- Pandas groupby for aggregation pattern inspiration: https://pandas.pydata.org/docs/user_guide/groupby.html
- Python packaging 2026 guide: https://learn.repoforge.io/posts/the-state-of-python-packaging-in-2026/

### Tertiary (LOW confidence)
- pytest-smoke plugin: https://github.com/yugokato/pytest-smoke (considered, not needed - pytest parametrize sufficient)

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - All libraries already in use, verified in pyproject.toml
- Architecture: HIGH - Typer and pytest patterns well-documented, existing codebase follows similar patterns
- Pitfalls: HIGH - Based on codebase inspection and documented issues (flash_attention fallback, vLLM noise)
- Campaign grouping: MEDIUM - Pattern inspired by pandas but needs custom implementation
- Schema versioning: MEDIUM - Pattern needs validation with existing validation.py approach

**Research date:** 2026-02-04
**Valid until:** 2026-03-04 (30 days - stable dependencies, established patterns)

**Key findings for planner:**
1. Leverage existing `tests/runtime/test_all_params.py` - don't rebuild from scratch
2. Verbosity system already implemented in `logging.py` - audit where it's used
3. CLI argument audit should focus on `experiment.py`, `batch.py`, `campaign.py`
4. Campaign grouping needs new implementation but can follow existing `_display_campaign_ci_summary` pattern
5. Schema version field should be added to config models as optional with validation
