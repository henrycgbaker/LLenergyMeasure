---
phase: 02.4-cli-polish-testing
plan: 04
type: execute
wave: 2
depends_on: ["02.4-01", "02.4-02", "02.4-03", "02.4-05", "02.4-06"]
files_modified:
  - tests/unit/cli/test_config_list.py
  - tests/unit/cli/test_campaign_group_by.py
  - tests/unit/test_schema_version.py
  - tests/unit/test_backend_filtering.py
  - tests/unit/cli/test_docker_lifecycle.py
autonomous: false

must_haves:
  truths:
    - "Unit tests verify config list command outputs correct table"
    - "Unit tests verify group_by field extraction works for simple and nested paths"
    - "Unit tests verify schema_version validation warnings"
    - "Unit tests verify backend log filtering based on verbosity"
    - "Unit tests verify Docker lifecycle status display functions"
    - "Manual verification confirms CLI commands work end-to-end"
  artifacts:
    - path: "tests/unit/cli/test_config_list.py"
      provides: "Tests for lem config list command"
      contains: "test_config_list"
    - path: "tests/unit/cli/test_campaign_group_by.py"
      provides: "Tests for --group-by flag"
      contains: "test_group_by"
    - path: "tests/unit/test_backend_filtering.py"
      provides: "Tests for backend noise filtering"
      contains: "test_backend_filtering"
  key_links:
    - from: "tests/unit/cli/test_config_list.py"
      to: "src/llenergymeasure/cli/config.py"
      via: "config_list function import"
      pattern: "from llenergymeasure.cli.config import"
---

<objective>
Write unit tests for the new Phase 2.4 features and perform manual verification checkpoint.

Purpose: Validate that all new functionality works correctly before considering the phase complete.

Output: Unit tests passing, manual verification of CLI features.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02.4-cli-polish-testing/02.4-CONTEXT.md

# Prior plan summaries (after completion)
@.planning/phases/02.4-cli-polish-testing/02.4-01-SUMMARY.md
@.planning/phases/02.4-cli-polish-testing/02.4-02-SUMMARY.md
@.planning/phases/02.4-cli-polish-testing/02.4-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Unit tests for config list command</name>
  <files>tests/unit/cli/test_config_list.py</files>
  <action>
Create unit tests for the `lem config list` command:

```python
"""Tests for lem config list command."""

from __future__ import annotations

from pathlib import Path
from unittest.mock import patch

import pytest
from typer.testing import CliRunner

from llenergymeasure.cli import app


runner = CliRunner()


class TestConfigList:
    """Tests for config list subcommand."""

    def test_config_list_empty_directory(self, tmp_path: Path) -> None:
        """Test config list with empty directory."""
        result = runner.invoke(app, ["config", "list", "-d", str(tmp_path)])
        assert result.exit_code == 0
        assert "No configuration files found" in result.stdout

    def test_config_list_finds_yaml_files(self, tmp_path: Path) -> None:
        """Test config list finds and displays YAML files."""
        # Create test config
        config = tmp_path / "test.yaml"
        config.write_text("config_name: test\nbackend: pytorch\nmodel_name: test-model")

        result = runner.invoke(app, ["config", "list", "-d", str(tmp_path)])
        assert result.exit_code == 0
        assert "test" in result.stdout
        assert "pytorch" in result.stdout

    def test_config_list_extracts_backend(self, tmp_path: Path) -> None:
        """Test that backend is extracted from config."""
        config = tmp_path / "vllm_test.yaml"
        config.write_text("config_name: vllm_test\nbackend: vllm\nmodel_name: test")

        result = runner.invoke(app, ["config", "list", "-d", str(tmp_path)])
        assert result.exit_code == 0
        assert "vllm" in result.stdout

    def test_config_list_handles_invalid_yaml(self, tmp_path: Path) -> None:
        """Test that invalid YAML is skipped with warning."""
        # Valid config
        valid = tmp_path / "valid.yaml"
        valid.write_text("config_name: valid\nbackend: pytorch")

        # Invalid YAML
        invalid = tmp_path / "invalid.yaml"
        invalid.write_text("not: valid: yaml: : :")

        result = runner.invoke(app, ["config", "list", "-d", str(tmp_path)])
        assert result.exit_code == 0
        # Should still show valid config
        assert "valid" in result.stdout

    def test_config_list_recursive(self, tmp_path: Path) -> None:
        """Test that config list finds configs in subdirectories."""
        subdir = tmp_path / "subdir"
        subdir.mkdir()
        config = subdir / "nested.yaml"
        config.write_text("config_name: nested\nbackend: pytorch")

        result = runner.invoke(app, ["config", "list", "-d", str(tmp_path)])
        assert result.exit_code == 0
        assert "nested" in result.stdout

    def test_config_list_show_user_config(self, tmp_path: Path) -> None:
        """Test --show-user-config flag."""
        # Create empty configs dir
        (tmp_path / "configs").mkdir()

        # Create .lem-config.yaml
        user_config = tmp_path / ".lem-config.yaml"
        user_config.write_text("results_dir: custom/results")

        with patch("llenergymeasure.cli.config.Path.cwd", return_value=tmp_path):
            result = runner.invoke(
                app,
                ["config", "list", "-d", str(tmp_path / "configs"), "--show-user-config"],
            )
            # Should not error even if feature not fully implemented
            assert result.exit_code == 0


class TestConfigListDefault:
    """Tests for config list with default configs/ directory."""

    def test_config_list_default_directory(self) -> None:
        """Test config list uses configs/ by default."""
        result = runner.invoke(app, ["config", "list"])
        # Should not error, may show examples if they exist
        assert result.exit_code == 0
```
  </action>
  <verify>
```bash
pytest tests/unit/cli/test_config_list.py -v
```
All tests should pass.
  </verify>
  <done>Unit tests for config list command pass.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for campaign group_by</name>
  <files>tests/unit/cli/test_campaign_group_by.py</files>
  <action>
Create unit tests for the `--group-by` flag and grouping logic:

```python
"""Tests for campaign --group-by flag and grouping logic."""

from __future__ import annotations

from unittest.mock import MagicMock

import pytest

from llenergymeasure.domain.experiment import AggregatedResult
from llenergymeasure.results.aggregation import (
    aggregate_campaign_with_grouping,
    _extract_field_value,
)


class TestFieldExtraction:
    """Tests for field value extraction from results."""

    @pytest.fixture
    def mock_result(self) -> AggregatedResult:
        """Create a mock AggregatedResult for testing."""
        result = MagicMock(spec=AggregatedResult)
        result.backend = "pytorch"
        result.effective_config = {
            "config_name": "test-config",
            "model_name": "test-model",
            "backend": "pytorch",
            "pytorch": {
                "batch_size": 8,
                "attn_implementation": "sdpa",
            },
            "vllm": None,
        }
        result.total_energy_j = 100.0
        result.avg_tokens_per_second = 50.0
        result.latency_stats = None
        return result

    def test_extract_simple_field_backend(self, mock_result: AggregatedResult) -> None:
        """Test extraction of simple backend field."""
        value = _extract_field_value(mock_result, "backend")
        assert value == "pytorch"

    def test_extract_simple_field_config_name(self, mock_result: AggregatedResult) -> None:
        """Test extraction of config_name field."""
        value = _extract_field_value(mock_result, "config_name")
        assert value == "test-config"

    def test_extract_simple_field_model_name(self, mock_result: AggregatedResult) -> None:
        """Test extraction of model_name field."""
        value = _extract_field_value(mock_result, "model_name")
        assert value == "test-model"

    def test_extract_nested_field(self, mock_result: AggregatedResult) -> None:
        """Test extraction of nested field like pytorch.batch_size."""
        value = _extract_field_value(mock_result, "pytorch.batch_size")
        assert value == "8"

    def test_extract_nested_field_deep(self, mock_result: AggregatedResult) -> None:
        """Test extraction of deeply nested field."""
        value = _extract_field_value(mock_result, "pytorch.attn_implementation")
        assert value == "sdpa"

    def test_extract_missing_field_returns_unknown(self, mock_result: AggregatedResult) -> None:
        """Test that missing field returns 'unknown'."""
        value = _extract_field_value(mock_result, "nonexistent_field")
        assert value == "unknown"

    def test_extract_missing_nested_field_returns_unknown(
        self, mock_result: AggregatedResult
    ) -> None:
        """Test that missing nested field returns 'unknown'."""
        value = _extract_field_value(mock_result, "pytorch.nonexistent")
        assert value == "unknown"


class TestCampaignGrouping:
    """Tests for campaign result grouping."""

    @pytest.fixture
    def sample_results(self) -> dict[str, list[AggregatedResult]]:
        """Create sample results for grouping tests."""
        results = {}

        # Create mock results for two configs
        for config_name in ["pytorch-config", "vllm-config"]:
            backend = "pytorch" if "pytorch" in config_name else "vllm"
            results[config_name] = []

            for cycle in range(3):
                result = MagicMock(spec=AggregatedResult)
                result.backend = backend
                result.effective_config = {
                    "config_name": config_name,
                    "model_name": "test-model",
                    "backend": backend,
                    "pytorch": {"batch_size": 8} if backend == "pytorch" else None,
                    "vllm": {"max_num_seqs": 256} if backend == "vllm" else None,
                }
                result.total_energy_j = 100.0 + cycle * 10
                result.avg_tokens_per_second = 50.0 + cycle * 5
                result.total_tokens = 1000
                result.latency_stats = None
                results[config_name].append(result)

        return results

    def test_grouping_by_config_name(
        self, sample_results: dict[str, list[AggregatedResult]]
    ) -> None:
        """Test grouping by config_name (default)."""
        grouped = aggregate_campaign_with_grouping(
            sample_results, group_by=["config_name"]
        )

        # Should have two groups
        assert len(grouped) == 2
        # Keys should be tuples
        keys = list(grouped.keys())
        assert all(isinstance(k, tuple) for k in keys)

    def test_grouping_by_backend(
        self, sample_results: dict[str, list[AggregatedResult]]
    ) -> None:
        """Test grouping by backend."""
        grouped = aggregate_campaign_with_grouping(
            sample_results, group_by=["backend"]
        )

        # Should have two groups (pytorch, vllm)
        assert len(grouped) == 2
        # Check group keys
        keys = {k[0] for k in grouped.keys()}
        assert "pytorch" in keys
        assert "vllm" in keys

    def test_grouping_by_multiple_fields(
        self, sample_results: dict[str, list[AggregatedResult]]
    ) -> None:
        """Test grouping by multiple fields."""
        grouped = aggregate_campaign_with_grouping(
            sample_results, group_by=["backend", "config_name"]
        )

        # Should have two groups (one per config)
        assert len(grouped) == 2
        # Keys should be tuples of (backend, config_name)
        for key in grouped.keys():
            assert len(key) == 2

    def test_grouped_results_have_metrics(
        self, sample_results: dict[str, list[AggregatedResult]]
    ) -> None:
        """Test that grouped results include expected metrics."""
        grouped = aggregate_campaign_with_grouping(
            sample_results, group_by=["config_name"]
        )

        for metrics in grouped.values():
            assert "n_cycles" in metrics
            assert "energy_j" in metrics
            assert "throughput_tps" in metrics
            # CI fields
            assert "mean" in metrics["energy_j"]


class TestGroupByFlag:
    """Tests for CLI --group-by flag parsing."""

    def test_parse_single_field(self) -> None:
        """Test parsing single field."""
        group_by_str = "backend"
        fields = group_by_str.split(",")
        assert fields == ["backend"]

    def test_parse_multiple_fields(self) -> None:
        """Test parsing comma-separated fields."""
        group_by_str = "backend,batch_size,model_name"
        fields = group_by_str.split(",")
        assert fields == ["backend", "batch_size", "model_name"]

    def test_parse_nested_field(self) -> None:
        """Test parsing nested field path."""
        group_by_str = "pytorch.batch_size"
        fields = group_by_str.split(",")
        assert fields == ["pytorch.batch_size"]
```
  </action>
  <verify>
```bash
pytest tests/unit/cli/test_campaign_group_by.py -v
```
All tests should pass.
  </verify>
  <done>Unit tests for group_by functionality pass.</done>
</task>

<task type="auto">
  <name>Task 3: Unit tests for schema_version validation</name>
  <files>tests/unit/test_schema_version.py</files>
  <action>
Create unit tests for schema_version field and validation:

```python
"""Tests for schema_version field and validation."""

from __future__ import annotations

import pytest

from llenergymeasure.config.loader import validate_config
from llenergymeasure.config.models import CURRENT_SCHEMA_VERSION, ExperimentConfig


class TestSchemaVersionField:
    """Tests for schema_version field on ExperimentConfig."""

    def test_schema_version_field_exists(self) -> None:
        """Test that schema_version field exists on ExperimentConfig."""
        fields = ExperimentConfig.model_fields
        assert "schema_version" in fields

    def test_schema_version_is_optional(self) -> None:
        """Test that schema_version defaults to None."""
        config = ExperimentConfig(
            config_name="test",
            model_name="test-model",
        )
        assert config.schema_version is None

    def test_schema_version_can_be_set(self) -> None:
        """Test that schema_version can be set."""
        config = ExperimentConfig(
            config_name="test",
            model_name="test-model",
            schema_version="3.0.0",
        )
        assert config.schema_version == "3.0.0"

    def test_current_schema_version_constant(self) -> None:
        """Test that CURRENT_SCHEMA_VERSION is defined."""
        assert CURRENT_SCHEMA_VERSION is not None
        assert isinstance(CURRENT_SCHEMA_VERSION, str)
        assert CURRENT_SCHEMA_VERSION == "3.0.0"


class TestSchemaVersionValidation:
    """Tests for schema_version validation warnings."""

    def test_missing_schema_version_emits_info(self) -> None:
        """Test that missing schema_version emits info warning."""
        config = ExperimentConfig(
            config_name="test",
            model_name="test-model",
            schema_version=None,
        )
        warnings = validate_config(config)

        # Find schema version warning
        schema_warnings = [w for w in warnings if w.param == "schema_version"]
        assert len(schema_warnings) == 1
        assert schema_warnings[0].severity == "info"
        assert "SCHEMA_VERSION_MISSING" in schema_warnings[0].code

    def test_matching_schema_version_no_warning(self) -> None:
        """Test that matching schema_version produces no schema warning."""
        config = ExperimentConfig(
            config_name="test",
            model_name="test-model",
            schema_version=CURRENT_SCHEMA_VERSION,
        )
        warnings = validate_config(config)

        # No schema version warnings
        schema_warnings = [w for w in warnings if w.param == "schema_version"]
        assert len(schema_warnings) == 0

    def test_mismatched_schema_version_emits_warning(self) -> None:
        """Test that mismatched schema_version emits warning."""
        config = ExperimentConfig(
            config_name="test",
            model_name="test-model",
            schema_version="2.0.0",  # Old version
        )
        warnings = validate_config(config)

        # Find schema version warning
        schema_warnings = [w for w in warnings if w.param == "schema_version"]
        assert len(schema_warnings) == 1
        assert schema_warnings[0].severity == "warning"
        assert "SCHEMA_VERSION_MISMATCH" in schema_warnings[0].code
        assert "2.0.0" in schema_warnings[0].message

    def test_schema_version_warning_not_blocking(self) -> None:
        """Test that schema warnings are not blocking errors."""
        from llenergymeasure.config.loader import has_blocking_warnings

        config = ExperimentConfig(
            config_name="test",
            model_name="test-model",
            schema_version=None,  # Missing
        )
        warnings = validate_config(config)
        assert not has_blocking_warnings(warnings)
```
  </action>
  <verify>
```bash
pytest tests/unit/test_schema_version.py -v
```
All tests should pass.
  </verify>
  <done>Unit tests for schema_version validation pass.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Phase 2.4 features:
1. `lem config list` command for config discoverability
2. `lem campaign --group-by` flag for flexible result grouping
3. schema_version field with validation warnings
4. Smoke test infrastructure with warning capture
5. Updated example configs with schema v3.0.0
  </what-built>
  <how-to-verify>
1. Run `lem config list` and verify it shows available configs
2. Run `lem config list -d configs/examples` and verify examples are listed with correct backends
3. Run `lem config validate configs/examples/pytorch_example.yaml` and verify no schema warnings
4. Run `lem campaign --help` and verify `--group-by` option is documented
5. Check that example configs have `schema_version: '3.0.0'` at top
6. Run unit tests: `pytest tests/unit/cli/test_config_list.py tests/unit/cli/test_campaign_group_by.py tests/unit/test_schema_version.py -v`
  </how-to-verify>
  <resume-signal>Type "approved" if all features work correctly, or describe issues found.</resume-signal>
</task>

</tasks>

<verification>
1. All unit tests pass: `pytest tests/unit/cli/test_config_list.py tests/unit/cli/test_campaign_group_by.py tests/unit/test_schema_version.py -v`
2. `lem config list` shows available configs
3. `lem campaign --group-by backend` accepted
4. Example configs validate without schema warnings
5. Smoke test captures warnings correctly
</verification>

<success_criteria>
- Unit tests for config list command pass
- Unit tests for group_by functionality pass
- Unit tests for schema_version validation pass
- Manual verification confirms all CLI features work end-to-end
- No regressions in existing functionality
</success_criteria>

<output>
After completion, create `.planning/phases/02.4-cli-polish-testing/02.4-04-SUMMARY.md`
</output>
