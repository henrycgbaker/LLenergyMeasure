---
phase: 02-campaign-orchestrator
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/results/aggregation.py
  - src/llenergymeasure/results/bootstrap.py
autonomous: true

must_haves:
  truths:
    - "bootstrap_ci() computes 95% confidence intervals using percentile method with 1000 iterations"
    - "bootstrap_ci() is deterministic with seed=42 default"
    - "bootstrap_ci() returns warning when fewer than 3 samples provided"
    - "Campaign-level aggregation groups results by config and computes CIs per metric"
    - "Bootstrap CIs computed for energy, throughput, TTFT, and ITL metrics"
  artifacts:
    - path: "src/llenergymeasure/results/bootstrap.py"
      provides: "bootstrap_ci() function and BootstrapResult model"
      exports: ["bootstrap_ci", "BootstrapResult"]
    - path: "src/llenergymeasure/results/aggregation.py"
      provides: "aggregate_campaign_results() function for cross-cycle aggregation with CIs"
      contains: "def aggregate_campaign_results"
  key_links:
    - from: "src/llenergymeasure/results/aggregation.py"
      to: "src/llenergymeasure/results/bootstrap.py"
      via: "import bootstrap_ci"
      pattern: "from llenergymeasure.results.bootstrap import bootstrap_ci"
---

<objective>
Implement bootstrap resampling for 95% confidence intervals and campaign-level result aggregation.

Purpose: MEAS-08 requirement — users running multi-cycle campaigns need statistically rigorous confidence intervals. The existing aggregation module handles single-experiment cross-process aggregation; this adds cross-cycle aggregation with bootstrap CIs.
Output: New bootstrap.py module + aggregate_campaign_results() function in aggregation.py.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-campaign-orchestrator/02-RESEARCH.md
@src/llenergymeasure/results/aggregation.py
@src/llenergymeasure/domain/experiment.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create bootstrap CI module</name>
  <files>src/llenergymeasure/results/bootstrap.py</files>
  <action>
Create `src/llenergymeasure/results/bootstrap.py` with bootstrap confidence interval computation.

**BootstrapResult Pydantic model:**
- `mean: float`
- `std: float`
- `ci_lower: float | None = None` — None when too few samples
- `ci_upper: float | None = None`
- `n_samples: int`
- `confidence: float`
- `warning: str | None = None` — Warning message when CI is unreliable

**bootstrap_ci function:**
```python
def bootstrap_ci(
    samples: list[float] | np.ndarray,
    n_iterations: int = 1000,
    confidence: float = 0.95,
    seed: int = 42,
) -> BootstrapResult:
```

Implementation:
1. Convert samples to numpy array
2. If len < 2: return mean/std only, ci_lower/ci_upper=None, warning="< 2 samples, CI not computable"
3. If len < 3: return mean/std with CI but add warning="< 3 samples, CI unreliable (recommend >= 3 cycles)"
4. Bootstrap loop:
   - Create `rng = np.random.default_rng(seed)` for reproducibility
   - For n_iterations: resample with replacement, compute mean of resample
   - Percentile method: `alpha = 1 - confidence`, lower = `alpha/2 * 100` percentile, upper = `(1-alpha/2) * 100` percentile
5. Return BootstrapResult with all fields

Also add a convenience function:
```python
def compute_metric_ci(
    values: list[float],
    metric_name: str = "",
    confidence: float = 0.95,
) -> dict[str, Any]:
    """Compute CI for a named metric, returning a dict suitable for JSON serialization."""
```
Returns dict with keys: mean, std, ci_lower, ci_upper, n_samples, confidence, warning (if any).

Use `from __future__ import annotations`, numpy (already a dependency), Pydantic BaseModel.
Add both functions and BootstrapResult to `__all__`.
  </action>
  <verify>
    python -c "
from llenergymeasure.results.bootstrap import bootstrap_ci, BootstrapResult
import numpy as np

# Test with good data (5 samples)
result = bootstrap_ci([10.0, 12.0, 11.0, 13.0, 10.5])
assert result.n_samples == 5
assert result.ci_lower is not None
assert result.ci_upper is not None
assert result.ci_lower < result.mean < result.ci_upper
assert result.warning is None
print(f'5 samples: mean={result.mean:.2f}, CI=[{result.ci_lower:.2f}, {result.ci_upper:.2f}]')

# Test determinism
result2 = bootstrap_ci([10.0, 12.0, 11.0, 13.0, 10.5])
assert result.ci_lower == result2.ci_lower
assert result.ci_upper == result2.ci_upper
print('Deterministic: same seed produces same results')

# Test with 2 samples (warning)
result3 = bootstrap_ci([10.0, 12.0])
assert result3.warning is not None
assert 'unreliable' in result3.warning.lower() or '< 3' in result3.warning
print(f'2 samples warning: {result3.warning}')

# Test with 1 sample (no CI)
result4 = bootstrap_ci([10.0])
assert result4.ci_lower is None
assert result4.ci_upper is None
print(f'1 sample: no CI, warning={result4.warning}')

print('All bootstrap tests passed')
"
  </verify>
  <done>bootstrap_ci() produces deterministic 95% CIs via percentile method. Graceful degradation for < 3 samples with warnings.</done>
</task>

<task type="auto">
  <name>Task 2: Add campaign-level aggregation with CIs</name>
  <files>src/llenergymeasure/results/aggregation.py</files>
  <action>
Add a campaign-level aggregation function to the existing aggregation.py. Do NOT modify any existing functions — this is purely additive.

Add at the bottom of the file (before `__all__` if it exists):

```python
def aggregate_campaign_results(
    results_by_config: dict[str, list[AggregatedResult]],
    confidence: float = 0.95,
) -> dict[str, dict[str, Any]]:
    """Aggregate campaign results by config with bootstrap confidence intervals.

    Groups results from multiple cycles of the same config and computes
    bootstrap CIs for key metrics.

    Args:
        results_by_config: Dict mapping config name to list of AggregatedResult
            objects (one per cycle).
        confidence: Confidence level for bootstrap CI (default 0.95).

    Returns:
        Dict mapping config name to aggregated metrics with CIs.
    """
```

Implementation:
1. Import `from llenergymeasure.results.bootstrap import bootstrap_ci`
2. For each config_name, extract metric arrays across cycles:
   - `energy_kwh` samples from each AggregatedResult
   - `throughput_tokens_per_sec` from each result
   - `total_tokens` from each result
   - Latency metrics if available (check `latency_statistics` attribute)
3. For each metric array, call `bootstrap_ci(samples, confidence=confidence)`
4. Return structured dict:
```python
{
    "config_name": {
        "n_cycles": len(results),
        "energy_kwh": bootstrap_result.model_dump(),
        "throughput_tps": bootstrap_result.model_dump(),
        "total_tokens": bootstrap_result.model_dump(),
        # latency metrics if available
    }
}
```

Look at the AggregatedResult model in domain/experiment.py to determine exact field names for energy, throughput, tokens, and latency. Use the actual field names, not guesses.

Add `aggregate_campaign_results` to `__all__` if it exists.
  </action>
  <verify>
    python -c "
from llenergymeasure.results.aggregation import aggregate_campaign_results
print('aggregate_campaign_results importable')
# Full test requires AggregatedResult instances — verified via unit tests in Plan 08
"
  </verify>
  <done>Campaign-level aggregation function exists, computes bootstrap CIs across cycles for energy, throughput, and latency metrics.</done>
</task>

</tasks>

<verification>
- `python -c "from llenergymeasure.results.bootstrap import bootstrap_ci, BootstrapResult"` succeeds
- `python -c "from llenergymeasure.results.aggregation import aggregate_campaign_results"` succeeds
- Bootstrap CI is deterministic (seed=42)
- `ruff check src/llenergymeasure/results/bootstrap.py` passes
- Existing tests still pass: `pytest tests/unit/results/ -x -q`
</verification>

<success_criteria>
- bootstrap_ci() with 1000 iterations, seed=42, percentile method
- Graceful degradation: 1 sample = no CI, 2 samples = CI with warning, 3+ = reliable CI
- Campaign aggregation groups by config name, computes CIs for all key metrics
- Deterministic results for reproducibility
</success_criteria>

<output>
After completion, create `.planning/phases/02-campaign-orchestrator/02-03-SUMMARY.md`
</output>
