---
phase: 02-campaign-orchestrator
plan: 08
type: execute
wave: 4
depends_on: ["02-06", "02-07"]
files_modified:
  - tests/unit/orchestration/test_container.py
  - tests/unit/orchestration/test_manifest.py
  - tests/unit/orchestration/test_grid.py
  - tests/unit/results/test_bootstrap.py
  - tests/unit/config/test_campaign_config.py
autonomous: false

must_haves:
  truths:
    - "Unit tests cover all new Phase 2 modules (container, manifest, grid, bootstrap, campaign config)"
    - "Bootstrap CI tests verify determinism, boundary cases, and statistical properties"
    - "Grid expansion tests verify cartesian product, backend filtering, and validation"
    - "Manifest tests verify atomic persistence, resume filtering, and status tracking"
    - "Campaign config tests verify new model defaults, validation, and backwards compatibility"
    - "All tests pass: pytest tests/unit/ -x -q"
    - "UAT checkpoint validates cross-backend campaign dispatch"
  artifacts:
    - path: "tests/unit/orchestration/test_container.py"
      provides: "ContainerManager unit tests (mocked Docker)"
    - path: "tests/unit/orchestration/test_manifest.py"
      provides: "ManifestManager unit tests (atomic write, resume)"
    - path: "tests/unit/orchestration/test_grid.py"
      provides: "Grid expansion and validation tests"
    - path: "tests/unit/results/test_bootstrap.py"
      provides: "Bootstrap CI tests (determinism, boundaries, statistics)"
    - path: "tests/unit/config/test_campaign_config.py"
      provides: "Extended campaign config model tests"
  key_links:
    - from: "tests/"
      to: "src/llenergymeasure/"
      via: "import and test all Phase 2 modules"
      pattern: "from llenergymeasure"
---

<objective>
Write unit tests for all Phase 2 modules and perform UAT checkpoint for cross-backend campaign orchestration.

Purpose: CAMP-08 (UAT round 2) + comprehensive unit test coverage for all new modules. Validates that the campaign orchestrator dispatches cross-backend campaigns correctly.
Output: 5 test files covering all Phase 2 modules, UAT verification checkpoint.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-campaign-orchestrator/02-01-SUMMARY.md
@.planning/phases/02-campaign-orchestrator/02-02-SUMMARY.md
@.planning/phases/02-campaign-orchestrator/02-03-SUMMARY.md
@.planning/phases/02-campaign-orchestrator/02-04-SUMMARY.md
@.planning/phases/02-campaign-orchestrator/02-05-SUMMARY.md
@.planning/phases/02-campaign-orchestrator/02-06-SUMMARY.md
@.planning/phases/02-campaign-orchestrator/02-07-SUMMARY.md
@tests/CLAUDE.md
@src/llenergymeasure/orchestration/container.py
@src/llenergymeasure/orchestration/manifest.py
@src/llenergymeasure/orchestration/grid.py
@src/llenergymeasure/results/bootstrap.py
@src/llenergymeasure/config/campaign_config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Write unit tests for all Phase 2 modules</name>
  <files>
    tests/unit/orchestration/test_container.py
    tests/unit/orchestration/test_manifest.py
    tests/unit/orchestration/test_grid.py
    tests/unit/results/test_bootstrap.py
    tests/unit/config/test_campaign_config.py
  </files>
  <action>
Create unit tests for all Phase 2 modules. Check tests/CLAUDE.md for testing conventions (test discovery, fixtures, naming). Create `tests/unit/orchestration/` directory if it doesn't exist (add `__init__.py`).

**test_bootstrap.py** (most important — pure logic, no mocks needed):
- `test_bootstrap_ci_basic`: 5 samples produces valid CI with lower < mean < upper
- `test_bootstrap_ci_deterministic`: Same inputs + seed produce identical outputs
- `test_bootstrap_ci_single_sample`: Returns no CI (None), has warning
- `test_bootstrap_ci_two_samples`: Returns CI with "unreliable" warning
- `test_bootstrap_ci_many_samples`: 100 samples produces tight CI
- `test_bootstrap_ci_confidence_levels`: 0.99 CI is wider than 0.90 CI
- `test_bootstrap_ci_constant_values`: All same value produces zero-width CI
- `test_bootstrap_result_model`: BootstrapResult serializes to dict correctly

**test_manifest.py** (file system tests with tmp_path fixture):
- `test_manifest_entry_creation`: CampaignManifestEntry validates all fields
- `test_manifest_status_tracking`: completed_count, pending_count, is_complete properties
- `test_manifest_get_remaining`: Returns pending + failed entries
- `test_manifest_manager_save_load`: Round-trip persistence (save then load)
- `test_manifest_manager_atomic_write`: Temp file renamed (check no .tmp file after save)
- `test_manifest_manager_load_missing`: Returns None when file doesn't exist
- `test_manifest_update_entry`: Update status on specific exp_id
- `test_manifest_config_hash_change_detection`: check_config_changed() detects changes

**test_grid.py** (pure logic + Pydantic validation):
- `test_expand_single_backend`: 1 backend x 2 shared params = 2 configs
- `test_expand_multi_backend`: 2 backends x 2 shared params = 4 configs
- `test_expand_with_backend_overrides`: Backend-specific params expand correctly per backend
- `test_expand_with_models`: Models axis included in cartesian product
- `test_expand_empty_grid`: Empty shared params produces 1 config per backend
- `test_validate_filters_invalid`: Invalid configs (e.g., bad backend) filtered with reason
- `test_validate_keeps_valid`: Valid configs pass through
- `test_grid_expansion_result_summary`: Summary string shows correct counts

**test_container.py** (mocked Docker — python-on-whales not needed):
- `test_container_manager_init`: Creates DockerClient with compose file
- `test_start_services`: Calls docker.compose.up with correct args
- `test_execute_experiment`: Calls docker.compose.execute with command
- `test_health_check_healthy`: Parses NVML output, returns healthy
- `test_health_check_unhealthy`: High memory returns unhealthy
- `test_restart_service`: Calls docker.compose.restart
- `test_teardown`: Calls docker.compose.down
- `test_context_manager`: teardown called on exit
- Mock `python_on_whales.DockerClient` using `unittest.mock.patch` or `monkeypatch`

**test_campaign_config.py** (Pydantic validation):
- `test_campaign_config_backwards_compat`: Existing config-list mode still works
- `test_campaign_config_grid_mode`: Grid config without configs list is valid
- `test_campaign_grid_config_validation`: Invalid backend name rejected
- `test_campaign_health_check_defaults`: Default values correct
- `test_campaign_cold_start_defaults`: Default is force_cold_start=False
- `test_campaign_io_defaults`: Default paths correct
- `test_campaign_daemon_config`: Interval parsing works (if validator added)
- `test_campaign_config_requires_source`: Neither configs nor grid raises validation error

Run all tests: `pytest tests/unit/ -x -q`
  </action>
  <verify>
    cd /home/h.baker@hertie-school.lan/workspace/llm-efficiency-measurement-tool && pytest tests/unit/orchestration/test_container.py tests/unit/results/test_bootstrap.py tests/unit/orchestration/test_manifest.py tests/unit/orchestration/test_grid.py tests/unit/config/test_campaign_config.py -x -q 2>&1 | tail -10
  </verify>
  <done>Unit tests cover all Phase 2 modules. Bootstrap CI verified for determinism and boundaries. Manifest persistence verified with atomic writes. Grid expansion verified for cartesian products. Container manager verified with mocked Docker. Campaign config backwards compatibility verified.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete Phase 2 Campaign Orchestrator:
- CampaignConfig extended with grid, manifest, cold start, health check, daemon, IO config
- ContainerManager for Docker lifecycle (up/exec/down) via python-on-whales
- Campaign manifest with atomic persistence and resume support
- Backend-aware grid expansion with Pydantic-first validation
- Bootstrap CI for multi-cycle campaign statistics
- Integrated campaign CLI with exec dispatch, manifest tracking, grid support
- Daemon mode with scheduled start times, interval-based repetition, and duration limits
- SSOT introspection extended for campaign config parameters
- Unit tests for all new modules
  </what-built>
  <how-to-verify>
1. **Config validation**: Run `python -c "from llenergymeasure.config.campaign_config import CampaignConfig, CampaignGridConfig; print('Config models OK')"` — should succeed
2. **Grid expansion**: Run `python -c "from llenergymeasure.orchestration.grid import expand_campaign_grid; print('Grid OK')"` — should succeed
3. **Bootstrap CI**: Run `python -c "from llenergymeasure.results.bootstrap import bootstrap_ci; r = bootstrap_ci([10,12,11,13,10.5]); print(f'CI: [{r.ci_lower:.2f}, {r.ci_upper:.2f}]')"` — should show narrow CI around 11.3
4. **Unit tests**: Run `pytest tests/unit/ -x -q` — all should pass
5. **CLI help**: Run `lem campaign --help` — should show new options (--resume, --force-cold-start, --validate-only)
6. **Campaign dry-run** (if Docker available): Create a campaign YAML with grid config, run `lem campaign test_campaign.yaml --dry-run` — should show expansion summary
7. **[GPU required] UAT**: If on GPU machine with Docker, run a simple 2-backend campaign:
   ```yaml
   campaign_name: "phase2-uat"
   grid:
     backends: [pytorch]
     shared:
       fp_precision: [float16]
   execution:
     cycles: 2
     structure: interleaved
   ```
   Run `lem campaign uat_campaign.yaml --dataset alpaca -n 5` and verify:
   - Containers start via docker compose up (not run --rm)
   - Experiments dispatched via exec
   - Manifest created and updated
   - Bootstrap CIs shown for 2-cycle results
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues to address</resume-signal>
</task>

</tasks>

<verification>
- All unit tests pass: `pytest tests/unit/ -x -q`
- Existing tests not broken: `pytest tests/ -x -q`
- No ruff errors: `ruff check src/llenergymeasure/`
- UAT verifies cross-backend campaign dispatch
</verification>

<success_criteria>
- 30+ unit tests covering all Phase 2 modules
- All tests pass
- UAT validates campaign orchestrator on real GPU (or documented for later GPU testing)
- No regressions in existing test suite
</success_criteria>

<output>
After completion, create `.planning/phases/02-campaign-orchestrator/02-08-SUMMARY.md`
</output>
