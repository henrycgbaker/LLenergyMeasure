---
phase: 03-gpu-routing-fix
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - tests/unit/config/test_parallelism_validation.py
  - tests/unit/cli/test_docker_gpu_propagation.py
autonomous: false
user_setup: []

must_haves:
  truths:
    - "Unit tests verify parallelism constraint validation catches violations"
    - "Unit tests verify GPU env var propagation in Docker command builder"
    - "Multi-GPU scenario validated manually on real hardware"
  artifacts:
    - path: "tests/unit/config/test_parallelism_validation.py"
      provides: "Parallelism validation unit tests"
      contains: "test_vllm_tensor_parallel"
    - path: "tests/unit/cli/test_docker_gpu_propagation.py"
      provides: "GPU propagation unit tests"
      contains: "test_build_docker_command_with_gpus"
  key_links:
    - from: "tests/unit/config/test_parallelism_validation.py"
      to: "src/llenergymeasure/config/validation.py"
      via: "import validate_parallelism_constraints"
      pattern: "from llenergymeasure.config.validation import"
---

<objective>
Add unit tests for GPU routing and parallelism validation, with manual verification checkpoint.

Purpose: Ensure the GPU routing fix is correct and catches regressions, plus verify on real hardware.

Output: Unit tests for validation and Docker propagation, manual verification of multi-GPU scenarios.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-gpu-routing-fix/03-01-SUMMARY.md
@.planning/phases/03-gpu-routing-fix/03-02-SUMMARY.md
@src/llenergymeasure/config/validation.py
@src/llenergymeasure/cli/campaign.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add parallelism validation unit tests</name>
  <files>tests/unit/config/test_parallelism_validation.py</files>
  <action>
Create comprehensive unit tests for parallelism validation:

```python
"""Unit tests for parallelism constraint validation."""

import pytest

from llenergymeasure.config.models import ExperimentConfig
from llenergymeasure.config.backend_configs import VLLMConfig, TensorRTConfig, PyTorchConfig
from llenergymeasure.config.validation import validate_parallelism_constraints


class TestVLLMParallelismValidation:
    """Tests for vLLM tensor/pipeline parallelism validation."""

    def test_vllm_tensor_parallel_exceeds_gpus_returns_error(self):
        """tensor_parallel_size > len(gpus) should return error warning."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="vllm",
            gpus=[0],  # Only 1 GPU
            vllm=VLLMConfig(tensor_parallel_size=4),  # Wants 4 GPUs
        )
        warnings = validate_parallelism_constraints(config)

        assert len(warnings) == 1
        assert warnings[0].severity == "error"
        assert "tensor_parallel_size=4" in warnings[0].message
        assert "exceeds available GPUs (1)" in warnings[0].message
        assert warnings[0].suggestion is not None

    def test_vllm_tensor_parallel_equals_gpus_passes(self):
        """tensor_parallel_size == len(gpus) should pass."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="vllm",
            gpus=[0, 1, 2, 3],  # 4 GPUs
            vllm=VLLMConfig(tensor_parallel_size=4),  # Wants 4 GPUs
        )
        warnings = validate_parallelism_constraints(config)
        assert len(warnings) == 0

    def test_vllm_pipeline_parallel_total_exceeds_gpus(self):
        """TP * PP > len(gpus) should return error."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="vllm",
            gpus=[0, 1],  # 2 GPUs
            vllm=VLLMConfig(tensor_parallel_size=2, pipeline_parallel_size=2),  # Wants 4
        )
        warnings = validate_parallelism_constraints(config)

        # Should have error about TP*PP
        errors = [w for w in warnings if w.severity == "error"]
        assert len(errors) >= 1
        assert any("pipeline_parallel" in w.field for w in errors)

    def test_vllm_no_config_passes(self):
        """vllm backend without vllm config section should pass (defaults)."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="vllm",
            gpus=[0],
        )
        warnings = validate_parallelism_constraints(config)
        assert len(warnings) == 0


class TestTensorRTParallelismValidation:
    """Tests for TensorRT tp/pp validation."""

    def test_tensorrt_tp_exceeds_gpus_returns_error(self):
        """tp_size > len(gpus) should return error warning."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="tensorrt",
            gpus=[0, 1],  # 2 GPUs
            fp_precision="float16",  # Required for TensorRT
            tensorrt=TensorRTConfig(tp_size=4),  # Wants 4 GPUs
        )
        warnings = validate_parallelism_constraints(config)

        assert len(warnings) == 1
        assert warnings[0].severity == "error"
        assert "tp_size=4" in warnings[0].message
        assert "exceeds available GPUs (2)" in warnings[0].message

    def test_tensorrt_tp_equals_gpus_passes(self):
        """tp_size == len(gpus) should pass."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="tensorrt",
            gpus=[0, 1, 2, 3],
            fp_precision="float16",
            tensorrt=TensorRTConfig(tp_size=4),
        )
        warnings = validate_parallelism_constraints(config)
        assert len(warnings) == 0


class TestPyTorchParallelismValidation:
    """Tests for PyTorch data parallelism validation."""

    def test_pytorch_num_processes_exceeds_gpus_returns_error(self):
        """num_processes > len(gpus) should return error."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="pytorch",
            gpus=[0],  # 1 GPU
            pytorch=PyTorchConfig(num_processes=4),  # Wants 4
        )
        warnings = validate_parallelism_constraints(config)

        assert len(warnings) == 1
        assert warnings[0].severity == "error"
        assert "num_processes=4" in warnings[0].message

    def test_pytorch_default_passes(self):
        """Default num_processes=1 with 1 GPU should pass."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="pytorch",
            gpus=[0],
            pytorch=PyTorchConfig(),  # Default num_processes=1
        )
        warnings = validate_parallelism_constraints(config)
        assert len(warnings) == 0


class TestEdgeCases:
    """Edge case tests for parallelism validation."""

    def test_empty_gpus_defaults_to_one(self):
        """Empty gpus list should be treated as 1 GPU."""
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="vllm",
            gpus=[],  # Edge case - empty
            vllm=VLLMConfig(tensor_parallel_size=2),
        )
        warnings = validate_parallelism_constraints(config)

        # Should fail because 2 > 1 (default)
        assert len(warnings) == 1
        assert warnings[0].severity == "error"

    def test_non_matching_backend_skipped(self):
        """Validation only checks the active backend."""
        # This has vllm config but pytorch backend - vllm validation shouldn't run
        # (This would be caught by Pydantic validation anyway, but good to test)
        config = ExperimentConfig(
            config_name="test",
            model_name="gpt2",
            backend="pytorch",
            gpus=[0],
            pytorch=PyTorchConfig(),
        )
        warnings = validate_parallelism_constraints(config)
        assert len(warnings) == 0
```
  </action>
  <verify>
Run `pytest tests/unit/config/test_parallelism_validation.py -v` - all tests should pass.
  </verify>
  <done>
Comprehensive unit tests for parallelism validation covering vLLM, TensorRT, PyTorch, and edge cases.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add GPU env propagation unit tests</name>
  <files>tests/unit/cli/test_docker_gpu_propagation.py</files>
  <action>
Create unit tests for GPU environment variable propagation in Docker commands:

```python
"""Unit tests for GPU environment propagation to Docker containers."""

import pytest


class TestBuildDockerCommand:
    """Tests for _build_docker_command GPU propagation."""

    def test_build_docker_command_with_gpus_sets_nvidia_visible_devices(self):
        """gpus parameter should set NVIDIA_VISIBLE_DEVICES."""
        from llenergymeasure.cli.campaign import _build_docker_command

        cmd = _build_docker_command(
            backend="vllm",
            config_path="/app/configs/test.yaml",
            dataset=None,
            sample_size=None,
            results_dir=None,
            gpus=[0, 1, 2, 3],
        )

        # Check that -e NVIDIA_VISIBLE_DEVICES=0,1,2,3 is in command
        assert "-e" in cmd
        nvidia_idx = None
        for i, arg in enumerate(cmd):
            if arg == "-e" and i + 1 < len(cmd) and "NVIDIA_VISIBLE_DEVICES" in cmd[i + 1]:
                nvidia_idx = i + 1
                break

        assert nvidia_idx is not None, "NVIDIA_VISIBLE_DEVICES not found in command"
        assert cmd[nvidia_idx] == "NVIDIA_VISIBLE_DEVICES=0,1,2,3"

    def test_build_docker_command_with_gpus_sets_cuda_visible_devices_remapped(self):
        """gpus parameter should set CUDA_VISIBLE_DEVICES with remapped indices."""
        from llenergymeasure.cli.campaign import _build_docker_command

        cmd = _build_docker_command(
            backend="vllm",
            config_path="/app/configs/test.yaml",
            dataset=None,
            sample_size=None,
            results_dir=None,
            gpus=[2, 5],  # Non-sequential host GPUs
        )

        # CUDA_VISIBLE_DEVICES should be 0,1 (remapped), not 2,5
        cuda_idx = None
        for i, arg in enumerate(cmd):
            if arg == "-e" and i + 1 < len(cmd) and "CUDA_VISIBLE_DEVICES" in cmd[i + 1]:
                cuda_idx = i + 1
                break

        assert cuda_idx is not None, "CUDA_VISIBLE_DEVICES not found in command"
        assert cmd[cuda_idx] == "CUDA_VISIBLE_DEVICES=0,1"

    def test_build_docker_command_without_gpus_no_env_vars(self):
        """No gpus parameter should not set GPU env vars."""
        from llenergymeasure.cli.campaign import _build_docker_command

        cmd = _build_docker_command(
            backend="pytorch",
            config_path="/app/configs/test.yaml",
            dataset=None,
            sample_size=None,
            results_dir=None,
            gpus=None,  # No GPU specification
        )

        # Should not have NVIDIA_VISIBLE_DEVICES in command
        cmd_str = " ".join(cmd)
        # Note: might still have campaign_context env vars, but not GPU ones
        nvidia_found = any("NVIDIA_VISIBLE_DEVICES" in arg for arg in cmd)
        # This depends on implementation - may want to allow default behaviour
        # Adjust assertion based on intended behaviour

    def test_build_docker_command_single_gpu(self):
        """Single GPU should set correct env vars."""
        from llenergymeasure.cli.campaign import _build_docker_command

        cmd = _build_docker_command(
            backend="pytorch",
            config_path="/app/configs/test.yaml",
            dataset=None,
            sample_size=None,
            results_dir=None,
            gpus=[3],  # Single GPU, index 3
        )

        nvidia_val = None
        cuda_val = None
        for i, arg in enumerate(cmd):
            if arg == "-e" and i + 1 < len(cmd):
                if "NVIDIA_VISIBLE_DEVICES" in cmd[i + 1]:
                    nvidia_val = cmd[i + 1].split("=")[1]
                if "CUDA_VISIBLE_DEVICES" in cmd[i + 1]:
                    cuda_val = cmd[i + 1].split("=")[1]

        assert nvidia_val == "3"
        assert cuda_val == "0"  # Remapped to 0


class TestEarlyCudaVisibleDevicesSetup:
    """Tests for _early_cuda_visible_devices_setup container detection."""

    def test_container_context_uses_remapped_indices(self, monkeypatch):
        """When NVIDIA_VISIBLE_DEVICES is set, use remapped indices."""
        import os

        # Simulate container context
        monkeypatch.setenv("NVIDIA_VISIBLE_DEVICES", "2,5")

        # The function reads from sys.argv for config path
        # This is a unit test of the logic, not the full function
        # Test the remapping logic directly

        nvidia_visible = os.environ.get("NVIDIA_VISIBLE_DEVICES", "")
        if nvidia_visible and nvidia_visible != "all":
            gpu_count = len(nvidia_visible.split(","))
            cuda_devices = ",".join(str(i) for i in range(gpu_count))
        else:
            cuda_devices = "2,5"  # Would use config.gpus

        assert cuda_devices == "0,1"

    def test_local_context_uses_config_gpus(self, monkeypatch):
        """When NVIDIA_VISIBLE_DEVICES is not set, use config.gpus directly."""
        import os

        # Ensure not in container context
        monkeypatch.delenv("NVIDIA_VISIBLE_DEVICES", raising=False)

        nvidia_visible = os.environ.get("NVIDIA_VISIBLE_DEVICES", "")
        gpus = [2, 5]  # From config

        if nvidia_visible and nvidia_visible != "all":
            cuda_devices = ",".join(str(i) for i in range(len(nvidia_visible.split(","))))
        else:
            cuda_devices = ",".join(str(g) for g in gpus)

        assert cuda_devices == "2,5"

    def test_nvidia_all_uses_config_gpus(self, monkeypatch):
        """When NVIDIA_VISIBLE_DEVICES=all, use config.gpus directly."""
        monkeypatch.setenv("NVIDIA_VISIBLE_DEVICES", "all")

        nvidia_visible = "all"
        gpus = [0, 1]

        if nvidia_visible and nvidia_visible != "all":
            cuda_devices = ",".join(str(i) for i in range(len(nvidia_visible.split(","))))
        else:
            cuda_devices = ",".join(str(g) for g in gpus)

        assert cuda_devices == "0,1"
```
  </action>
  <verify>
Run `pytest tests/unit/cli/test_docker_gpu_propagation.py -v` - all tests should pass.
  </verify>
  <done>
Unit tests for Docker GPU env propagation covering NVIDIA_VISIBLE_DEVICES, CUDA_VISIBLE_DEVICES remapping, and container detection.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
GPU routing fix with:
1. NVIDIA_VISIBLE_DEVICES/CUDA_VISIBLE_DEVICES propagation to Docker containers
2. Fail-fast parallelism validation
3. Comprehensive unit tests
  </what-built>
  <how-to-verify>
**Pre-requisites:** Multi-GPU system with Docker and vLLM image built.

**Test 1: Parallelism validation catches errors**
```bash
# Create invalid config
cat > /tmp/bad_tp.yaml << 'EOF'
config_name: bad-tp-test
model_name: gpt2
backend: vllm
gpus: [0]
vllm:
  tensor_parallel_size: 4
EOF

# Should fail with clear error
lem config validate /tmp/bad_tp.yaml
# Expected: Error message mentioning tensor_parallel_size=4 exceeds GPUs
```

**Test 2: GPU env vars passed to Docker (dry inspection)**
```bash
# Create valid multi-GPU config
cat > /tmp/good_tp.yaml << 'EOF'
config_name: good-tp-test
model_name: meta-llama/Llama-2-7b-hf
backend: vllm
gpus: [0, 1]
dataset:
  name: alpaca
  sample_size: 5
vllm:
  tensor_parallel_size: 2
EOF

# If you have 2+ GPUs available, run campaign with --dry-run
lem campaign /tmp/good_tp.yaml --campaign-name gpu-test --dry-run
# Should show execution plan without errors
```

**Test 3: Actual multi-GPU vLLM run (if hardware available)**
```bash
# Only if you have 2+ GPUs:
docker compose run --rm vllm lem experiment /app/configs/examples/vllm_example.yaml \
  --dataset alpaca -n 5

# Verify GPU visibility inside container
docker compose run --rm vllm python -c "import torch; print(f'GPUs visible: {torch.cuda.device_count()}')"
```

**Expected behaviour:**
- Test 1: Clear error about parallelism constraint
- Test 2: Dry-run shows execution plan without validation errors
- Test 3: vLLM initialises correctly with tensor parallelism (if hardware available)
  </how-to-verify>
  <resume-signal>Type "approved" if GPU routing works correctly, or describe any issues observed.</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:
1. `pytest tests/unit/config/test_parallelism_validation.py tests/unit/cli/test_docker_gpu_propagation.py -v` passes
2. Full test suite passes: `pytest tests/unit/ -v --tb=short`
3. Manual verification on multi-GPU system confirms GPU routing works
</verification>

<success_criteria>
- Unit tests for parallelism validation pass
- Unit tests for GPU env propagation pass
- Manual verification confirms:
  - Invalid parallelism configs fail with clear errors
  - Valid multi-GPU configs work correctly
  - vLLM/TensorRT tensor parallelism initialises (if hardware available)
</success_criteria>

<output>
After completion, create `.planning/phases/03-gpu-routing-fix/03-03-SUMMARY.md`
</output>
