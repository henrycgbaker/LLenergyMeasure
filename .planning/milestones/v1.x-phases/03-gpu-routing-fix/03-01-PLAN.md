---
phase: 03-gpu-routing-fix
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/llenergymeasure/cli/campaign.py
  - src/llenergymeasure/orchestration/launcher.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Docker containers receive correct GPU visibility via NVIDIA_VISIBLE_DEVICES"
    - "CUDA_VISIBLE_DEVICES inside container uses remapped indices (0,1,2,...)"
    - "config.gpus is the single source of truth for GPU selection"
  artifacts:
    - path: "src/llenergymeasure/cli/campaign.py"
      provides: "GPU env var propagation in _build_docker_command"
      contains: "NVIDIA_VISIBLE_DEVICES"
    - path: "src/llenergymeasure/orchestration/launcher.py"
      provides: "Remapped CUDA_VISIBLE_DEVICES handling"
      contains: "_early_cuda_visible_devices_setup"
  key_links:
    - from: "src/llenergymeasure/cli/campaign.py"
      to: "docker compose run -e"
      via: "_build_docker_command env_vars"
      pattern: "NVIDIA_VISIBLE_DEVICES.*join"
---

<objective>
Propagate config.gpus to Docker containers via NVIDIA_VISIBLE_DEVICES for correct GPU routing.

Purpose: Fix the critical bug where vLLM/TensorRT tensor parallelism fails because containers don't receive the correct GPU visibility settings.

Output: Modified campaign.py and launcher.py that correctly propagate GPU selection to containers.
</objective>

<execution_context>
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/workflows/execute-plan.md
@/home/h.baker@hertie-school.lan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-gpu-routing-fix/03-RESEARCH.md
@src/llenergymeasure/cli/campaign.py
@src/llenergymeasure/orchestration/launcher.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add GPU env var propagation to Docker command builder</name>
  <files>src/llenergymeasure/cli/campaign.py</files>
  <action>
Modify `_build_docker_command()` to propagate GPU selection via environment variables:

1. Add `gpus: list[int] | None = None` parameter to function signature
2. Build env_vars dict before the cmd list:
   ```python
   env_vars = {}
   if gpus:
       # NVIDIA_VISIBLE_DEVICES controls which GPUs are mounted by container runtime
       env_vars["NVIDIA_VISIBLE_DEVICES"] = ",".join(str(g) for g in gpus)
       # CUDA_VISIBLE_DEVICES inside container uses remapped indices (0,1,2,...)
       env_vars["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in range(len(gpus)))
   ```
3. Add env_vars to cmd via `-e` flags (BEFORE existing campaign_context handling):
   ```python
   for key, value in env_vars.items():
       cmd.extend(["-e", f"{key}={value}"])
   ```
4. Update all call sites of `_build_docker_command()` to pass gpus from experiment config:
   - In `_run_single_experiment()`, extract gpus from config_data and pass to builder
   - Example: `gpus=config_data.get("gpus", [0])`

WHY: NVIDIA Container Toolkit uses NVIDIA_VISIBLE_DEVICES to control which GPUs are mounted. Inside the container, these become devices 0,1,2,... (remapped). Setting both ensures the application sees the correct GPU count.
  </action>
  <verify>
Run `grep -n "NVIDIA_VISIBLE_DEVICES" src/llenergymeasure/cli/campaign.py` - should show the new env var propagation code.
  </verify>
  <done>
`_build_docker_command()` accepts gpus parameter and propagates NVIDIA_VISIBLE_DEVICES and CUDA_VISIBLE_DEVICES to container via -e flags.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix launcher.py early CUDA setup for container context</name>
  <files>src/llenergymeasure/orchestration/launcher.py</files>
  <action>
Update `_early_cuda_visible_devices_setup()` to handle the container context correctly:

1. The function currently sets CUDA_VISIBLE_DEVICES from config.gpus directly. This is WRONG when running inside a container because:
   - Container has NVIDIA_VISIBLE_DEVICES=0,1 (host GPUs 0,1 mounted)
   - Inside container, these appear as cuda:0, cuda:1 (remapped)
   - Setting CUDA_VISIBLE_DEVICES=0,1 from config is correct by coincidence but conceptually wrong

2. Add detection for container context and use remapped indices:
   ```python
   # Check if we're inside a container (NVIDIA_VISIBLE_DEVICES was set by runtime)
   nvidia_visible = os.environ.get("NVIDIA_VISIBLE_DEVICES", "")

   # If NVIDIA_VISIBLE_DEVICES is set and not "all", we're in a container
   # with specific GPUs mounted - use remapped indices
   if nvidia_visible and nvidia_visible != "all":
       # Container has specific GPUs mounted, use 0-based remapped indices
       gpu_count = len(nvidia_visible.split(","))
       cuda_devices = ",".join(str(i) for i in range(gpu_count))
   else:
       # Local execution or "all" GPUs - use config.gpus directly
       cuda_devices = ",".join(str(g) for g in gpus)
   ```

3. Add logging to indicate which path was taken:
   ```python
   print(
       f"[launcher] Set CUDA_VISIBLE_DEVICES={cuda_devices} "
       f"(container={bool(nvidia_visible and nvidia_visible != 'all')})",
       file=sys.stderr,
   )
   ```

WHY: When the container runtime sets NVIDIA_VISIBLE_DEVICES, GPUs are remapped. The application inside must use 0-based indices regardless of which host GPUs were selected.
  </action>
  <verify>
Run `grep -n "NVIDIA_VISIBLE_DEVICES" src/llenergymeasure/orchestration/launcher.py` - should show the container detection logic.
  </verify>
  <done>
`_early_cuda_visible_devices_setup()` correctly handles container context with remapped GPU indices and local execution with config.gpus directly.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update launch_experiment_accelerate for GPU env propagation</name>
  <files>src/llenergymeasure/orchestration/launcher.py</files>
  <action>
Review and update `launch_experiment_accelerate()` to ensure GPU env vars are handled correctly:

1. The current code sets CUDA_VISIBLE_DEVICES from config.gpus but doesn't handle MIG properly in all cases. Verify the MIG/UUID detection is correct.

2. Add NVIDIA_VISIBLE_DEVICES propagation for subprocess spawning (when not in container):
   ```python
   # For local execution (not in container), propagate both env vars
   if "NVIDIA_VISIBLE_DEVICES" not in env:
       env["NVIDIA_VISIBLE_DEVICES"] = ",".join(str(g) for g in gpus)
   ```

3. Ensure the env dict is passed to subprocess.run() for all launch modes (direct, torchrun, accelerate).

WHY: Consistent GPU env propagation ensures subprocess workers see the correct GPUs regardless of launch mode.
  </action>
  <verify>
Run `python -c "from llenergymeasure.orchestration.launcher import launch_experiment_accelerate; print('OK')"` - should import without error.
  </verify>
  <done>
`launch_experiment_accelerate()` propagates both NVIDIA_VISIBLE_DEVICES and CUDA_VISIBLE_DEVICES to subprocesses, with correct handling for MIG UUIDs.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:
1. `ruff check src/llenergymeasure/cli/campaign.py src/llenergymeasure/orchestration/launcher.py` passes
2. `grep -r "NVIDIA_VISIBLE_DEVICES" src/llenergymeasure/` shows propagation in campaign.py and launcher.py
3. Unit tests pass: `pytest tests/unit/cli/ tests/unit/orchestration/ -v --tb=short`
</verification>

<success_criteria>
- _build_docker_command() propagates NVIDIA_VISIBLE_DEVICES and CUDA_VISIBLE_DEVICES via -e flags
- _early_cuda_visible_devices_setup() handles container context with remapped indices
- launch_experiment_accelerate() propagates GPU env vars to subprocesses
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/03-gpu-routing-fix/03-01-SUMMARY.md`
</output>
