# syntax=docker/dockerfile:1
# =============================================================================
# TensorRT-LLM Backend: High-performance compiled inference
# =============================================================================
# TensorRT-LLM backend for maximum throughput with compiled engines
#
# Usage:
#   docker build -f docker/Dockerfile.tensorrt \
#     --build-arg BASE_IMAGE=llm-energy-measure-base:latest \
#     -t llm-energy-measure:tensorrt .
#
# Note: TensorRT-LLM requires NVIDIA GPU with compute capability >= 8.0 (Ampere+)
# and installs its own PyTorch version for CUDA compatibility.
# =============================================================================

ARG BASE_IMAGE=llm-energy-measure-base:latest

# ============================================================================
# Stage 1: Builder - Install TensorRT-LLM and dependencies
# ============================================================================
FROM ${BASE_IMAGE} AS builder

# Install MPI libraries required by TensorRT-LLM for multi-GPU execution
RUN apt-get update && apt-get install -y --no-install-recommends \
    libopenmpi3 openmpi-bin openmpi-common \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /build

# Install TensorRT-LLM (brings its own compatible PyTorch and CUDA libraries)
# Use 0.21.x series - compatible with CUDA 12.4 base image
# Note: tensorrt-llm has strict CUDA version requirements
# Requires NVIDIA PyPI index for the actual wheel download
# Fix dependency versions: onnx 1.16.0 for graphsurgeon, cuda-python 12.4.0 for CUDA bindings
RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com/ "tensorrt-llm>=0.21.0,<1.0.0" \
    && pip install --no-cache-dir "onnx==1.16.0" "cuda-python==12.4.0"

# Copy project files
COPY pyproject.toml poetry.lock* README.md ./
COPY src/ ./src/

# Install the package with tensorrt extras (--no-deps to avoid torch conflict)
# Then install remaining dependencies for HuggingFace compatibility
# Includes: peft (LoRA), sentencepiece (Llama/Qwen tokenizers), safetensors (model loading)
RUN pip install --no-cache-dir --no-deps ".[tensorrt]" \
    && pip install --no-cache-dir \
        pydantic loguru typer codecarbon datasets schedule python-dotenv \
        transformers accelerate huggingface_hub sentencepiece peft safetensors

# ============================================================================
# Stage 2: Runtime - Minimal production image
# ============================================================================
FROM ${BASE_IMAGE} AS runtime

# Install MPI libraries required by TensorRT-LLM for multi-GPU execution
RUN apt-get update && apt-get install -y --no-install-recommends \
    libopenmpi3 openmpi-bin openmpi-common \
    && rm -rf /var/lib/apt/lists/*

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv

# TensorRT engine cache directory
ENV TRT_ENGINE_CACHE=/app/.cache/tensorrt-engines
RUN mkdir -p ${TRT_ENGINE_CACHE} && chmod 777 ${TRT_ENGINE_CACHE}

# Use entrypoint with PUID/PGID support
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]
CMD ["llm-energy-measure", "--help"]

# ============================================================================
# Stage 3: Dev - For development with source mounting
# ============================================================================
FROM ${BASE_IMAGE} AS dev

# Install MPI libraries required by TensorRT-LLM for multi-GPU execution
RUN apt-get update && apt-get install -y --no-install-recommends \
    libopenmpi3 openmpi-bin openmpi-common \
    && rm -rf /var/lib/apt/lists/*

# Install TensorRT-LLM (brings its own PyTorch)
# Use 0.21.x series - compatible with CUDA 12.4 base image
# Fix dependency versions: onnx 1.16.0 for graphsurgeon, cuda-python 12.4.0 for CUDA bindings
RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com/ "tensorrt-llm>=0.21.0,<1.0.0" \
    && pip install --no-cache-dir "onnx==1.16.0" "cuda-python==12.4.0"

# Copy project files and install with tensorrt + dev extras
# (--no-deps to avoid torch conflict, then install remaining deps)
COPY pyproject.toml poetry.lock* README.md ./
COPY src/ ./src/
RUN pip install --no-cache-dir --no-deps ".[tensorrt]" \
    && pip install --no-cache-dir \
        pydantic loguru typer codecarbon datasets schedule python-dotenv \
        transformers accelerate huggingface_hub sentencepiece peft safetensors \
        pytest pytest-cov ruff mypy pre-commit commitizen

# TensorRT engine cache directory
ENV TRT_ENGINE_CACHE=/app/.cache/tensorrt-engines
RUN mkdir -p ${TRT_ENGINE_CACHE}

# Source can be mounted at runtime for editable install
ENTRYPOINT []
CMD ["/bin/bash"]
