# syntax=docker/dockerfile:1
# =============================================================================
# TensorRT-LLM Backend: High-performance compiled inference
# =============================================================================
# TensorRT-LLM backend for maximum throughput with compiled engines
#
# Usage:
#   docker build -f docker/Dockerfile.tensorrt \
#     --build-arg BASE_IMAGE=llm-energy-measure-base:latest \
#     -t llm-energy-measure:tensorrt .
#
# Note: TensorRT-LLM requires NVIDIA GPU with compute capability >= 8.0 (Ampere+)
# and installs its own PyTorch version for CUDA compatibility.
# =============================================================================

ARG BASE_IMAGE=llm-energy-measure-base:latest

# ============================================================================
# Stage 1: Builder - Install TensorRT-LLM and dependencies
# ============================================================================
FROM ${BASE_IMAGE} AS builder

WORKDIR /build

# Install TensorRT-LLM (brings its own compatible PyTorch and CUDA libraries)
# Pin version for reproducibility - update as needed
# Note: tensorrt-llm has strict CUDA version requirements
# Requires NVIDIA PyPI index for the actual wheel download
RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com/ tensorrt-llm>=0.12.0

# Copy project files
COPY pyproject.toml poetry.lock* README.md ./
COPY src/ ./src/

# Install the package with tensorrt extras (--no-deps to avoid torch conflict)
# Then install remaining dependencies for HuggingFace compatibility
RUN pip install --no-cache-dir --no-deps ".[tensorrt]" \
    && pip install --no-cache-dir \
        pydantic loguru typer codecarbon pynvml datasets schedule \
        transformers accelerate

# ============================================================================
# Stage 2: Runtime - Minimal production image
# ============================================================================
FROM ${BASE_IMAGE} AS runtime

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv

# TensorRT engine cache directory
ENV TRT_ENGINE_CACHE=/app/.cache/tensorrt-engines
RUN mkdir -p ${TRT_ENGINE_CACHE} && chmod 777 ${TRT_ENGINE_CACHE}

# Use entrypoint with PUID/PGID support
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]
CMD ["llm-energy-measure", "--help"]

# ============================================================================
# Stage 3: Dev - For development with source mounting
# ============================================================================
FROM ${BASE_IMAGE} AS dev

# Install TensorRT-LLM (brings its own PyTorch)
RUN pip install --no-cache-dir --extra-index-url https://pypi.nvidia.com/ tensorrt-llm>=0.12.0

# Copy project files and install with tensorrt + dev extras
# (--no-deps to avoid torch conflict, then install remaining deps)
COPY pyproject.toml poetry.lock* README.md ./
COPY src/ ./src/
RUN pip install --no-cache-dir --no-deps ".[tensorrt]" \
    && pip install --no-cache-dir \
        pydantic loguru typer codecarbon pynvml datasets schedule \
        transformers accelerate \
        pytest pytest-cov ruff mypy pre-commit commitizen

# TensorRT engine cache directory
ENV TRT_ENGINE_CACHE=/app/.cache/tensorrt-engines
RUN mkdir -p ${TRT_ENGINE_CACHE}

# Source can be mounted at runtime for editable install
ENTRYPOINT []
CMD ["/bin/bash"]
