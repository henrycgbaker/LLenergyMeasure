config_name: vllm_prefix_caching
model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
backend: vllm
max_input_tokens: 64
max_output_tokens: 32
num_input_prompts: 5
gpus: [0]
fp_precision: float16

decoder:
  temperature: 0.0
  do_sample: false

vllm:
  gpu_memory_utilization: 0.8
  max_num_seqs: 128
  enable_prefix_caching: true
  enable_chunked_prefill: true
  enforce_eager: true
