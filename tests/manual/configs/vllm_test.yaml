# vLLM backend test config
config_name: vllm_backend_test
model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
backend: vllm

max_input_tokens: 64
max_output_tokens: 32
num_input_prompts: 10
gpus: [0]

fp_precision: float16

decoder:
  preset: deterministic

# vLLM-specific config
vllm:
  max_num_seqs: 64
  gpu_memory_utilization: 0.85
  enable_prefix_caching: true
  enforce_eager: false
