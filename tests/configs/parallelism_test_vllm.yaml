# =============================================================================
# vLLM Parallelism Test Config
# =============================================================================
# Tests tensor parallelism (model sharded across GPUs, managed by vLLM).
# Use with: lem experiment tests/configs/parallelism_test_vllm.yaml
#
# Expected behaviour:
# - Single launcher process
# - vLLM internally shards model layers across 2 GPUs via NCCL
# - Communication handled by vLLM's tensor parallel implementation
# =============================================================================

config_name: vllm-parallelism-test
model_name: Qwen/Qwen2.5-0.5B  # Small model for quick testing

dataset:
  name: ai_energy_score
  sample_size: 20

max_input_tokens: 256
max_output_tokens: 32
gpus: [0, 1]  # Use 2 GPUs
backend: vllm

decoder:
  preset: deterministic  # Reproducible outputs

vllm:
  # Tensor parallelism: model sharded across 2 GPUs
  tensor_parallel_size: 2
  pipeline_parallel_size: 1  # No pipeline parallelism

  # Memory settings
  max_num_seqs: 32
  gpu_memory_utilization: 0.8

  # Disable features for cleaner test
  enable_prefix_caching: false
  enable_chunked_prefill: false
