# =============================================================================
# TensorRT-LLM Parallelism Test Config
# =============================================================================
# Tests tensor parallelism (model sharded across GPUs, managed by TRT-LLM).
# Use with: lem experiment tests/configs/parallelism_test_tensorrt.yaml
#
# Requirements:
# - NVIDIA GPU with compute capability >= 8.0 (Ampere+)
# - TensorRT-LLM installed
#
# Expected behaviour:
# - Single launcher process
# - TRT-LLM compiles engine with tensor parallel config
# - Model layers sharded across 2 GPUs via NCCL
# =============================================================================

config_name: tensorrt-parallelism-test
model_name: Qwen/Qwen2.5-0.5B  # Small model for quick testing

dataset:
  name: ai_energy_score
  sample_size: 20

max_input_tokens: 256
max_output_tokens: 32
gpus: [0, 1]  # Use 2 GPUs
backend: tensorrt
fp_precision: float16  # TensorRT doesn't support float32

decoder:
  preset: deterministic  # Reproducible outputs

tensorrt:
  # Tensor parallelism: model sharded across 2 GPUs
  tp_size: 2
  pp_size: 1  # No pipeline parallelism

  # Build configuration
  max_batch_size: 8
  builder_opt_level: 3

  # No quantization for cleaner test
  quantization: none
